---
title: 'Brain Dump: January 2023'
author: Alex Zajichek
date: '2023-01-12'
slug: brain-dump-january-2023
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2023-01-12T10:23:09-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<p>Over time, I’ve found myself constantly learning new things or having new ideas, only to later forget them (like a lot of the things I learned in grad school), then trying to relearn the same thing at a later date. So I thought it might be worthwhile to try to create a knowledge base for myself that I add to weekly so that I can refer back when needed to maintain intuition. These might be little programming tricks, math and/or statistics concepts, general thoughts or ideas, etc. We’ll see how this goes…</p>
<div id="table-of-contents" class="section level1">
<h1>Table of Contents</h1>
<ul>
<li><a href="#bernoulli">Deriving a Bernoulli MLE</a></li>
<li><a href="#splines">Basis Functions &amp; Splines</a></li>
<li><a href="#effectivedatastory"><em>Effective Data Storytelling</em>: Key Takeaways</a></li>
</ul>
</div>
<div id="bernoulli" class="section level1">
<h1>Deriving a Bernoulli MLE</h1>
<p><em>1/12/2023</em></p>
<p>I tried to derive the MLE for a set of Bernoulli responses just to see if I could, and got stuck. The <em>probability mass function (PMF)</em> is</p>
<p><span class="math display">\[P(Y_i = y) = p^y(1-p)^{1-y}\]</span>
First, we need to define the likelihood function:</p>
<p><span class="math display">\[L(p) = \prod_{i=1}^nP(Y_i=y)\]</span>
Then, convert ot a log likelihood:</p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
log(L(p))
&amp;= log(\prod_{i=1}^nP(Y_i=y)) \\
&amp;= \sum_{i=1}^nlog(P(Y_i=y)) \\
&amp;= \sum_{i=1}^nlog(p^{y_i}(1-p)^{1-y_i}) \\
&amp;= log(p)\sum_{i=1}^ny_i + log(1-p)(n - \sum_{i=1}^ny_i)
\end{split}
\end{equation}
\]</span>
Next, we need to differentiate with respect to <span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
\frac{d}{dp}log(L(p))
&amp;= \frac{d}{dp} log(p)\sum_{i=1}^ny_i + log(1-p)(n - \sum_{i=1}^ny_i) \\
&amp;= \sum_{i=1}^ny_i \frac{d}{dp} log(p) + (n - \sum_{i=1}^ny_i) \frac{d}{dp} log(1-p) \\
&amp;= \sum_{i=1}^ny_i \frac{1}{p} + (n - \sum_{i=1}^ny_i) \frac{1}{1-p} \frac{d}{dp} (1-p) \\
\end{split}
\end{equation}
\]</span>
This is where my problem was. I forgot about the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a> when taking a derivative, so I wasn’t doing the additional step of taking multiplying by the derivative of <span class="math inline">\(1-p\)</span>, thus things weren’t cancelling appropriately when solving for <span class="math inline">\(p\)</span>. It should reduce to:</p>
<p><span class="math display">\[\sum_{i=1}^ny_i \frac{1}{p} + (n - \sum_{i=1}^ny_i) \frac{-1}{1-p}\]</span>
Finally, we just set that equal to zero:</p>
<p><span class="math display">\[\sum_{i=1}^ny_i \frac{1}{p} + (n - \sum_{i=1}^ny_i) \frac{-1}{1-p} = 0\]</span>
After some algebra:</p>
<p><span class="math display">\[\hat{p} = \frac{\sum_{i=1}^ny_i}{n}\]</span></p>
</div>
<div id="splines" class="section level1">
<h1>Basis Functions &amp; Splines</h1>
<p><em>1/13/2023</em></p>
<p>This one always gets me. I’ve learned and forgot how splines work many times over the years, and when I need to relearn it, I read the <em>Moving Beyond Linearity</em> chapter of <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a>.</p>
<p><strong>Basis Functions</strong> are just a general approach for transforming a simple model like:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1X_i + \epsilon_i\]</span>
into a linear combination of transformations of the <span class="math inline">\(X_i\)</span> of the form:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1b_1(X_i) + \beta_2b_2(X_i) + ... + \beta_Kb_k(X_i) + \epsilon_i\]</span>
where <span class="math inline">\(b_i\)</span> is a known function that transforms the predictor variable. <em>Note: <span class="math inline">\(\beta_1\)</span> is not the same in both of these, they are just placeholders for an arbitrary parameter.</em> For example, in the case of a piecewise regression where the model is of the form:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1I(X_i &lt; c_1) + \beta_2I(c_1 &lt; X_i &lt; c_2) + ... + \beta_kI(c_{k-1} &lt; X_i &lt; c_k) + \epsilon_i\]</span>
the indicators are <em>basis functions</em> such that:</p>
<p><span class="math display">\[b_j(X_i) = I(c_{j-1} &lt; X_i &lt; c_j) \hskip.1in \text{for j=1,..,k}\]</span>
or in a polynomial model, the basis functions are <span class="math inline">\(b_j(X_i) = X_i^j\)</span>.</p>
<p><strong>Knots</strong> are points (cutoffs) along <span class="math inline">\(X_i\)</span> that a <em>local</em> regression starts/ends. For example, we might fit a cubic model (e.g., with parameters <span class="math inline">\(\beta_1, \beta_2, \beta_3\)</span>) where <span class="math inline">\(X_i &lt; C\)</span>, and another model (with a <em>separate</em> set of <span class="math inline">\(\beta_1, \beta_2, \beta_3\)</span>) where <span class="math inline">\(X_i \geq C\)</span>. <span class="math inline">\(C\)</span> is a <em>knot</em>. In this sense, the piecewise regression above was also a polynomial regression with degree 0, and knots at each value of <span class="math inline">\(c_j\)</span>.</p>
<p>The general problem with an unconstrained polynomial model is that there are no restrictions that force a smooth function across <span class="math inline">\(X\)</span>, so there are discontinuities. Thus, restrictions need to be put in place such as (1) making it continuous at the knot(s), and/or even further, (2) making the first and second derivatives continuous at the knots. These restrictions reduce the complexity of the model (i.e., the number of parameters we estimate).</p>
<p><strong>Cubic Splines</strong></p>
<p>A cubic spline with <span class="math inline">\(K\)</span> knots uses <span class="math inline">\(K+4\)</span> parameters. The best way to do this is to use (1) the basis of a cubic polynomial (<span class="math inline">\(x, x^2, x^3\)</span>) and (2) a truncated power basis <em>for each knot</em>:</p>
<p><span class="math display">\[h(x,\nu) = {(x-\nu)}_+^3\]</span>
where <span class="math inline">\(\nu\)</span> is the knot location. Thus, a <em>one-knot</em> model looks like:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1X_i + \beta_2X_i^2 + \beta_3X_i^3 + \beta_4h(X_i,\nu_1) + \epsilon_i\]</span>
We can add more knots as needed, and it simply adds <span class="math inline">\(h(x,\nu)\)</span> terms only (so 1 more parameter per knot). A function of this form is guaranteed to have continuous first and second derivatives.</p>
<p><strong>So how does this relate to what is produced in the <code>rcs</code> function from the <code>rms</code> package?</strong></p>
<p>Well, the package actually fits a <em>restricted</em> cubic spline, which is a <em>natural spline</em>. This adds even more restrictions that the general cubic spline by forcing it to be <em>linear</em> where <span class="math inline">\(X\)</span> is less than the smallest knot and where <span class="math inline">\(X\)</span> is larger than the largest not (i.e., the boundaries). These add an additional <em>two</em> constraints at <em>each</em> boundary. So if we have a regular cubic spline model above with 3 knots (i.e., 7 parameters), then a <em>restricted</em> cubic spline model with 3 knots should have only 3 parameters.</p>
<pre class="r"><code>set.seed(123)
x &lt;- rnorm(100)
y &lt;- .5*x^2 - .75*x + rnorm(100)
x_trans &lt;- rms::rcs(x, 3)
head(x_trans)</code></pre>
<pre><code>##                x         x&#39;
## [1,] -0.56047565 0.02405534
## [2,] -0.23017749 0.10816181
## [3,]  1.55870831 2.14013743
## [4,]  0.07050839 0.27135366
## [5,]  0.12928774 0.31547103
## [6,]  1.71506499 2.36735648
## attr(,&quot;class&quot;)
## [1] &quot;rms&quot;
## attr(,&quot;name&quot;)
## [1] &quot;x&quot;
## attr(,&quot;label&quot;)
## [1] &quot;x&quot;
## attr(,&quot;assume&quot;)
## [1] &quot;rcspline&quot;
## attr(,&quot;assume.code&quot;)
## [1] 4
## attr(,&quot;parms&quot;)
## [1] -1.06822046  0.06175631  1.26449867
## attr(,&quot;nonlinear&quot;)
## [1] FALSE  TRUE
## attr(,&quot;colnames&quot;)
## [1] &quot;x&quot;  &quot;x&#39;&quot;</code></pre>
<pre class="r"><code>summary(lm(y~x_trans))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x_trans)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9888 -0.7341 -0.0803  0.6900  3.2215 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -0.3838     0.1842  -2.084   0.0398 *  
## x_transx     -1.6552     0.2449  -6.758 1.04e-09 ***
## x_transx&#39;     1.2922     0.2925   4.417 2.61e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9823 on 97 degrees of freedom
## Multiple R-squared:  0.3805, Adjusted R-squared:  0.3677 
## F-statistic: 29.79 on 2 and 97 DF,  p-value: 8.214e-11</code></pre>
<p>We can see this model contains three estimated parameters as expected. The actual transformation completed for the restricted cubic spline in producing, in general, the <span class="math inline">\(K-2\)</span> additional predictors is more complex than the cubic spline (although similar). In this case, the 3 knot positions were selected to be:</p>
<pre class="r"><code>knots &lt;- attr(x_trans, &quot;parms&quot;)
knots</code></pre>
<pre><code>## [1] -1.06822046  0.06175631  1.26449867</code></pre>
<p>Note, these are just the <span class="math inline">\(10^{th}, 50^{th}, 90^{th}\)</span> percentiles:</p>
<pre class="r"><code>quantile(x, c(.1,.5,.9))</code></pre>
<pre><code>##         10%         50%         90% 
## -1.06822046  0.06175631  1.26449867</code></pre>
<p>The following transformation is made (general solution <a href="https://support.sas.com/resources/papers/proceedings16/5621-2016.pdf">here</a>):</p>
<p><span class="math display">\[X_{trans} = (x-\nu_1)_+^3 - (x-\nu_2)_+^3\frac{\nu_3-\nu_1}{\nu_3-\nu_2} + (x-\nu_3)_+^3\frac{\nu_2-\nu_1}{\nu_3-\nu_2}\]</span>
Let’s check out that transformation on our data:</p>
<pre class="r"><code>tibble::tibble(
  x = as.numeric(x_trans[,&quot;x&quot;]),
  x_trans_actual = as.numeric(x_trans[,&quot;x&#39;&quot;]),
  x_trans_calculated = 
    pmax((x-knots[1])^3, 0) -
    pmax((x-knots[2])^3, 0) * ((knots[3]-knots[1]) / (knots[3]-knots[2])) +
    pmax((x-knots[3])^3, 0) * ((knots[2]-knots[1])/(knots[3]-knots[2]))
)</code></pre>
<pre><code>## # A tibble: 100 × 3
##          x x_trans_actual x_trans_calculated
##      &lt;dbl&gt;          &lt;dbl&gt;              &lt;dbl&gt;
##  1 -0.560          0.0241             0.131 
##  2 -0.230          0.108              0.589 
##  3  1.56           2.14              11.6   
##  4  0.0705         0.271              1.48  
##  5  0.129          0.315              1.72  
##  6  1.72           2.37              12.9   
##  7  0.461          0.634              3.45  
##  8 -1.27           0                  0     
##  9 -0.687          0.0102             0.0555
## 10 -0.446          0.0443             0.241 
## # … with 90 more rows</code></pre>
<p>For some reason this is close but off by a factor close to 5? Looking into the documentation/code, it is because of the <code>norm</code> argument in the <code>Hmisc::rcspline.eval</code> function. When we run this, we get the same result that we calculated (which is the original restricted cubic spline calculation):</p>
<pre class="r"><code>head(Hmisc::rcspline.eval(x,nk=3, norm = 0))</code></pre>
<pre><code>##           [,1]
## [1,]  0.130899
## [2,]  0.588571
## [3,] 11.645726
## [4,]  1.476592
## [5,]  1.716660
## [6,] 12.882156</code></pre>
<p>By default, this function uses <code>norm=2</code>, which <em>“normalizes by the square of the spacing between the first and last knots…has the advantage of making all nonlinear terms be on the x-scale”</em>.</p>
</div>
<div id="effectivedatastory" class="section level1">
<h1><em>Effective Data Storytelling</em>: Key Takeaways</h1>
<p><em>1/14/2023</em></p>
<p>I’ve been reading through the book <a href="https://www.effectivedatastorytelling.com/"><em>Effective Data Storytelling: How to drive change with data, narrative, and visuals</em></a> by Brent Dykes and just wanted to document some thoughts/takeaways.</p>
<ol style="list-style-type: decimal">
<li>Importance of domain knowledge</li>
</ol>
<p>One of the biggest (and obvious) takeaways is the importance of domain knowledge is being able to craft an effective data story. You can have all the technical skills in world (e.g., math, statistics, programming, etc.), but if you aren’t able to relay that back to how it can impact things people care about, in simple terms, you’ll have trouble getting buy in. Thus, it is essential to be able to (i) acquire business knowledge and intuition for yourself, and (ii) asking the right questions, and (iii) have subject matter experts (SMEs) who can <em>partner</em> with you to problem solve together. Domain knowledge is also incredibly important <em>in</em> the technical aspects as well (e.g., construction of useful statistical models generally requires assumptions about the context of the problem you are solving).</p>
<ol start="2" style="list-style-type: decimal">
<li>Organizational culture and support</li>
</ol>
<p>I think in order to put data professionals in a position to be able to craft effective data stories, there needs to be some level of forethought into the data culture and structure of the organization. Just from a logistical perspective, it would be very difficult for someone to be able to take the necessary time and delicacy needed to truly create an effective data story (e.g., using the right data, in the right analysis, with the right communication of results) without the foundational support of data infrastructure, data governance, process/project management, defined roles/responsibilities, overall strategy, etc. These things need to be in place (or at least working towards it) to apply the apprpriate amount of rigor and detail required in an efficient and timely manner.</p>
<ol start="3" style="list-style-type: decimal">
<li>Tailor communication of results to your audience</li>
</ol>
<p>In my current role, I often build analyses in the form of HTML documents (via <a href="https://rmarkdown.rstudio.com/">R Markdown</a>) where the intention is to lay out a story of the data in a particular order, such the the end-user would read through the document on their own to gain the messages of the analysis with interactive figures and tables (the idea of <em>“scrollytelling”</em>). However, I would often get asked by stakeholders to instead present/explain the material for them instead of them (understandably) reading through the whole thing on their own. So I would attend a meeting, bring up the document, and begin walking them through the document. I quickly found that this didn’t work. The intention of the document was to be read, not presented, and so people would get bogged down/distracted by the narrative in the document while presenting. It just felt uncomfortable. This is <em>exactly</em> something that is talked about in the book of what not to do. So, before sharing with stakeholders or presenting, I started converting the information in the document into a slide deck, pulling out the most useful information that was tailored to the way I would present it. Also, recreating graphs, tables, and/or figures that point out just the most pertinent data, instead of using the exhaustive summaries built in the exploratory analysis. These strategies proved to be much more effective and satisfying both for myself and the stakeholders.</p>
</div>
