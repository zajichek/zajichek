---
title: 'Brain Dump: January 2023'
author: Alex Zajichek
date: '2023-01-12'
slug: brain-dump-january-2023
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2023-01-12T10:23:09-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

Over time, I've found myself constantly learning new things or having new ideas, only to later forget them (like a lot of the things I learned in grad school), then trying to relearn the same thing at a later date. So I thought it might be worthwhile to try to create a knowledge base for myself that I add to weekly so that I can refer back when needed to maintain intuition. These might be little programming tricks, math and/or statistics concepts, general thoughts or ideas, etc. We'll see how this goes...

# Table of Contents

* [Deriving a Bernoulli MLE](#bernoulli)
* [Basis Functions & Splines](#splines)
* [_Effective Data Storytelling_: Key Takeaways](#effectivedatastory)

# Deriving a Bernoulli MLE {#bernoulli}

_1/12/2023_

I tried to derive the MLE for a set of Bernoulli responses just to see if I could, and got stuck. The _probability mass function (PMF)_ is

$$P(Y_i = y) = p^y(1-p)^{1-y}$$
First, we need to define the likelihood function:

$$L(p) = \prod_{i=1}^nP(Y_i=y)$$
Then, convert ot a log likelihood:

$$
\begin{equation} 
\begin{split}
log(L(p)) 
&= log(\prod_{i=1}^nP(Y_i=y)) \\
&= \sum_{i=1}^nlog(P(Y_i=y)) \\
&= \sum_{i=1}^nlog(p^{y_i}(1-p)^{1-y_i}) \\
&= log(p)\sum_{i=1}^ny_i + log(1-p)(n - \sum_{i=1}^ny_i)
\end{split}
\end{equation}
$$
Next, we need to differentiate with respect to $p$.

$$
\begin{equation} 
\begin{split}
\frac{d}{dp}log(L(p)) 
&= \frac{d}{dp} log(p)\sum_{i=1}^ny_i + log(1-p)(n - \sum_{i=1}^ny_i) \\
&= \sum_{i=1}^ny_i \frac{d}{dp} log(p) + (n - \sum_{i=1}^ny_i) \frac{d}{dp} log(1-p) \\
&= \sum_{i=1}^ny_i \frac{1}{p} + (n - \sum_{i=1}^ny_i) \frac{1}{1-p} \frac{d}{dp} (1-p) \\
\end{split}
\end{equation}
$$
This is where my problem was. I forgot about the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) when taking a derivative, so I wasn't doing the additional step of taking multiplying by the derivative of $1-p$, thus things weren't cancelling appropriately when solving for $p$. It should reduce to:

$$\sum_{i=1}^ny_i \frac{1}{p} + (n - \sum_{i=1}^ny_i) \frac{-1}{1-p}$$
Finally, we just set that equal to zero:

$$\sum_{i=1}^ny_i \frac{1}{p} + (n - \sum_{i=1}^ny_i) \frac{-1}{1-p} = 0$$
After some algebra:

$$\hat{p} = \frac{\sum_{i=1}^ny_i}{n}$$

# Basis Functions & Splines {#splines}

_1/13/2023_

This one always gets me. I've learned and forgot how splines work many times over the years, and when I need to relearn it, I read the _Moving Beyond Linearity_ chapter of [An Introduction to Statistical Learning](https://www.statlearning.com/). 

**Basis Functions** are just a general approach for transforming a simple model like:

$$Y_i = \beta_0 + \beta_1X_i + \epsilon_i$$
into a linear combination of transformations of the $X_i$ of the form:

$$Y_i = \beta_0 + \beta_1b_1(X_i) + \beta_2b_2(X_i) + ... + \beta_Kb_k(X_i) + \epsilon_i$$
where $b_i$ is a known function that transforms the predictor variable. _Note: $\beta_1$ is not the same in both of these, they are just placeholders for an arbitrary parameter._ For example, in the case of a piecewise regression where the model is of the form:

$$Y_i = \beta_0 + \beta_1I(X_i < c_1) + \beta_2I(c_1 < X_i < c_2) + ... + \beta_kI(c_{k-1} < X_i < c_k) + \epsilon_i$$
the indicators are _basis functions_ such that:

$$b_j(X_i) = I(c_{j-1} < X_i < c_j) \hskip.1in \text{for j=1,..,k}$$
or in a polynomial model, the basis functions are $b_j(X_i) = X_i^j$.

**Knots** are points (cutoffs) along $X_i$ that a _local_ regression starts/ends. For example, we might fit a cubic model (e.g., with parameters $\beta_1, \beta_2, \beta_3$) where $X_i < C$, and another model (with a _separate_ set of $\beta_1, \beta_2, \beta_3$) where $X_i \geq C$. $C$ is a _knot_. In this sense, the piecewise regression above was also a polynomial regression with degree 0, and knots at each value of $c_j$.

The general problem with an unconstrained polynomial model is that there are no restrictions that force a smooth function across $X$, so there are discontinuities. Thus, restrictions need to be put in place such as (1) making it continuous at the knot(s), and/or even further, (2) making the first and second derivatives continuous at the knots. These restrictions reduce the complexity of the model (i.e., the number of parameters we estimate). 

**Cubic Splines**

A cubic spline with $K$ knots uses $K+4$ parameters. The best way to do this is to use (1) the basis of a cubic polynomial ($x, x^2, x^3$) and (2) a truncated power basis _for each knot_:

$$h(x,\nu) = {(x-\nu)}_+^3$$
where $\nu$ is the knot location. Thus, a _one-knot_ model looks like:

$$Y_i = \beta_0 + \beta_1X_i + \beta_2X_i^2 + \beta_3X_i^3 + \beta_4h(X_i,\nu_1) + \epsilon_i$$
We can add more knots as needed, and it simply adds $h(x,\nu)$ terms only (so 1 more parameter per knot). A function of this form is guaranteed to have continuous first and second derivatives.

**So how does this relate to what is produced in the `rcs` function from the `rms` package?**

Well, the package actually fits a _restricted_ cubic spline, which is a _natural spline_. This adds even more restrictions that the general cubic spline by forcing it to be _linear_ where $X$ is less than the smallest knot and where $X$ is larger than the largest not (i.e., the boundaries). These add an additional _two_ constraints at _each_ boundary. So if we have a regular cubic spline model above with 3 knots (i.e., 7 parameters), then a _restricted_ cubic spline model with 3 knots should have only 3 parameters.

```{r}
set.seed(123)
x <- rnorm(100)
y <- .5*x^2 - .75*x + rnorm(100)
x_trans <- rms::rcs(x, 3)
head(x_trans)
summary(lm(y~x_trans))
```

We can see this model contains three estimated parameters as expected. The actual transformation completed for the restricted cubic spline in producing, in general, the $K-2$ additional predictors is more complex than the cubic spline (although similar). In this case, the 3 knot positions were selected to be:

```{r}
knots <- attr(x_trans, "parms")
knots
```

Note, these are just the $10^{th}, 50^{th}, 90^{th}$ percentiles:

```{r}
quantile(x, c(.1,.5,.9))
```

The following transformation is made (general solution [here](https://support.sas.com/resources/papers/proceedings16/5621-2016.pdf)):

$$X_{trans} = (x-\nu_1)_+^3 - (x-\nu_2)_+^3\frac{\nu_3-\nu_1}{\nu_3-\nu_2} + (x-\nu_3)_+^3\frac{\nu_2-\nu_1}{\nu_3-\nu_2}$$
Let's check out that transformation on our data:
```{r}
tibble::tibble(
  x = as.numeric(x_trans[,"x"]),
  x_trans_actual = as.numeric(x_trans[,"x'"]),
  x_trans_calculated = 
    pmax((x-knots[1])^3, 0) -
    pmax((x-knots[2])^3, 0) * ((knots[3]-knots[1]) / (knots[3]-knots[2])) +
    pmax((x-knots[3])^3, 0) * ((knots[2]-knots[1])/(knots[3]-knots[2]))
)
```

For some reason this is close but off by a factor close to 5? Looking into the documentation/code, it is because of the `norm` argument in the `Hmisc::rcspline.eval` function. When we run this, we get the same result that we calculated (which is the original restricted cubic spline calculation):

```{r}
head(Hmisc::rcspline.eval(x,nk=3, norm = 0))
```

By default, this function uses `norm=2`, which _"normalizes by the square of the spacing between the first and last knots...has the advantage of making all nonlinear terms be on the x-scale"_.

# _Effective Data Storytelling_: Key Takeaways {#effectivedatastory}

_1/14/2023_

I've been reading through the book [_Effective Data Storytelling: How to drive change with data, narrative, and visuals_](https://www.effectivedatastorytelling.com/) by Brent Dykes and just wanted to document some thoughts/takeaways.

1. Importance of domain knowledge

One of the biggest (and obvious) takeaways is the importance of domain knowledge is being able to craft an effective data story. You can have all the technical skills in world (e.g., math, statistics, programming, etc.), but if you aren't able to relay that back to how it can impact things people care about, in simple terms, you'll have trouble getting buy in. Thus, it is essential to be able to (i) acquire business knowledge and intuition for yourself, and (ii) asking the right questions, and (iii) have subject matter experts (SMEs) who can _partner_ with you to problem solve together. Domain knowledge is also incredibly important _in_ the technical aspects as well (e.g., construction of useful statistical models generally requires assumptions about the context of the problem you are solving).

2. Organizational culture and support

I think in order to put data professionals in a position to be able to craft effective data stories, there needs to be some level of forethought into the data culture and structure of the organization. Just from a logistical perspective, it would be very difficult for someone to be able to take the necessary time and delicacy needed to truly create an effective data story (e.g., using the right data, in the right analysis, with the right communication of results) without the foundational support of data infrastructure, data governance, process/project management, defined roles/responsibilities, overall strategy, etc. These things need to be in place (or at least working towards it) to apply the apprpriate amount of rigor and detail required in an efficient and timely manner.

3. Tailor communication of results to your audience

In my current role, I often build analyses in the form of HTML documents (via [R Markdown](https://rmarkdown.rstudio.com/)) where the intention is to lay out a story of the data in a particular order, such the the end-user would read through the document on their own to gain the messages of the analysis with interactive figures and tables (the idea of _"scrollytelling"_). However, I would often get asked by stakeholders to instead present/explain the material for them instead of them (understandably) reading through the whole thing on their own. So I would attend a meeting, bring up the document, and begin walking them through the document. I quickly found that this didn't work. The intention of the document was to be read, not presented, and so people would get bogged down/distracted by the narrative in the document while presenting. It just felt uncomfortable. This is _exactly_ something that is talked about in the book of what not to do. So, before sharing with stakeholders or presenting, I started converting the information in the document into a slide deck, pulling out the most useful information that was tailored to the way I would present it. Also, recreating graphs, tables, and/or figures that point out just the most pertinent data, instead of using the exhaustive summaries built in the exploratory analysis. These strategies proved to be much more effective and satisfying both for myself and the stakeholders.


