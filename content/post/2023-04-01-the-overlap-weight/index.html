---
title: The overlap weight in survival analysis
author: Alex Zajichek
date: '2023-08-25'
slug: the-overlap-weight
categories: []
tags: ['causal inference', 'survival analysis', 'rstats']
subtitle: "A simulation review"
summary: "The goal is to estimate what the outcome difference would be had the counterfactual also been observed."
authors: []
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/core-js/shim.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/react/react.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/react/react-dom.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/reactwidget/react-tools.js"></script>
<script src="{{< blogdown/postref >}}index_files/htmlwidgets/htmlwidgets.js"></script>
<link href="{{< blogdown/postref >}}index_files/reactable/reactable.css" rel="stylesheet" />
<script src="{{< blogdown/postref >}}index_files/reactable-binding/reactable.js"></script>


<p><strong><em>In progress</em></strong></p>
<p>I was recently introduced to <a href="https://jamanetwork.com/journals/jama/article-abstract/2765748">overlap weighting</a>, which is part of a general family of methods for balancing covariates when estimating treatment effects with observational data. Specifically, it focuses on the <a href="https://www.sciencedirect.com/topics/medicine-and-dentistry/clinical-equipoise#:~:text=Clinical%20equipoise%20is%20defined%20as,Seminars%20in%20Vascular%20Surgery%2C%202022"><em>clinical equipoise</em></a>; that is, the patients in which the treatment decision is most uncertain. I find it more elegant than <a href="https://en.wikipedia.org/wiki/Propensity_score_matching">matching</a> (which I’ve <a href="https://jamanetwork.com/journals/jama/fullarticle/2749478">used in the past</a>), and figured a useful way to better understand the approach is to deconstruct, interpret, and translate a <a href="http://www2.stat.duke.edu/~fl35/OW/OW_survival_Demo.sas">SAS simulation</a> in the context of survival analysis implemented by the <a href="https://pubmed.ncbi.nlm.nih.gov/30189042/">original authors</a>. All code is written in (and translated to) <a href="https://www.r-project.org/"><code>R</code></a>. We’ll start by loading some packages.</p>
<pre class="r"><code>library(tidyverse)
library(survival)
library(reactable)</code></pre>
<div id="table-of-contents" class="section level1">
<h1>Table of Contents</h1>
<ul>
<li><a href="#potentialoutcomes">Setting up the potential outcomes</a>
<ul>
<li><a href="#simulatepatients">Simulate some patients</a></li>
<li><a href="#treatmentpropensity">Define the treatment propensity</a></li>
<li><a href="#treatmentassignment">Generate the treatment assignment</a></li>
<li><a href="#trueoverlapweight">Compute the (true) overlap weight</a></li>
<li><a href="#treatmenteffect">Set the treatment effect</a></li>
<li><a href="#observedoutcome">Assign the observed outcome</a></li>
</ul></li>
<li><a href="#estimatingweights">Estimating the weights</a>
<ul>
<li><a href="#modelpropensityscores">Model the propensity scores</a></li>
<li><a href="#estimatedoverlapweight">Calculate the (estimated) overlap weight</a></li>
</ul></li>
</ul>
</div>
<div id="potentialoutcomes" class="section level1">
<h1>Setting up the potential outcomes</h1>
<p>The theoretical underpinnings of <a href="https://pubmed.ncbi.nlm.nih.gov/30189042/">overlap weighting</a> live in the context of the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5841618/#:~:text=The%20potential%20outcomes%20framework%20provides%20a%20way%20to%20quantify%20causal,exposure%20or%20intervention%20under%20consideration.">potential outcomes</a> paradigm of <a href="https://www.sciencedirect.com/topics/social-sciences/causal-inference">causal inference</a>. Basically, we wonder what <em>would have</em> happened had we been able to observe each patient under both treatments (known as the <a href="https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-5-28">counterfactual</a>). If we knew the outcomes from the two worlds, the causal treatment effect would simply be the average difference across all patients. The problem of course is that in reality the outcome can only be observed for the treatment in which the patient was assigned (or chose), and so our goal is to work with the “incomplete” information that we do have to try to estimate what the outcome difference would have been had the counterfactual been observed as well.</p>
<div id="simulatepatients" class="section level2">
<h2>Simulate some patients</h2>
<p>The first thing we need to do is generate some (fake) patients to facilitate the simulation. Here we’ll focus on age, sex, and income as patient-identifying characteristics.</p>
<pre class="r"><code># Set the sample size
n &lt;- 10000

# Set the seed
set.seed(123)

# Build the population
population &lt;-
  tibble(
    age = rnorm(n, mean = 50, sd = 10),
    zage = (age - 50) / 10,
    male = rbinom(n, size = 1, prob = 0.6),
    income = rnorm(n, mean = 50, sd = 10),
    zincome = (income - 50) / 10
  )
population</code></pre>
<pre><code>## # A tibble: 10,000 × 5
##      age    zage  male income zincome
##    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;
##  1  44.4 -0.560      0   36.5  -1.35 
##  2  47.7 -0.230      1   44.2  -0.579
##  3  65.6  1.56       1   41.4  -0.861
##  4  50.7  0.0705     1   59.7   0.973
##  5  51.3  0.129      0   56.2   0.619
##  6  67.2  1.72       1   63.9   1.39 
##  7  54.6  0.461      1   35.1  -1.49 
##  8  37.3 -1.27       1   56.4   0.639
##  9  43.1 -0.687      1   53.7   0.375
## 10  45.5 -0.446      1   53.7   0.370
## # … with 9,990 more rows</code></pre>
<p>We’ve generated a sample of 10000 patients. We’ll assume <code>age</code> is measured in years, <code>income</code> in thousands of dollars ($), and <code>male</code> is 1 for <em>Male</em> and 0 for <em>Female</em>. The columns <code>zage</code> and <code>zincome</code> are just <a href="https://statisticsbyjim.com/glossary/standardization/#:~:text=In%20statistics%2C%20standardization%20is%20the,standard%20deviation%20for%20a%20variable.">standardized</a> versions of the originals to avoid scaling nuances (we’ll refer to these as <span class="math inline">\(age_z\)</span> and <span class="math inline">\(income_z\)</span>).</p>
</div>
<div id="treatmentpropensity" class="section level2">
<h2>Define the treatment propensity</h2>
<p>Here’s where the important theory starts to creep in (already). We assume that there is a <em>true</em> propensity score, say <span class="math inline">\(p_i^A\)</span>, that is the <em>true</em> probability that patient <em>i</em> receives treatment <em>A</em> given their specific characteristics (and let’s assume there are possible treatments <em>A</em> &amp; <em>B</em>). Further, we assume (some transformation of) this probability is a linear combination of all the characteristics that confound the crude treatment effect on the outcome. In this simulation, we’ll assume the <a href="https://en.wikipedia.org/wiki/Logistic_regression">logit</a> model:</p>
<p><span class="math display">\[log(\frac{p_i^A}{1 - p_i^A}) = 0.41 \times age_z - 0.22 \times male - 0.69 \times income_z - 0.40\]</span></p>
<p>So let’s add this <em>true</em> propensity score to the <code>population</code> data set:</p>
<pre class="r"><code>population &lt;-
  population |&gt;
  
  # Add the true propensity score (unknown quantity)
  mutate(
    log_odds_pA = -0.4 + log(1.5)*zage + log(0.8)*male + log(.50)*zincome,
    pA = 1 / (1 + exp(-log_odds_pA))
  )
population</code></pre>
<pre><code>## # A tibble: 10,000 × 7
##      age    zage  male income zincome log_odds_pA    pA
##    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;
##  1  44.4 -0.560      0   36.5  -1.35        0.311 0.577
##  2  47.7 -0.230      1   44.2  -0.579      -0.315 0.422
##  3  65.6  1.56       1   41.4  -0.861       0.606 0.647
##  4  50.7  0.0705     1   59.7   0.973      -1.27  0.219
##  5  51.3  0.129      0   56.2   0.619      -0.777 0.315
##  6  67.2  1.72       1   63.9   1.39       -0.888 0.292
##  7  54.6  0.461      1   35.1  -1.49        0.595 0.644
##  8  37.3 -1.27       1   56.4   0.639      -1.58  0.171
##  9  43.1 -0.687      1   53.7   0.375      -1.16  0.238
## 10  45.5 -0.446      1   53.7   0.370      -1.06  0.257
## # … with 9,990 more rows</code></pre>
<p>Obviously in practice we don’t know what <span class="math inline">\(p_i^A\)</span> is, so it must be <em>estimated</em> from the treatment assignments we observe in the data. Additionally, we aren’t strictly required to assume the <a href="https://en.wikipedia.org/wiki/Logistic_regression">logit</a> model, but when we use it to estimate the propensity score for <a href="https://pubmed.ncbi.nlm.nih.gov/30189042/">overlap weighting</a>, it has the advantageous property of perfect balance. That is, the weighted-mean differences for all covariates included in the logistic regression model will be zero across the treatment groups.</p>
</div>
<div id="treatmentassignment" class="section level2">
<h2>Generate the treatment assignment</h2>
<p>The treatment assignment is one of the <em>known</em> quantities we would observe in a real life sample, and that is what would be used as the dependent variable for <em>estimating</em> propensity scores. However, since we are running a simulation, we need to generate the treatment assignments from the governing distribution. We assume that the observed treatment for patient <em>i</em> is the result of an (unfair) coin flip that has a probability equal to the <a href="#treatmentpropensity">true propensity score</a>. In statistical notation,</p>
<p><span class="math display">\[A_i \sim Bernoulli(p_i^A)\]</span></p>
<p>where <span class="math inline">\(A_i\)</span> is the indicator of treatment A. Let’s add a <em>realized</em> treatment assignment to the <code>population</code>:</p>
<pre class="r"><code>population &lt;-
  population |&gt;
  
  # Add the realized treatment assignment (known quantity)
  mutate(
    A = rbinom(n, size = 1, prob = pA),
  )
population</code></pre>
<pre><code>## # A tibble: 10,000 × 8
##      age    zage  male income zincome log_odds_pA    pA     A
##    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
##  1  44.4 -0.560      0   36.5  -1.35        0.311 0.577     1
##  2  47.7 -0.230      1   44.2  -0.579      -0.315 0.422     0
##  3  65.6  1.56       1   41.4  -0.861       0.606 0.647     1
##  4  50.7  0.0705     1   59.7   0.973      -1.27  0.219     0
##  5  51.3  0.129      0   56.2   0.619      -0.777 0.315     1
##  6  67.2  1.72       1   63.9   1.39       -0.888 0.292     1
##  7  54.6  0.461      1   35.1  -1.49        0.595 0.644     0
##  8  37.3 -1.27       1   56.4   0.639      -1.58  0.171     0
##  9  43.1 -0.687      1   53.7   0.375      -1.16  0.238     0
## 10  45.5 -0.446      1   53.7   0.370      -1.06  0.257     0
## # … with 9,990 more rows</code></pre>
<p>In this case, <code>A=1</code> represents treatment <em>A</em> and <code>A=0</code> represents treatment <em>B</em>. Again, this is the actual treatment we would observe the patient receiving in a sample.</p>
</div>
<div id="trueoverlapweight" class="section level2">
<h2>Compute the (true) overlap weight</h2>
<p>We need to define what the overlap weight is, and it’s actually quite simple: just assign the patient the probability they <em>did not</em> receive their observed treatment.</p>
<p><span class="math display">\[
\begin{equation}
OW_i=
    \begin{cases}
        1-p_i^A &amp; \text{if } A_i=1\\
        p_i^A &amp; \text{if } A_i=0\\
    \end{cases}
\end{equation}
\]</span></p>
<p>Notice the notation. Since it depends on the <a href="#treatmentpropensity">true propensity score</a>, the overlap weight is another <em>unknown</em> quantity that is estimated from the data. It also depends on the <a href="#treatmentassignment">realized treatment assignment</a>, so the collection of weights across patients differ depending on the observed treatment distribution. We can add these weights to the <code>population</code> data set:</p>
<pre class="r"><code>population &lt;-
  population |&gt;
  
  # Add the true overlap weight for the realized treatment (unknown quantity)
  mutate(
    OW = A * (1-pA) + (1-A) * pA
  )
population</code></pre>
<pre><code>## # A tibble: 10,000 × 9
##      age    zage  male income zincome log_odds_pA    pA     A    OW
##    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;
##  1  44.4 -0.560      0   36.5  -1.35        0.311 0.577     1 0.423
##  2  47.7 -0.230      1   44.2  -0.579      -0.315 0.422     0 0.422
##  3  65.6  1.56       1   41.4  -0.861       0.606 0.647     1 0.353
##  4  50.7  0.0705     1   59.7   0.973      -1.27  0.219     0 0.219
##  5  51.3  0.129      0   56.2   0.619      -0.777 0.315     1 0.685
##  6  67.2  1.72       1   63.9   1.39       -0.888 0.292     1 0.708
##  7  54.6  0.461      1   35.1  -1.49        0.595 0.644     0 0.644
##  8  37.3 -1.27       1   56.4   0.639      -1.58  0.171     0 0.171
##  9  43.1 -0.687      1   53.7   0.375      -1.16  0.238     0 0.238
## 10  45.5 -0.446      1   53.7   0.370      -1.06  0.257     0 0.257
## # … with 9,990 more rows</code></pre>
<p>In a real analysis, these weights are what we are ultimately after in order to estimate the average <em>causal</em> treatment effect on the outcome of interest while balancing differences in patient characteristics across the groups (i.e., removing the confounding factors).</p>
<div id="targetpopulation" class="section level3">
<h3>Target population</h3>
<p>Now we do end up normalizing the weights so they sum to one within each treatment group. But the intuition about what’s happening is that the focus of the treatment effect estimation is shifted to patients that are most likely in <em>either</em> treatment group. This is known as the <a href="https://www.sciencedirect.com/topics/medicine-and-dentistry/clinical-equipoise#:~:text=Clinical%20equipoise%20is%20defined%20as,Seminars%20in%20Vascular%20Surgery%2C%202022"><em>clinical equipoise</em></a>, and the resulting treatment effect is interpreted as the <em>average treatment effect in the overlap population</em>.</p>
<p>This makes a lot of sense. When we’re comparing treatments, we should focus most heavily on patients that could be candidates for either, and less so on patients that were bound for one. Thus, we up-weight patients who have the most overlap in characteristics with the opposing treatment group. The beautiful thing here is that we do this without throwing away any information; smoothly and proportionately weighting each subject just the amount that we should.</p>
</div>
</div>
<div id="treatmenteffect" class="section level2">
<h2>Set the treatment effect</h2>
<p>We’ve <a href="#treatmentassignment">generated the treatments</a> and established how they are <a href="#treatmentpropensity">related to patient characteristics</a>, but haven’t talked about the outcome in which we’re ultimately interested in estimating the treatment effect for. Since we’re focusing on <a href="https://www.publichealth.columbia.edu/research/population-health-methods/time-event-data-analysis">time-to-event</a> outcomes, we’ll stay in that context, but the general idea is that we assume there <em>exists</em> a realization of what a patient’s outcome would have been under each treatment scenario. Then if we compare the difference of those outcomes across all patients, the average difference must be caused by the treatment.</p>
<div id="defining-the-event-times" class="section level3">
<h3>Defining the event times</h3>
<p>In this simulation, we’ll generate event times from a <a href="https://en.wikipedia.org/wiki/Weibull_distribution">Weibull</a> distribution. Starting at treatment initiation, this might be the time until cancer recurrence, hospitalization, or something else; we’re just looking to see how long it takes for some event to occur. The <a href="https://en.wikipedia.org/wiki/Probability_density_function">PDF</a> for this distribution looks like this:</p>
<p><span class="math display">\[f(t) = \frac{\alpha}{\sigma}\left(\frac{t}{\sigma}\right)^{\alpha-1}e^{-\left(\frac{t}{\sigma}\right)^\alpha}\]</span></p>
<p>where <span class="math inline">\(t\)</span> is the event time, <span class="math inline">\(\alpha\)</span> is the <em>shape</em> parameter, and <span class="math inline">\(\sigma\)</span> is the <em>scale</em> parameter. In our example, we will say that:</p>
<p><span class="math display">\[T_i^A \sim Weibull(\alpha, \sigma_i^A)\]</span>
<span class="math display">\[T_i^B \sim Weibull(\alpha, \sigma_i^B)\]</span>
where <span class="math inline">\(T_i^A\)</span> and <span class="math inline">\(T_i^B\)</span> are the event times under treatments <em>A</em> and <em>B</em>, respectively. That is, the event times for each patient under each treatment follow a <a href="">Weibull</a> distribution with a common <em>shape</em> parameter (in this case, across both treatments), but a <em>scale</em> parameter that depends on the patient’s specific characteristics and differs by treatment, which captures the treatment effect. Specifically,</p>
<p><span class="math display">\[\alpha = 1\]</span>
<span class="math display">\[\lambda = 4055.56\]</span></p>
<p><span class="math display">\[\phi_i = 1.10 \times age_z - 0.22 \times male - 0.36 \times income_z\]</span>
<span class="math display">\[\sigma_i^A = \lambda \times e^{-(\phi_i - 0.36)}\]</span>
<span class="math display">\[\sigma_i^B = \lambda \times e^{-\phi_i}\]</span></p>
<p>Basically, the <span class="math inline">\(\phi_i\)</span> term does the baseline adjustment on the outcome risk for the patient’s specific characteristics, and the treatment effect is simply a fixed, additive deviation from that for all patients. The <span class="math inline">\(\lambda\)</span> term is the <a href="https://www.linkedin.com/advice/0/how-do-you-interpret-hazard-ratio-baseline-function">baseline hazard function</a>, providing the <em>scale</em> parameter when all covariate values are zero (for treatment <em>B</em>), which in this case is constant over time.</p>
</div>
<div id="samplingeventtimes" class="section level3">
<h3>Sampling the event times</h3>
<p>It’s not immediately clear how to <em>interpret</em> the treatment effect we set (i.e., <em>-0.36</em>) in a meaningful way since complexities arise once these parameters are plugged back into the <a href="https://en.wikipedia.org/wiki/Probability_density_function">PDF</a>, but we can do some simple visualization to gain intuition. First, we need to actually sample the times for our <code>population</code> (<em>note that it was confirmed that SAS and R have the same parameterizations</em>):</p>
<pre class="r"><code>population &lt;-
  population |&gt;
  
  # Sample outcome event times under each treatment (counterfactual)
  mutate(
    
    # Baseline hazard (constant over time)
    lambda = 1*365/0.09,
    
    # Define TRUE Weibull linear predictor
    phi = log(3) * zage + log(0.8) * male + log(.70) * zincome,
    sigma_A = lambda * exp(-(phi + log(0.70))),
    sigma_B = lambda * exp(-phi),
    
    # Generate REALIZED survival times under each treatment scenario (same parameterizations in SAS)
    t_A = rweibull(n, shape = 1, scale = sigma_A),
    t_B = rweibull(n, shape = 1, scale = sigma_B), 
  )
population |&gt; select(lambda, phi, sigma_A, sigma_B, t_A, t_B)</code></pre>
<pre><code>## # A tibble: 10,000 × 6
##    lambda     phi sigma_A sigma_B    t_A   t_B
##     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
##  1  4056. -0.133    6617.   4632.  5689. 7760.
##  2  4056. -0.269    7585.   5309.  1831. 9482.
##  3  4056.  1.80      961.    673.   488.  300.
##  4  4056. -0.493    9482.   6637. 23722.  255.
##  5  4056. -0.0788   6269.   4388.  7649. 1800.
##  6  4056.  1.17     1804.   1263.  1953. 1493.
##  7  4056.  0.814    2568.   1797.  5462. 2652.
##  8  4056. -1.84    36514.  25560. 38262. 2385.
##  9  4056. -1.11    17604.  12323.  3577. 6894.
## 10  4056. -0.845   13485.   9439. 38541. 3743.
## # … with 9,990 more rows</code></pre>
<p>Now we can make some density plots comparing the event time distributions across treatments (we’ll use <em>log</em> scaling for visual appeal):</p>
<pre class="r"><code>population |&gt;
  
  # Send down the rows
  pivot_longer(
    cols = starts_with(&quot;t_&quot;),
    names_to = &quot;Treatment&quot;,
    values_to = &quot;EventTime&quot;,
    names_prefix = &quot;t_&quot;
  ) |&gt; 
  
  # Make a plot
  ggplot() + 
  geom_density(
    aes(
      x = log(EventTime),
      fill = Treatment
    ),
    alpha = .75
  ) + 
  scale_x_continuous(labels = function(x) round(exp(x))) +
  theme(
    panel.background = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    legend.position = &quot;top&quot;
  ) +
  xlab(&quot;Event Time&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Notice the shift to the right in the distribution for treatment <em>A</em>. This is the treatment effect. In the hypothetical world where all patients received treatment <em>A</em>, the event times happened <em>later</em> than in the world where all patients received treatment <em>B</em> (and all event times were observed), indicating, on average, a benefit to treatment <em>A</em>.</p>
</div>
</div>
<div id="observedoutcome" class="section level2">
<h2>Assign the observed outcome</h2>
<p>The final step for simulation setup is to assign the event time as we might observe in a real data set. The event times generated in the <a href="#samplingeventtimes">previous section</a> capture both treatment scenarios for all patients, but in reality we would only (potentially) observe the event time for the <a href="#treatmentassignment">treatment the patient received</a>. Additionally, we likely (or definitely) won’t be following all patients long enough to observe all events occur, meaning some patients will be <a href="https://en.wikipedia.org/wiki/Survival_analysis#:~:text=Censoring%20%2F%20Censored%20observation%3A%20Censoring%20occurs,after%20the%20time%20of%20censoring.">censored</a>. Let’s add the observed outcomes to the <code>population</code>:</p>
<pre class="r"><code>population &lt;-
  population |&gt; 
  
  # Add the observed outcome
  mutate(
    
    # The ACTUAL event time outcome depends on the treatment ACTUALLY observed for the patient
    actual_event_time = t_B * (1 - A) + t_A * A,
    
    # Generate a REALIZED censoring time (completely random)
    censor_time = 500 + 500 * runif(n),
    
    # Calculate the OBSERVED time in the data set (known quantity)
    time = pmin(actual_event_time, censor_time), # They either had the event, or were censored first
    
    # Calculate the event status (TRUE if event observed, FALSE if censored)
    status = as.numeric(actual_event_time &lt; censor_time)
  )
population |&gt; select(actual_event_time, censor_time, time, status)</code></pre>
<pre><code>## # A tibble: 10,000 × 4
##    actual_event_time censor_time  time status
##                &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1             5689.        843.  843.      0
##  2             9482.        814.  814.      0
##  3              488.        882.  488.      1
##  4              255.        936.  255.      1
##  5             7649.        653.  653.      0
##  6             1953.        890.  890.      0
##  7             2652.        737.  737.      0
##  8             2385.        703.  703.      0
##  9             6894.        952.  952.      0
## 10             3743.        526.  526.      0
## # … with 9,990 more rows</code></pre>
<p>Our final simulated data set has the following <em>observed</em> outcome summaries:</p>
<pre class="r"><code>population |&gt;
  
  # Compute summary metrics
  summarize(
    Patients = n(),
    Time = mean(time),
    .by = c(A, status)
  ) |&gt; 
  
  # Add shares within treatment
  mutate(
    Percent = Patients / sum(Patients),
    .by = A
  ) |&gt;
  
  # Clean/rearrange
  transmute(
    Treatment = 
      case_when(
        A == 1 ~ &quot;A&quot;,
        TRUE ~ &quot;B&quot;
      ),
    Status = 
      case_when(
        status == 1 ~ &quot;Event&quot;,
        TRUE ~ &quot;Censored&quot;
      ),
    Patients,
    Percent,
    Time
  ) |&gt;
  arrange(Treatment, desc(Status)) |&gt;
  
  # Make a table
  reactable(
    groupBy = &quot;Treatment&quot;,
    columns = 
      list(
        Patients = colDef(aggregate = &quot;sum&quot;, align = &quot;center&quot;),
        Percent = colDef(name = &quot;Percent of patients in treatment group (%)&quot;, aggregate = &quot;sum&quot;, align = &quot;center&quot;, format = colFormat(digits = 1, percent = TRUE)),
        Time = colDef(name = &quot;Avg. time to outcome&quot;, aggregate = zildge::rectbl_agg_wtd(&quot;Patients&quot;), align = &quot;center&quot;, format = colFormat(digits = 1))
      ),
    striped = TRUE,
    highlight = TRUE,
    bordered = TRUE,
    resizable = TRUE,
    theme = reactablefmtr::sandstone()
  ) |&gt;
  reactablefmtr::add_source(&quot;Use arrows to expand table&quot;, font_size = 12, font_style = &quot;italic&quot;)</code></pre>
<div id="htmlwidget-1" class="reactable html-widget" style="width:auto;height:auto;"></div>
<p style="color:#000;background:#FFFFFF;text-align:left;font-size:12px;font-style:italic;font-weight:normal;text-decoration:;letter-spacing:px;word-spacing:px;text-transform:;text-shadow:;margin-top:0px;margin-right:0px;margin-bottom:0px;margin-left:0px">Use arrows to expand table</p>
<script type="application/json" data-for="htmlwidget-1">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"Treatment":["A","A","B","B"],"Status":["Event","Censored","Event","Censored"],"Patients":[836,3046,1137,4981],"Percent":[0.215352910870685,0.784647089129315,0.185845047401111,0.814154952598888],"Time":[345.503703338068,745.228837321148,343.475165801799,746.272923924016]},"columns":[{"id":"Treatment","name":"Treatment","type":"character"},{"id":"Status","name":"Status","type":"character"},{"id":"Patients","name":"Patients","type":"numeric","aggregate":"sum","align":"center"},{"id":"Percent","name":"Percent of patients in treatment group (%)","type":"numeric","aggregate":"sum","format":{"cell":{"digits":1,"percent":true},"aggregated":{"digits":1,"percent":true}},"align":"center"},{"id":"Time","name":"Avg. time to outcome","type":"numeric","aggregate":"function(values, rows) {\n            var numerator = 0\n            var denominator = 0\n\n            rows.forEach(function(row, index) {\n                numerator += row['Patients'] * values[index]\n                denominator += row['Patients']\n            })\n\n            if('mean' == 'mean') {\n                return numerator / denominator\n            } else {\n                return numerator\n            }\n        }","format":{"cell":{"digits":1},"aggregated":{"digits":1}},"align":"center"}],"groupBy":["Treatment"],"resizable":true,"highlight":true,"bordered":true,"striped":true,"theme":{"color":"#3e3f3a","backgroundColor":"#ffffff","borderColor":"#f8f5f0","borderWidth":"1px","stripedColor":"#ededed","highlightColor":"#f8f5f0","cellPadding":6,"tableStyle":{"fontSize":15},"headerStyle":{"borderWidth":"2px","backgroundColor":"#f8f5f0","color":"#7c7a78","transitionDuration":"0.5s","&:hover[aria-sort]":{"color":"#000000"},"&[aria-sort='ascending'], &[aria-sort='descending']":{"color":"#000000"},"fontSize":16},"groupHeaderStyle":{"&:not(:empty)":{"color":"#3e3f3a","fontSize":16},"&:hover":{"fontWeight":"bold","transitionDuration":"1s","transitionTimingFunction":"ease-out","color":"#000000"}},"rowSelectedStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84"},"inputStyle":{"backgroundColor":"#ffffff","borderColor":"#bcbfc1","color":"#3e3f3a"},"searchInputStyle":{"backgroundColor":"#ffffff","color":"#3e3f3a","borderColor":"#bcbfc1","&:focus":{"color":"#3e3f3a"}},"selectStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84","borderColor":"#ffffff","outlineColor":"#ffffff"},"pageButtonStyle":{"backgroundColor":"#f8f5f0","color":"#8e8c84","&:hover":{"backgroundColor":"#f3969a","color":"#8e8c84"}},"pageButtonHoverStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84"},"pageButtonActiveStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84"},"pageButtonCurrentStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84"}},"dataKey":"c7e4627927bc259cbc5e7bcd101044f6"},"children":[]},"class":"reactR_markup"},"evals":["tag.attribs.columns.4.aggregate"],"jsHooks":[]}</script>
<p><br></p>
<pre class="r"><code>population |&gt;
  
  # Nest by treatment
  group_by(A) |&gt;
  nest() |&gt;
  
  # Esimtate survival curves for each treatment
  mutate(
    Surv = 
      data |&gt;
      map(
        function(.trt) {
          
          # Fit the model
          mod &lt;- survfit(Surv(time, status) ~ 1, data = .trt)
          
          # Extract the elements
          tibble(
            Time = mod$time,
            Survival = mod$surv,
            Lower = mod$lower,
            Upper = mod$upper
          )
          
        }
      )
  ) |&gt; 
  
  # Unnest the curves
  select(-data) |&gt;
  unnest(cols = Surv) |&gt;
  ungroup() |&gt;

  # Clean labels
  mutate(
    Treatment = 
      case_when(
        A == 1 ~ &quot;A&quot;,
        TRUE ~ &quot;B&quot;
      )
  ) |&gt;
  
  # Make a plot
  ggplot(
    aes(
      x = Time,
      y = Survival
    )
  ) + 
  geom_line(aes(color = Treatment)) +
  geom_ribbon(
    aes(
      ymin = Lower,
      ymax = Upper,
      fill = Treatment
    ),
    alpha = .25
  ) +
  scale_y_continuous(labels = scales::percent) +
  theme(
    panel.background = element_blank(),
    panel.grid.major.y = element_line(color = &quot;gray&quot;),
    legend.position = &quot;top&quot;
  ) +
  xlab(&quot;Time since treatment&quot;) +
  ylab(&quot;Survival Probability&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We don’t know how much of these differences are explained by the treatment. And actually, the crude survival suggests a possible benefit to treatment <em>B</em>, even though we know there is a <a href="#treatmenteffect">benefit to treatment <em>A</em></a>. The goal is to use this <em>known</em> information (along with <a href="#simulatepatients">patient characteristics</a>) to estimate the causal treatment effect that is baked into all of the <em>unknown</em> quantities we have here that would be unavailable to us in a real-life analysis.</p>
</div>
</div>
<div id="estimatingweights" class="section level1">
<h1>Estimating the weights</h1>
<p>Now that our <a href="#potentialoutcomes">simulation is set</a>, we can start using the observed data (i.e., known quantities) to estimate the case-weights needed for treatment effect estimation. This is the process we’d go through in a real-life analysis, but here we have the benefit of knowing the truth, so we can see how close our estimations are to reality.</p>
<p>The intention of these weights is to <em>adjust</em> the observed sample to create a <em>pseudo-population</em> such that the patient characteristics that we believe are confounding the treatment effect are corrected for, or <em>balanced</em>, across the treatment groups. The term “pseudo” here is a little misleading. It’s simply referring to the fact that each patient will not contribute the same weight in estimating the treatment effect. In fact, the patients with the most <em>overlap</em> in characteristics across the treatment groups will contribute the most, with patients being proportionately down-weighted the less overlap they have (I’d argue this is the same thing that is done in <a href="https://en.wikipedia.org/wiki/Propensity_score_matching">matching</a>, it’s just that the weights for patients are either exactly <em>1</em> or <em>0</em>). This is done to tease out the portion of the outcome differences that is explained by the treatment, namely, the causal treatment effect.</p>
<div id="modelpropensityscores" class="section level2">
<h2>Model the propensity scores</h2>
<p>We’ve <a href="#trueoverlapweight">established</a> that the overlap weights are quantities that need to be estimated from the data. In order to calculate those weights, we first need to estimate the propensity scores. We know what the <a href="#treatmentpropensity">true model</a> is, but let’s assume we are only working with observable data:</p>
<pre class="r"><code>population |&gt; select(zage, male, zincome, A, time, status)</code></pre>
<pre><code>## # A tibble: 10,000 × 6
##       zage  male zincome     A  time status
##      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1 -0.560      0  -1.35      1  843.      0
##  2 -0.230      1  -0.579     0  814.      0
##  3  1.56       1  -0.861     1  488.      1
##  4  0.0705     1   0.973     0  255.      1
##  5  0.129      0   0.619     1  653.      0
##  6  1.72       1   1.39      1  890.      0
##  7  0.461      1  -1.49      0  737.      0
##  8 -1.27       1   0.639     0  703.      0
##  9 -0.687      1   0.375     0  952.      0
## 10 -0.446      1   0.370     0  526.      0
## # … with 9,990 more rows</code></pre>
<p>Our goal is to obtain patient-specific probabilities of receiving treatment <em>A</em>. We’ll estimate this with a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> model, but we’ll allow for some flexibility in the shape of the relationships between the continuous variables (age and income) and the outcome with the use of <a href="https://www.nature.com/articles/s41409-019-0679-x">restricted cubic splines</a> (see my <a href="https://www.zajichekstats.com/post/the-evasive-spline/">other post</a> for another explanation):</p>
<pre class="r"><code># Fit the propensity score model
ps_mod &lt;-
  glm(
    formula = A ~ rms::rcs(zage, 3) + rms::rcs(zincome, 3) + factor(male),
    data = population,
    family = &quot;binomial&quot;
  )
summary(ps_mod)</code></pre>
<pre><code>## 
## Call:
## glm(formula = A ~ rms::rcs(zage, 3) + rms::rcs(zincome, 3) + 
##     factor(male), family = &quot;binomial&quot;, data = population)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0098  -0.9636  -0.6644   1.1489   2.3504  
## 
## Coefficients:
##                              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                  -0.30027    0.06527  -4.600 4.22e-06 ***
## rms::rcs(zage, 3)zage         0.40206    0.05099   7.885 3.16e-15 ***
## rms::rcs(zage, 3)zage&#39;       -0.07337    0.05813  -1.262    0.207    
## rms::rcs(zincome, 3)zincome  -0.63792    0.04950 -12.888  &lt; 2e-16 ***
## rms::rcs(zincome, 3)zincome&#39; -0.04020    0.06434  -0.625    0.532    
## factor(male)1                -0.25432    0.04415  -5.760 8.41e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 13359  on 9999  degrees of freedom
## Residual deviance: 12193  on 9994  degrees of freedom
## AIC: 12205
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The model seems to indicate that the non-linear terms for age and income don’t particularly matter (which <a href="#treatmentpropensity">is expected</a>), but we’ll leave it as-is. Next, let’s attach the <em>estimated</em> propensity scores to the <code>population</code>:</p>
<pre class="r"><code>population &lt;-
  population |&gt;
  
  # Add the estimated propensity score
  mutate(
    pA_hat = predict(ps_mod, type = &quot;response&quot;) # P(A = 1 | X)
  )</code></pre>
<p>We can take a look at the estimated propensity score distribution across the treatment groups (we’ll also overlay the <em>true</em> distributions for comparison):</p>
<pre class="r"><code>population |&gt;
  
  # Send PS scores down the rows
  pivot_longer(
    cols = c(pA, pA_hat),
    names_to = &quot;Type&quot;,
    values_to = &quot;Score&quot;
  ) |&gt;

  # Clean labels
  mutate(
    Treatment = 
      case_when(
        A == 1 ~ &quot;A&quot;,
        TRUE ~ &quot;B&quot;
      ),
    Type = 
      case_when(
        Type == &quot;pA&quot; ~ &quot;True&quot;,
        TRUE ~ &quot;Estimated&quot;
      )
  ) |&gt;
  
  # Make a plot
  ggplot() + 
  geom_density(
    aes(
      x = Score,
      fill = Treatment,
      linetype = Type
    ),
    alpha = .40
  ) +
  scale_x_continuous(labels = scales::percent) +
  theme(
    panel.background = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    legend.position = &quot;top&quot;
  ) +
  xlab(&quot;P(A=1|X)&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>The model does a great job at estimating the true propensity scores. In a real analysis we probably wouldn’t be this close since we likely wouldn’t have only and all true confounders accounted for.</p>
<div id="visualizing-propensity-score-effects" class="section level3">
<h3>Visualizing propensity score effects</h3>
<p>In the same vein, we can explore the modeled relationships between each patient characteristic and the propensity scores.</p>
<pre class="r"><code>plots &lt;-
  population |&gt;
  
  # Send PS scores down the rows
  pivot_longer(
    cols = c(pA, pA_hat),
    names_to = &quot;Type&quot;,
    values_to = &quot;Score&quot;
  ) |&gt;
  
  # Clean labels
  mutate(
    Treatment = 
      case_when(
        A == 1 ~ &quot;A&quot;,
        TRUE ~ &quot;B&quot;
      ),
    Type = 
      case_when(
        Type == &quot;pA&quot; ~ &quot;True&quot;,
        TRUE ~ &quot;Estimated&quot;
      )
  ) |&gt;
  
  # Send covariates down the rows
  pivot_longer(
    cols = c(zage, zincome, male),
    names_prefix = &quot;^z&quot;
  ) |&gt;
  
  # Make groups
  mutate(
    Group = 
      case_when(
        name == &quot;male&quot; ~ &quot;categorical&quot;,
        TRUE ~ &quot;continuous&quot;
      )
  ) |&gt;
  
  # Make a nested frame
  group_by(Group) |&gt;
  nest() |&gt;
  
  # Make plot depending on type
  mutate(
    plot = 
      data |&gt;
      map(
        function(.group) {
          
          # Check condition
          if(n_distinct(.group$value) == 2) {
            
            .group |&gt;
              summarize(
                Rate = mean(Score),
                .by = c(Treatment, Type, name, value)
              ) |&gt;
              mutate(
                Sex = 
                  case_when(
                    value == 1 ~ &quot;Male&quot;,
                    TRUE ~ &quot;Female&quot;
                  ),
                name = &quot;Sex&quot;
              ) |&gt;
              ggplot() +
              geom_point(
                aes(
                  x = Sex,
                  y = Rate,
                  color = Treatment,
                  shape = Type,
                  group = Type
                )
              ) +
              geom_line(
                aes(
                  x = Sex,
                  y = Rate,
                  color = Treatment,
                  linetype = Type,
                  group = interaction(Type, Treatment)
                )
              ) +
              facet_wrap(~name) +
              coord_flip() +
              scale_y_continuous(labels = scales::percent) +
              theme(
                panel.background = element_blank(),
                panel.grid.major.x = element_line(color = &quot;gray&quot;),
                legend.position = &quot;top&quot;,
                axis.title.y = element_blank()
              ) +
              ylab(&quot;P(A=1|X)&quot;)
            
          } else {
            
            .group |&gt; 
              ggplot() + 
              geom_smooth(
                aes(
                  x = value,
                  y = Score,
                  color = Treatment,
                  fill = Treatment,
                  linetype = Type
                ),
                alpha = .25
              ) +
              facet_wrap(~name) +
              scale_y_continuous(labels = scales::percent) +
              theme(
                panel.background = element_blank(),
                panel.grid.major.y = element_line(color = &quot;gray&quot;),
                legend.position = &quot;top&quot;
              ) +
              xlab(&quot;Z-Score&quot;) +
              ylab(&quot;P(A=1|X)&quot;)
          }
          
        } 
      )
  ) 

gridExtra::grid.arrange(plots$plot[[1]], plots$plot[[2]])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>As known from the <a href="#treatmentpropensity">true model</a>, patients who received treatment <em>A</em> tend to be older, female patients with lower income. We can also see slight miscalibration in the estimates for patients who are older in that the model tends to <em>underestimate</em> the true propensity score in these patients.</p>
</div>
</div>
<div id="estimatedoverlapweight" class="section level2">
<h2>Calculate the (estimated) overlap weight</h2>
<p>We’ve already <a href="#trueoverlapweight">defined</a> how to calculate the overlap weights. The only difference here is that we’ll do it from the <a href="#modelpropensityscores"><em>estimated</em></a> propensity scores instead of the <a href="#treatmentpropensity">true</a> ones. First, we’ll apply the formula to add the estimated weights to the <code>population</code>:</p>
<pre class="r"><code>population &lt;-
  population |&gt;
  
  # Add the estimated overlap weight for the realized treatment (known quantity)
  mutate(
    OW_hat = A * (1-pA_hat) + (1-A) * pA_hat
  )</code></pre>
<p>As we’ve <a href="#targetpopulation">mentioned earlier</a>, we will then <em>normalize</em> the weights <em>within</em> each treatment group so they have the same cumulative contribution for estimating the treatment effect in the outcome model.</p>
<pre class="r"><code>population &lt;- 
  population |&gt;
  
  # Add the normalized weights (adding true and estimated)
  mutate(
    OW_norm = OW / sum(OW),
    OW_hat_norm = OW_hat / sum(OW_hat),
    .by = A
  )
population |&gt; select(A, pA, pA_hat, OW, OW_norm, OW_hat, OW_hat_norm)</code></pre>
<pre><code>## # A tibble: 10,000 × 7
##        A    pA pA_hat    OW   OW_norm OW_hat OW_hat_norm
##    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;
##  1     1 0.577  0.583 0.423 0.000202   0.417   0.000198 
##  2     0 0.422  0.427 0.422 0.000201   0.427   0.000202 
##  3     1 0.647  0.610 0.353 0.000168   0.390   0.000185 
##  4     0 0.219  0.226 0.219 0.000105   0.226   0.000107 
##  5     1 0.315  0.329 0.685 0.000327   0.671   0.000318 
##  6     1 0.292  0.265 0.708 0.000338   0.735   0.000348 
##  7     0 0.644  0.628 0.644 0.000307   0.628   0.000297 
##  8     0 0.171  0.181 0.171 0.0000814  0.181   0.0000856
##  9     0 0.238  0.250 0.238 0.000114   0.250   0.000118 
## 10     0 0.257  0.268 0.257 0.000123   0.268   0.000127 
## # … with 9,990 more rows</code></pre>
<p>To get a sense of the impact of these weights, let’s first look at their distributions (again, adding the <a href="#trueoverlapweight">true</a> weights for comparison):</p>
<pre class="r"><code>population |&gt;
  
  # Send overlap weight down the rows
  pivot_longer(
    cols = c(OW_norm, OW_hat_norm),
    names_to = &quot;Type&quot;,
    values_to = &quot;Weight&quot;
  ) |&gt;
  
  # Clean labels
  mutate(
    Treatment = 
      case_when(
        A == 1 ~ &quot;A&quot;,
        TRUE ~ &quot;B&quot;
      ),
    Type = 
      case_when(
        Type == &quot;OW_norm&quot; ~ &quot;True&quot;,
        TRUE ~ &quot;Estimated&quot;
      )
  ) |&gt;
  
  # Make a plot
  ggplot() + 
  geom_density(
    aes(
      x = Weight,
      fill = Treatment,
      linetype = Type
    ),
    alpha = .40
  ) +
  theme(
    panel.background = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    legend.position = &quot;top&quot;
  ) +
  xlab(&quot;Normalized Overlap Weight&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>The distribution shift has to do with the sampled treatment distribution. Even though both groups will contribute equally to the outcome model in aggregate, only 38.8% of patients have treatment <em>A</em> so at an individual level will contribute more on average.</p>
<p>We can also look at the accumulation of patients (i.e., when all patients contribute equally) as a function of the cumulative overlap weight in each group. This allows us to understand how much (or little) the treatment effect estimation will be dominated by a smaller concentration of patients.</p>
<pre class="r"><code>population |&gt;
  
  # Send overlap weight down the rows
  pivot_longer(
    cols = c(OW_norm, OW_hat_norm),
    names_to = &quot;Type&quot;,
    values_to = &quot;Weight&quot;
  ) |&gt;
  
  # Clean labels
  mutate(
    Treatment = 
      case_when(
        A == 1 ~ &quot;A&quot;,
        TRUE ~ &quot;B&quot;
      ),
    Type = 
      case_when(
        Type == &quot;OW_norm&quot; ~ &quot;True&quot;,
        TRUE ~ &quot;Estimated&quot;
      ),
    NullWeight = 1
  ) |&gt; 
  
  # Rearrange
  arrange(Treatment, Type, Weight) |&gt;
  
  # Compute the cumulative weight
  mutate(
    Weight = cumsum(Weight) / sum(Weight),
    NullWeight = cumsum(NullWeight) / sum(NullWeight),
    .by = c(Treatment, Type)
  ) |&gt; 
  
  # Make a plot
  ggplot() + 
  geom_line(
    aes(
      x = Weight,
      y = NullWeight,
      color = Treatment,
      linetype = Type
    ),
    linewidth = 1
  ) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  theme(
    panel.background = element_blank(),
    panel.grid.major.x = element_line(color = &quot;gray&quot;),
    panel.grid.major.y = element_line(color = &quot;gray&quot;),
    legend.position = &quot;top&quot;
  ) +
  xlab(&quot;Cumulative percent of overlap weight (%)&quot;) +
  ylab(&quot;Cumulative percent of patients (%)&quot;) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>We can see that for treatment <em>B</em>, about 40% of the outcome model weight will be accounted for by only 25% of patients.</p>
<div id="weighted-mean-differences" class="section level3">
<h3>Weighted-mean differences</h3>
<p>It was <a href="#treatmentpropensity">previously mentioned</a> that the overlap weight methodology leads to perfect balance among the covariates used in the propensity score model. We can verify by looking at the group means for <a href="#modelpropensityscores">each model factor</a> before and after weighting.</p>
<pre class="r"><code>population |&gt; 
  
  # Add uniform weights
  add_column(NullWeight = 1) |&gt;
  
  # Send weights down the rows
  pivot_longer(
    cols = c(NullWeight, OW_hat_norm),
    names_to = &quot;Type&quot;,
    values_to = &quot;Weight&quot;
  ) |&gt; 
  
  # Send factors down the rows
  pivot_longer(
    cols = c(age, income, male),
    names_to = &quot;Factor&quot;,
    values_to = &quot;Value&quot;
  ) |&gt; 
  
  # For each 
  summarize(
    Patients = n(),
    Mean = sum(Weight * Value) / sum(Weight),
    .by = c(Factor, Type, A)
  ) |&gt;
  
  # Clean up
  transmute(
    Factor = 
      case_when(
        Factor == &quot;age&quot; ~ &quot;Age (years)&quot;,
        Factor == &quot;income&quot; ~ &quot;Income ($)&quot;,
        Factor == &quot;male&quot; ~ &quot;Male (%)&quot;
      ),
    Treatment = 
      case_when(
        A == 1 ~ &quot;A&quot;,
        TRUE ~ &quot;B&quot;
      ),
    Type,
    Patients,
    Mean = 
      case_when(
        Factor == &quot;Male (%)&quot; ~ 100 * Mean,
        TRUE ~ Mean
      )
  ) |&gt;
  
  # Send over the columns
  pivot_wider(
    names_from = Type,
    values_from = c(Patients, Mean)
  ) |&gt; 
  
  # Rearrange
  arrange(Factor, Treatment) |&gt;
  select(Factor, Treatment, ends_with(&quot;NullWeight&quot;), Mean_OW_hat_norm) |&gt; 
  
  # Make a table
  reactable(
    groupBy = &quot;Factor&quot;,
    columns = 
      list(
        Patients_NullWeight = colDef(name = &quot;Patients&quot;, aggregate = &quot;sum&quot;, align = &quot;center&quot;),
        Mean_NullWeight = colDef(name = &quot;Before Weighting&quot;, aggregate = zildge::rectbl_agg_wtd(&quot;Patients_NullWeight&quot;), align = &quot;center&quot;, format = colFormat(digits = 1)),
        Mean_OW_hat_norm = colDef(name = &quot;After Weighting&quot;, aggregate = zildge::rectbl_agg_wtd(&quot;Patients_NullWeight&quot;), align = &quot;center&quot;, format = colFormat(digits = 1))
      ),
    columnGroups = 
      list(
        colGroup(
          name = &quot;Mean Value&quot;,
          columns = c(&quot;Mean_NullWeight&quot;, &quot;Mean_OW_hat_norm&quot;)
        )
      ),
    striped = TRUE,
    highlight = TRUE,
    bordered = TRUE,
    resizable = TRUE,
    theme = reactablefmtr::sandstone()
  ) |&gt;
  reactablefmtr::add_source(&quot;Use arrows to expand table&quot;, font_size = 12, font_style = &quot;italic&quot;)</code></pre>
<div id="htmlwidget-2" class="reactable html-widget" style="width:auto;height:auto;"></div>
<p style="color:#000;background:#FFFFFF;text-align:left;font-size:12px;font-style:italic;font-weight:normal;text-decoration:;letter-spacing:px;word-spacing:px;text-transform:;text-shadow:;margin-top:0px;margin-right:0px;margin-bottom:0px;margin-left:0px">Use arrows to expand table</p>
<script type="application/json" data-for="htmlwidget-2">{"x":{"tag":{"name":"Reactable","attribs":{"data":{"Factor":["Age (years)","Age (years)","Income ($)","Income ($)","Male (%)","Male (%)"],"Treatment":["A","B","A","B","A","B"],"Patients_NullWeight":[3882,6118,3882,6118,3882,6118],"Mean_NullWeight":[51.8724351584409,48.7731344443606,46.1525421379549,52.1470439747779,55.7444616177228,61.7522066034652],"Mean_OW_hat_norm":[50.690947042348,50.6909470423471,48.480546358988,48.4805463589894,58.1435359891369,58.1435359891386]},"columns":[{"id":"Factor","name":"Factor","type":"character"},{"id":"Treatment","name":"Treatment","type":"character"},{"id":"Patients_NullWeight","name":"Patients","type":"numeric","aggregate":"sum","align":"center"},{"id":"Mean_NullWeight","name":"Before Weighting","type":"numeric","aggregate":"function(values, rows) {\n            var numerator = 0\n            var denominator = 0\n\n            rows.forEach(function(row, index) {\n                numerator += row['Patients_NullWeight'] * values[index]\n                denominator += row['Patients_NullWeight']\n            })\n\n            if('mean' == 'mean') {\n                return numerator / denominator\n            } else {\n                return numerator\n            }\n        }","format":{"cell":{"digits":1},"aggregated":{"digits":1}},"align":"center"},{"id":"Mean_OW_hat_norm","name":"After Weighting","type":"numeric","aggregate":"function(values, rows) {\n            var numerator = 0\n            var denominator = 0\n\n            rows.forEach(function(row, index) {\n                numerator += row['Patients_NullWeight'] * values[index]\n                denominator += row['Patients_NullWeight']\n            })\n\n            if('mean' == 'mean') {\n                return numerator / denominator\n            } else {\n                return numerator\n            }\n        }","format":{"cell":{"digits":1},"aggregated":{"digits":1}},"align":"center"}],"columnGroups":[{"name":"Mean Value","columns":["Mean_NullWeight","Mean_OW_hat_norm"]}],"groupBy":["Factor"],"resizable":true,"highlight":true,"bordered":true,"striped":true,"theme":{"color":"#3e3f3a","backgroundColor":"#ffffff","borderColor":"#f8f5f0","borderWidth":"1px","stripedColor":"#ededed","highlightColor":"#f8f5f0","cellPadding":6,"tableStyle":{"fontSize":15},"headerStyle":{"borderWidth":"2px","backgroundColor":"#f8f5f0","color":"#7c7a78","transitionDuration":"0.5s","&:hover[aria-sort]":{"color":"#000000"},"&[aria-sort='ascending'], &[aria-sort='descending']":{"color":"#000000"},"fontSize":16},"groupHeaderStyle":{"&:not(:empty)":{"color":"#3e3f3a","fontSize":16},"&:hover":{"fontWeight":"bold","transitionDuration":"1s","transitionTimingFunction":"ease-out","color":"#000000"}},"rowSelectedStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84"},"inputStyle":{"backgroundColor":"#ffffff","borderColor":"#bcbfc1","color":"#3e3f3a"},"searchInputStyle":{"backgroundColor":"#ffffff","color":"#3e3f3a","borderColor":"#bcbfc1","&:focus":{"color":"#3e3f3a"}},"selectStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84","borderColor":"#ffffff","outlineColor":"#ffffff"},"pageButtonStyle":{"backgroundColor":"#f8f5f0","color":"#8e8c84","&:hover":{"backgroundColor":"#f3969a","color":"#8e8c84"}},"pageButtonHoverStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84"},"pageButtonActiveStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84"},"pageButtonCurrentStyle":{"backgroundColor":"#dfd7ca","color":"#8e8c84"}},"dataKey":"d59eb5927b49690a0e75cffae96dbc04"},"children":[]},"class":"reactR_markup"},"evals":["tag.attribs.columns.3.aggregate","tag.attribs.columns.4.aggregate"],"jsHooks":[]}</script>
</div>
<div id="confounder-weight-shifts" class="section level3">
<h3>Confounder weight shifts</h3>
<p>Similar to looking at weight attributions <a href="#estimatedoverlapweight">as a whole</a>, we can explore weight shifts within subgroups of patient characteristics. This helps build intuition about which patients the subsequent treatment effect estimates will focus on most (and least).</p>
<pre class="r"><code>population |&gt; 
  
  # Add uniform weights
  add_column(NullWeight = 1) |&gt;
  
  # Convert to quntiles
  mutate(
    across(
      c(age, income),
      \(x) Hmisc::cut2(x, g = 10)
    ),
    Age = age,
    Income = income,
    Sex = 
      case_when(
        male == 1 ~ &quot;Male&quot;,
        TRUE ~ &quot;Female&quot;
      ),
    Treatment = 
      case_when(
        A == 1 ~ &quot;A&quot;,
        TRUE ~ &quot;B&quot;
      )
  ) |&gt; 
  
  # Send weights down the rows
  pivot_longer(
    cols = c(NullWeight, OW_hat_norm),
    names_to = &quot;Type&quot;,
    values_to = &quot;Weight&quot;
  ) |&gt;
  
  # Send factors down the rows
  pivot_longer(
    cols = c(Age, Income, Sex),
    names_to = &quot;Factor&quot;,
    values_to = &quot;Level&quot;
  ) |&gt; 
  
  # Compute total weights
  summarize(
    Weight = sum(Weight),
    .by = c(Factor, Level, Type, Treatment)
  ) |&gt;
  
  # Find percent of weight
  mutate(
    Weight = Weight / sum(Weight),
    .by = c(Factor, Type, Treatment)
  ) |&gt;
  
  # Send over columns
  pivot_wider(
    names_from = Type,
    values_from = Weight
  ) |&gt;
  
  # Clean up
  mutate(
    Level = factor(Level) |&gt; fct_rev(),
    Factor = 
      case_when(
        Factor == &quot;Age&quot; ~ &quot;Age (years)&quot;,
        Factor == &quot;Income&quot; ~ &quot;Income ($)&quot;,
        TRUE ~ Factor
      )
  ) |&gt;
  
  # Make a plot
  ggplot() +
  geom_col(
    aes(
      x = Level,
      y = NullWeight,
      fill = Treatment
    ),
    color = &quot;black&quot;,
    linewidth = .75,
    alpha = .40,
    position = &quot;dodge&quot;
  ) + 
  geom_point(
    aes(
      x = Level,
      y = OW_hat_norm,
      color = Treatment,
      shape = Treatment
    ),
    position = position_dodge(width = 1),
    size = 3
  ) +
  geom_line(
    aes(
      x = Level,
      y = OW_hat_norm,
      color = Treatment,
      group = Treatment
    ),
    position = position_dodge(width = 1),
    linewidth = .5
  ) +
  facet_wrap(~Factor, scales = &quot;free&quot;) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  theme(
    panel.background = element_blank(),
    panel.grid.major.x = element_line(color = &quot;gray&quot;),
    legend.position = &quot;top&quot;
  ) +
  xlab(&quot;Level&quot;) +
  ylab(&quot;Share of weight within group (%)&quot;) +
  labs(
    fill = &quot;Before Weighting&quot;,
    shape = &quot;After Weighting&quot;,
    color = &quot;After Weighting&quot;
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>We can see the balancing take place. After overlap weighting, the contribution to treatment effect estimation is reduced in older patients with lower income for treatment <em>A</em>, and vice-versa for treatment <em>B</em>, with some levels in the middle having increased weight for <em>both</em> treatments. This is where the most overlap is between the groups.</p>
<p>It is also useful to look at these plots for factors that were <em>not</em> included in the propensity score model. We may find drastic weight shifts in these factors which might indicate significant co-variation among factors already accounted for.</p>
</div>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<ol style="list-style-type: decimal">
<li>All subjects are used instead of arbitrary cutoffs (e.g., 1:5 matching)</li>
<li>It proportionately down-weights the impacts of subjects most likely belonging to a certain treatment group</li>
<li>Achieves perfect balance in covariates across treatment groups instead of being arbitrarily close</li>
<li>Treatment groups equally contribute to the (weighted) estimation in the outcome model</li>
<li>The weights themselves can be used to gain insight about the imbalance in the sample and intuition about their impacts on the estimation</li>
<li>Avoids extreme case weights by being bounded between 0 and 1</li>
<li>It has <a href="https://academic.oup.com/aje/article/188/1/250/5090958">been shown</a> to produce more unbiased and efficient estimates than other inclusive weighting approaches like <a href="https://en.wikipedia.org/wiki/Inverse_probability_weighting">inverse probability weighting (IPW)</a></li>
</ol>
<pre class="r"><code>dat %&gt;%
  plot_ly(
    x = ~PS,
    type = &quot;histogram&quot;
  ) %&gt;%
  layout(
    xaxis = list(title = &quot;True Propensity Score (PS)&quot;, tickformat = &quot;2%&quot;),
    yaxis = list(title = &quot;Patients&quot;)
  )</code></pre>
<pre class="r"><code>ggplotly(
  dat %&gt;%
    
    # Make age groups
    mutate(
      Age = Hmisc::cut2(Age, g = 5)
    ) %&gt;%
    
    # For each group
    group_by(Age, Sex) %&gt;%
    
    # Compute the rate
    summarise(
      Patients = n(),
      Treated = sum(Treated),
      .groups = &quot;drop&quot;
    ) %&gt;%
    
    # Make the proportion
    mutate(
      Rate = Treated / Patients
    ) %&gt;%
    
    # Make a plot
    ggplot(
      aes(
        x = Age,
        y = Rate,
        color = Sex,
        group = Sex,
        text = 
          paste0(
            &quot;Patients: &quot;, Patients,
            &quot;&lt;br&gt;Treated: &quot;, Treated,
            &quot;&lt;br&gt;Percent Treated: &quot;, round(Rate * 100, 1), &quot;%&quot;
          )
      )
    ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(labels = scales::percent) +
    ylab(&quot;Percent Treated (%)&quot;),
  tooltip = &quot;text&quot;
)</code></pre>
<pre class="r"><code>dat %&gt;%
  plot_ly(
    x = ~OW_norm,
    y = ~factor(Treated),
    color = ~factor(Treated),
    type = &quot;box&quot;
  )</code></pre>
</div>
<div id="resources" class="section level1">
<h1>Resources</h1>
<p>The overlap weight is part of a general class of causal methodology/theory for balancing covariates to estimate treatment effects using propensity scores. More broadly, these fall into the <a href="https://en.wikipedia.org/wiki/Rubin_causal_model"><em>potential outcomes</em></a> framework of causal inference, which I happened to be introduced to by accident while learning about overlap weighting from these resources:</p>
<ul>
<li>Original paper for overlap weighting (<a href="https://pubmed.ncbi.nlm.nih.gov/30189042/">link</a>)</li>
<li>Paper extending overlap weighting to survival analysis (<a href="https://arxiv.org/abs/2108.04394">link</a>)</li>
<li>Example simulation of overlap weighting in the survival setting in SAS (<a href="http://www2.stat.duke.edu/~fl35/OW/OW_survival_Demo.sas">link</a>)</li>
<li>R package for implementing overlap weight methods (<a href="https://github.com/thuizhou/PSweight">link</a>)</li>
<li>R functions for overlap weighting in the survival setting (<a href="https://github.com/chaochengstat/OW_Survival">link</a>)</li>
<li>Author’s web page dedicated to overlap weighting (<a href="https://www2.stat.duke.edu/~fl35/OW.html">link</a>)</li>
</ul>
<p>All of these were super helpful. Understanding the idea of potential outcomes made the math and methodological concepts a lot easier to grasp conceptually and gain intuition. The way I like to think about it is (roughly) in the following order:</p>
<ol style="list-style-type: decimal">
<li>Each observation has a <em>true</em> propensity score (probability of treatment). The treatment we observe is just a realization of a random draw from that <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a> distribution (or <a href="https://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial</a> distribution if there are more treatments). We must estimate this from the treatment we observe.</li>
<li>Since there is a true propensity score, there is also a <em>true</em> overlap weight (for each treatment option) that balances the observations back with the population. We must also estimate this from the data.</li>
<li>The outcome we observe is conditional on the observed treatment. So there exists some true function (e.g., linear predictor) relating the observation to the outcome under the treatment we observed.</li>
<li>However, this also all exists for the treatment(s) we did <em>NOT</em> observe. So there are analogous functions relating the observation to the outcome under the alternate treatment scenarios.</li>
<li>If we could observe the outcome for all treatments, we could just compute the mean differences in the outcome across the treatment groups and that would be the causal treatment effect. I imagine a stacked data set where there is a row for each observation under each treatment, it’s just that the the outcome value (and other data) for the treatments that weren’t observed are missing, and our goal is to estimate what the mean differences in the outcomes would have been had that data not been missing. Thus we must use what we have to approximate that.</li>
</ol>
</div>
