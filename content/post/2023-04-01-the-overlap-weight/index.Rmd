---
title: The overlap weight
author: Alex Zajichek
date: '2023-04-01'
slug: the-overlap-weight
categories: []
tags: ['statistics', 'datascience', 'causation', 'rstats']
subtitle: 'A smoother, more inclusive approach to balancing'
summary: "It starts the same as propsensity score matching. It's what is done with the scores that differentiate the methods."
authors: []
lastmod: '2023-04-01T11:39:12-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

**_In progress_**

I was recently introduced to a method called [_overlap weighting_](https://jamanetwork.com/journals/jama/article-abstract/2765748). It is part of a family of methods for balancing covariates when estimating treatment effects in observational studies. Although I've just started getting into the details, I find it much more elegant than [propensity score (PS) matching](https://en.wikipedia.org/wiki/Propensity_score_matching) (which I've [used in the past](https://jamanetwork.com/journals/jama/fullarticle/2749478)) for many reasons:

1. All subjects are used instead of arbitrary cutoffs (e.g., 1:5 matching)
2. It proportionately down-weights the impacts of subjects most likely belonging to a certain treatment group
3. Achieves perfect balance in covariates across treatment groups instead of being arbitrarily close
4. Treatment groups equally contribute to the (weighted) estimation in the outcome model
5. The weights themselves can be used to gain insight about the imbalance in the sample and intuition about their impacts on the estimation
6. Avoids extreme case weights by being bounded between 0 and 1
7. It has [been shown](https://academic.oup.com/aje/article/188/1/250/5090958) to produce more unbiased and efficient estimates than other inclusive weighting approaches like [inverse probability weighting (IPW)](https://en.wikipedia.org/wiki/Inverse_probability_weighting)

It actually starts the same as the matching approach by deriving a propensity score. It's what is done with the scores that differentiate the methods. 

# Table of Contents

* [Defining the overlap weight](#definition)
* [Using the overlap weight](#usage)
* [Heart disease example](#example)
  + [Making an artificial treatment](#artificial)
  + [Fitting the propensity score model](#modelfit)
* [Resources](#resources)

# How is it defined? {#definition}

It's actually quite simple. Suppose you have treatments _A_ & _B_. Then,

$$P(T_i = A|X_i) = p_i$$
is the _true_ propensity score, or probability, for subject _i_ getting treatment _A_, given their specific set of characteristics, $X_i$, that contribute to the treatment imbalance.

So you build a model to estimate $p_i$ using the set of treatments that were actually observed. If you use [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), which is the common approach, then the estimated propensity score $\hat{p_i}$ is

$$p_i \approx \hat{p_i} = \frac{1}{1 + e^{-X_i\hat{\beta}}}$$
where $\hat{\beta}$ are the estimated regression coefficients. Then the (estimated) overlap weight, $\hat{OW_i}$, is calculated by:

$$
\begin{equation}
\hat{OW_i}=
    \begin{cases}
        1-\hat{p_i} & \text{if } T_i = A\\
        \hat{p_i} & \text{if } T_i = B\\
    \end{cases}
\end{equation}
$$
That's it. We just assign the probability that each subject _did not_ receive their respective treatment.

Now we do end up normalizing these weights so they sum to one within each treatment group. But the intuition about what's happening is that the focus of the downstream estimation is shifted to subjects that are most likely in _either_ treatment group (i.e., $\hat{p_i}$ near 0.5). 

This makes a lot of sense. When we're comparing treatments, we should focus most heavily on subjects that could be candidates for either one, and less so on subjects that were bound for one of the treatment options. The beautiful thing here is that we do this without throwing away any information; smoothly and proportionately weighting each subject just the amount that we should.

# How do I use it? {#usage}

In it's most basic usage, the treatment effect is estimated as the difference in average outcome values across treatment groups after subjects are weighted by their overlap weight.

$$\hat{Effect} = \frac{\sum_{i=1}^{N_A}\hat{OW_{A_i}} \times Y_{A_i}}{\sum_{i=1}^{N_A}\hat{OW_{A_i}}} - \frac{\sum_{i=1}^{N_B}\hat{OW_{B_i}} \times Y_{B_i}}{\sum_{i=1}^{N_B}\hat{OW_{B_i}}}$$

where $N$ is the sample size, $Y$ is the outcome value, and subscripts $A$ and $B$ indicate the set of subjects in the respective treatment groups.

and this provides an estimate of the _average treatment effect in the overlap population_ (ATO).

# Example {#example}

Sure. Let's first load some packages to get started.

```{r, message = FALSE}
library(tidyverse)
library(plotly)
```

We're going to be creating an artificial example using the `cheese::heart_disease` data set to estimate the effect of some treatment on the likelihood of heart disease. 

```{r}
dat <- cheese::heart_disease
print(dat, n = 5)
```

## Making an artificial treatment {#artificial}

There is not actually a treatment column in this data set, so we'll make one. Let's assume the following:

$$logit(P(T_i|X_i)) = 0.75Age_{std} + 0.5Male$$
That is, the probability of being _treated_ ($T_i$) is linearly related to the (standardized) age and sex of the patient. Further, older patients and male patients are more likely to be treated. 

```{r}
dat <- 
  dat %>%
  
  # Add the true propensity score
  mutate(
    PS = 1 / (1 + exp(-(.75 * ((Age - mean(Age))/sd(Age)) + .5 * (Sex == "Male"))))
  ) 
```

Let's look at the _true_ propensity score distribution (note these are never known).

```{r}
dat %>%
  plot_ly(
    x = ~PS,
    type = "histogram"
  ) %>%
  layout(
    xaxis = list(title = "True Propensity Score (PS)", tickformat = "2%"),
    yaxis = list(title = "Patients")
  )
```

Now, we'll generate the treatment indicator for each patient by taking a random draw from a Bernoulli distribution:

```{r}
set.seed(123)
dat <- 
  dat %>%
  
  # Assign treatment
  mutate(
    Treated = rbinom(n = nrow(dat), size = 1, prob = PS)
  )
```

Of the `r nrow(dat)` patients, `r paste0(sum(dat$Treated), " (", round(mean(dat$Treated) * 100, 1), "%)")` were treated. Let's look at the observed rate of treatment by age and sex.

```{r}
ggplotly(
  dat %>%
    
    # Make age groups
    mutate(
      Age = Hmisc::cut2(Age, g = 5)
    ) %>%
    
    # For each group
    group_by(Age, Sex) %>%
    
    # Compute the rate
    summarise(
      Patients = n(),
      Treated = sum(Treated),
      .groups = "drop"
    ) %>%
    
    # Make the proportion
    mutate(
      Rate = Treated / Patients
    ) %>%
    
    # Make a plot
    ggplot(
      aes(
        x = Age,
        y = Rate,
        color = Sex,
        group = Sex,
        text = 
          paste0(
            "Patients: ", Patients,
            "<br>Treated: ", Treated,
            "<br>Percent Treated: ", round(Rate * 100, 1), "%"
          )
      )
    ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(labels = scales::percent) +
    ylab("Percent Treated (%)"),
  tooltip = "text"
)

```

As expected, we see an increasing treatment rate with age, as well as for male patients (this is only expected because we defined it this way; in practice, this graph might give us the evidence to use age and sex in a propensity score model). 

**Important**: _Note that we did not relate the treatment to likelihood of heart disease (our outcome), so there is actually no effect. Any effect we do see is completely explained by confounding due to age and sex._

## Fitting the propensity score model {#modelfit}

As described [above](#definition), we just need to fit a logistic regression model, regressing the treatment variable on age and sex.

```{r}
ps_mod <- glm(Treated ~ scale(Age) + Sex, data = dat, family = "binomial")
summary(ps_mod)
```

Note that we centered and scaled `Age` which resulted in basically the same model we [specified](#artificial) (which we would hope it does).

## Extracting the overlap weights {#extractingweights}

To get the weights, we have to extract the fitted probabilities of treatment for each observation, and then apply the [formula](#definition). We'll also take one extra step and normalize the weights within each treatment group so they sum to 1.

```{r}
dat <-
  dat %>%
  
  # Compute the weights
  mutate(
    p_hat = predict(ps_mod, type = "response"),
    OW = 
      case_when(
        Treated == 1 ~ 1 - p_hat,
        TRUE ~ p_hat
      )
  ) %>%
  
  # Normalize in each group
  group_by(Treated) %>%
  mutate(
    OW_norm = OW / sum(OW)
  ) %>%
  ungroup()

dat %>%
  plot_ly(
    x = ~OW_norm,
    y = ~factor(Treated),
    color = ~factor(Treated),
    type = "box"
  )
```

We can use the `PSweight` package to verify that we computed the weights correctly.

## Estimate treatment effect

* Verify standardized differences are zero (compared to unweighted)
* Plot the treatment effect before and after weighting.

# Explaining the weights 

The [example](#example) above just gave the steps to run the procedure and get an estimated treatment effect. However, in practice, it is much more complex than this (mostly because we already knew the answer!). There are many ways we can think to summarize the weights as an exploratory step to better understand the impact of the weighting procedure.

* Intuition about who it focuses on, proportional down weights
* Show differences in outcome weighted and unweighted
* Show standarized differences of covariates (some not included in PS model as well) (use plotly)
* Show ways to assess cumulative contribution of cases and impact on other covariates
* Assess redundancy

# Resources {#resources}

The overlap weight is part of a general class of causal methodology/theory for balancing covariates to estimate treatment effects using propensity scores. More broadly, these fall into the [_potential outcomes_](https://en.wikipedia.org/wiki/Rubin_causal_model) framework of causal inference, which I happened to be introduced to by accident while learning about overlap weighting from these resources:

* Original paper for overlap weighting ([link](https://pubmed.ncbi.nlm.nih.gov/30189042/))
* Paper extending overlap weighting to survival analysis ([link](https://arxiv.org/abs/2108.04394))
* Example simulation of overlap weighting in the survival setting in SAS ([link](http://www2.stat.duke.edu/~fl35/OW/OW_survival_Demo.sas))
* R package for implementing overlap weight methods ([link](https://github.com/thuizhou/PSweight))
* R functions for overlap weighting in the survival setting ([link](https://github.com/chaochengstat/OW_Survival))
* Author's web page dedicated to overlap weighting ([link](https://www2.stat.duke.edu/~fl35/OW.html))

All of these were super helpful. Understanding the idea of potential outcomes made the math and methodological concepts a lot easier to grasp conceptually and gain intuition. The way I like to think about it is (roughly) in the following order:

1. Each observation has a _true_ propensity score (probability of treatment). The treatment we observe is just a realization of a random draw from that [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution) distribution (or [Multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution) distribution if there are more treatments). We must estimate this from the treatment we observe. 
2. Since there is a true propensity score, there is also a _true_ overlap weight (for each treatment option) that balances the observations back with the population. We must also estimate this from the data.
3. The outcome we observe is conditional on the observed treatment. So there exists some true function (e.g., linear predictor) relating the observation to the outcome under the treatment we observed.
4. However, this also all exists for the treatment(s) we did _NOT_ observe. So there are analogous functions relating the observation to the outcome under the alternate treatment scenarios.
5. If we could observe the outcome for all treatments, we could just compute the mean differences in the outcome across the treatment groups and that would be the causal treatment effect. I imagine a stacked data set where there is a row for each observation under each treatment, it's just that the the outcome value (and other data) for the treatments that weren't observed are missing, and our goal is to estimate what the mean differences in the outcomes would have been had that data not been missing. Thus we must use what we have to approximate that.



