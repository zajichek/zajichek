---
title: The overlap weight in survival analysis
author: Alex Zajichek
date: '2023-08-17'
slug: the-overlap-weight
categories: []
tags: ['causal inference', 'survival analysis', 'rstats']
subtitle: "A simulation review"
summary: "The goal is to estimate what the outcome difference would be had the counterfactual also been observed."
authors: []
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

**_In progress_**

I was recently introduced to [overlap weighting](https://jamanetwork.com/journals/jama/article-abstract/2765748), which is part of a general family of methods for balancing covariates when estimating treatment effects with observational data. Specifically, it focuses on the [_clinical equipoise_](https://www.sciencedirect.com/topics/medicine-and-dentistry/clinical-equipoise#:~:text=Clinical%20equipoise%20is%20defined%20as,Seminars%20in%20Vascular%20Surgery%2C%202022); that is, the patients in which the treatment decision is most uncertain. I find it more elegant than [matching](https://en.wikipedia.org/wiki/Propensity_score_matching) (which I've [used in the past](https://jamanetwork.com/journals/jama/fullarticle/2749478)), and figured a useful way to better understand the approach is to deconstruct, interpret, and translate a [SAS simulation](http://www2.stat.duke.edu/~fl35/OW/OW_survival_Demo.sas) in the context of survival analysis implemented by the [original authors](https://pubmed.ncbi.nlm.nih.gov/30189042/). All code is written in (and translated to) [`R`](https://www.r-project.org/). We'll start by loading some packages.

```{r}
library(tidyverse)
library(survival)
```

# Table of Contents

* [Setting up the potential outcomes](#potentialoutcomes)
  + [Simulate some patients](#simulatepatients)
  + [Define the treatment propensity](#treatmentpropensity)
  + [Generate the treatment assignment](#treatmentassignment)
  + [Compute the (true) overlap weight](#trueoverlapweight)
  + [Set the treatment effect](#treatmenteffect)

# Setting up the potential outcomes {#potentialoutcomes}

The theoretical underpinnings of [overlap weighting](https://pubmed.ncbi.nlm.nih.gov/30189042/) live in the context of the [potential outcomes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5841618/#:~:text=The%20potential%20outcomes%20framework%20provides%20a%20way%20to%20quantify%20causal,exposure%20or%20intervention%20under%20consideration.) paradigm of [causal inference](https://www.sciencedirect.com/topics/social-sciences/causal-inference). Basically, we wonder what _would have_ happened had we been able to observe each patient under both treatments (known as the [counterfactual](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-5-28)). If we knew the outcomes from the two worlds, the causal treatment effect would simply be the average difference across all patients. The problem of course is that in reality the outcome can only be observed for the treatment in which the patient was assigned (or chose), and so our goal is to work with the "incomplete" information that we do have to try to estimate what the outcome difference would have been had the counterfactual been observed as well.

## Simulate some patients {#simulatepatients}

The first thing we need to do is generate some (fake) patients to facilitate the simulation. Here we'll focus on age, sex, and income as patient-identifying characteristics.

```{r}
# Set the sample size
n <- 10000

# Set the seed
set.seed(123)

# Build the population
population <-
  tibble(
    age = rnorm(n, mean = 50, sd = 10),
    zage = (age - 50) / 10,
    male = rbinom(n, size = 1, prob = 0.6),
    income = rnorm(n, mean = 50, sd = 10),
    zincome = (income - 50) / 10
  )
population
```

We've generated a sample of `r paste(n)` patients. We'll assume `age` is measured in years, `income` in thousands of dollars (\$), and `male` is 1 for _Male_ and 0 for _Female_. The columns `zage` and `zincome` are just [standardized](https://statisticsbyjim.com/glossary/standardization/#:~:text=In%20statistics%2C%20standardization%20is%20the,standard%20deviation%20for%20a%20variable.) versions of the originals to avoid scaling nuances (we'll refer to these as $age_z$ and $income_z$).

## Define the treatment propensity {#treatmentpropensity}

Here's where the important theory starts to creep in (already). We assume that there is a _true_ propensity score, say $p_i^A$, that is the _true_ probability that patient _i_ receives treatment _A_ given their specific characteristics (and let's assume there are possible treatments _A_ & _B_). Further, we assume (some transformation of) this probability is some linear combination of all the characteristics that confound the crude treatment effect on the outcome. In this simulation, we'll assume the [logit](https://en.wikipedia.org/wiki/Logistic_regression) model:

$$log(\frac{p_i^A}{1 - p_i^A}) = -0.4 + 0.405*age_z - 0.223*male - 0.693*income_z$$

So let's add this _true_ propensity score to the `population` data set:

```{r}
population <-
  population |>
  
  # Add the true propensity score (unknown quantity)
  mutate(
    log_odds_pA = -0.4 + log(1.5)*zage + log(0.8)*male + log(.50)*zincome,
    pA = 1 / (1 + exp(-log_odds_pA))
  )
population
```

Obviously in practice we don't know what $p_i^A$ is, so it must be _estimated_ from the treatment assignments we observe in the data. Additionally, we aren't strictly required to assume the [logit](https://en.wikipedia.org/wiki/Logistic_regression) model, but when we use it to estimate the propensity score for [overlap weighting](https://pubmed.ncbi.nlm.nih.gov/30189042/), it has the advantageous property of perfect balance. That is, the weighted-mean differences for all covariates included in the logistic regression model will be zero across the treatment groups.

## Generate the treatment assignment {#treatmentassignment}

The treatment assignment is one of the _known_ quantities we would observe in a real life sample, and that is what would be used as the dependent variable for _estimating_ propensity scores. However, since we are running a simulation, we need to generate the treatment assignments from the governing distribution. We assume that the observed treatment for patient _i_ is the result of an (unfair) coin flip that has a probability equal to the [true propensity score](#treatmentpropensity). In math notation,

$$A_i \sim Bernoulli(p_i^A)$$

where $A_i$ is the indicator of treatment A. Let's add a _realized_ treatment assignment to the `population`:

```{r}
population <-
  population |>
  
  # Add the realized treatment assignment (known quantity)
  mutate(
    A = rbinom(n, size = 1, prob = pA),
  )
population
```

In this case, `A=1` represents treatment _A_ and `A=0` represents treatment _B_. Again, this is the actual treatment we would observe the patient receiving in a sample.

## Compute the (true) overlap weight {#trueoverlapweight}

We need to define what the overlap weight is, and it's actually quite simple: just assign the patient the probability they _did not_ receive their observed treatment. 

$$
\begin{equation}
OW_i=
    \begin{cases}
        1-p_i^A & \text{if } A_i=1\\
        p_i^A & \text{if } A_i=0\\
    \end{cases}
\end{equation}
$$

Notice the notation. Since it depends on the [true propensity score](#treatmentpropensity), the overlap weight is another _unknown_ quantity that is estimated from the data. It also depends on the [realized treatment assignment](#treatmentassignment), so the collection of weights across patients differ depending on the observed treatment distribution. We can add these weights to the `population` data set:

```{r}
population <-
  population |>
  
  # Add the true overlap weight for the realized treatment (unknown quantity)
  mutate(
    OW = A * (1-pA) + (1-A) * pA
  )
population
```

In a real analysis, these weights are what we are ultimately after in order to estimate the average _causal_ treatment effect on the outcome of interest while balancing differences in patient characteristics across the groups (i.e., removing the confounding factors).

### Target population

Now we do end up normalizing the weights so they sum to one within each treatment group. But the intuition about what's happening is that the focus of the treatment effect estimation is shifted to patients that are most likely in _either_ treatment group (i.e., $p_i^A$ near 0.5). This is known as the [_clinical equipoise_](https://www.sciencedirect.com/topics/medicine-and-dentistry/clinical-equipoise#:~:text=Clinical%20equipoise%20is%20defined%20as,Seminars%20in%20Vascular%20Surgery%2C%202022), and the resulting treatment effect is interpreted as the _average treatment effect in the overlap population_.

This makes a lot of sense. When we're comparing treatments, we should focus most heavily on patients that could be candidates for either, and less so on patients that were bound for one. Thus, we up-weight patients who have the most overlap in characteristics with the opposing treatment group. The beautiful thing here is that we do this without throwing away any information; smoothly and proportionately weighting each subject just the amount that we should.

## Set the treatment effect {#treatmenteffect}

# Summary 

1. All subjects are used instead of arbitrary cutoffs (e.g., 1:5 matching)
2. It proportionately down-weights the impacts of subjects most likely belonging to a certain treatment group
3. Achieves perfect balance in covariates across treatment groups instead of being arbitrarily close
4. Treatment groups equally contribute to the (weighted) estimation in the outcome model
5. The weights themselves can be used to gain insight about the imbalance in the sample and intuition about their impacts on the estimation
6. Avoids extreme case weights by being bounded between 0 and 1
7. It has [been shown](https://academic.oup.com/aje/article/188/1/250/5090958) to produce more unbiased and efficient estimates than other inclusive weighting approaches like [inverse probability weighting (IPW)](https://en.wikipedia.org/wiki/Inverse_probability_weighting)

```{r, eval = FALSE}
dat %>%
  plot_ly(
    x = ~PS,
    type = "histogram"
  ) %>%
  layout(
    xaxis = list(title = "True Propensity Score (PS)", tickformat = "2%"),
    yaxis = list(title = "Patients")
  )
```

```{r, eval = FALSE}
ggplotly(
  dat %>%
    
    # Make age groups
    mutate(
      Age = Hmisc::cut2(Age, g = 5)
    ) %>%
    
    # For each group
    group_by(Age, Sex) %>%
    
    # Compute the rate
    summarise(
      Patients = n(),
      Treated = sum(Treated),
      .groups = "drop"
    ) %>%
    
    # Make the proportion
    mutate(
      Rate = Treated / Patients
    ) %>%
    
    # Make a plot
    ggplot(
      aes(
        x = Age,
        y = Rate,
        color = Sex,
        group = Sex,
        text = 
          paste0(
            "Patients: ", Patients,
            "<br>Treated: ", Treated,
            "<br>Percent Treated: ", round(Rate * 100, 1), "%"
          )
      )
    ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(labels = scales::percent) +
    ylab("Percent Treated (%)"),
  tooltip = "text"
)

```

```{r, eval = FALSE}
dat %>%
  plot_ly(
    x = ~OW_norm,
    y = ~factor(Treated),
    color = ~factor(Treated),
    type = "box"
  )
```

# Resources {#resources}

The overlap weight is part of a general class of causal methodology/theory for balancing covariates to estimate treatment effects using propensity scores. More broadly, these fall into the [_potential outcomes_](https://en.wikipedia.org/wiki/Rubin_causal_model) framework of causal inference, which I happened to be introduced to by accident while learning about overlap weighting from these resources:

* Original paper for overlap weighting ([link](https://pubmed.ncbi.nlm.nih.gov/30189042/))
* Paper extending overlap weighting to survival analysis ([link](https://arxiv.org/abs/2108.04394))
* Example simulation of overlap weighting in the survival setting in SAS ([link](http://www2.stat.duke.edu/~fl35/OW/OW_survival_Demo.sas))
* R package for implementing overlap weight methods ([link](https://github.com/thuizhou/PSweight))
* R functions for overlap weighting in the survival setting ([link](https://github.com/chaochengstat/OW_Survival))
* Author's web page dedicated to overlap weighting ([link](https://www2.stat.duke.edu/~fl35/OW.html))

All of these were super helpful. Understanding the idea of potential outcomes made the math and methodological concepts a lot easier to grasp conceptually and gain intuition. The way I like to think about it is (roughly) in the following order:

1. Each observation has a _true_ propensity score (probability of treatment). The treatment we observe is just a realization of a random draw from that [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution) distribution (or [Multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution) distribution if there are more treatments). We must estimate this from the treatment we observe. 
2. Since there is a true propensity score, there is also a _true_ overlap weight (for each treatment option) that balances the observations back with the population. We must also estimate this from the data.
3. The outcome we observe is conditional on the observed treatment. So there exists some true function (e.g., linear predictor) relating the observation to the outcome under the treatment we observed.
4. However, this also all exists for the treatment(s) we did _NOT_ observe. So there are analogous functions relating the observation to the outcome under the alternate treatment scenarios.
5. If we could observe the outcome for all treatments, we could just compute the mean differences in the outcome across the treatment groups and that would be the causal treatment effect. I imagine a stacked data set where there is a row for each observation under each treatment, it's just that the the outcome value (and other data) for the treatments that weren't observed are missing, and our goal is to estimate what the mean differences in the outcomes would have been had that data not been missing. Thus we must use what we have to approximate that.



