---
title: The overlap weight
author: Alex Zajichek
date: '2023-04-01'
slug: the-overlap-weight
categories: []
tags: ['statistics', 'datascience', 'causation', 'rstats']
subtitle: 'A smoother, more inclusive approach to matching'
summary: ''
authors: []
lastmod: '2023-04-01T11:39:12-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

I was recently introduced to a method in the field of [matching](https://en.wikipedia.org/wiki/Matching_(statistics)) (or _balancing_) for observational studies called [_overlap weighting_](https://jamanetwork.com/journals/jama/article-abstract/2765748). Although I've just started getting into the details, I already see it as superior to [propensity score (PS) matching](https://en.wikipedia.org/wiki/Propensity_score_matching), a well-known matching method and what I've done in the past, for a few reasons (that I've observed at least):

1. It uses all available information (all observations) instead of arbitrary cutoffs like, for example, 1:5 matching.
2. Its properties _force_ the standardized differences of all factors included in the propensity score model to be zero across the treatment groups instead of arbitrarily close.
3. Each treatment group contributes equally to the outcome model when the overlap weights are used regardless of the balance of the treatment groups in the original sample.
4. Because of (1), it makes it much more intuitive to explore the impacts of the weighting process on other covariates and the outcome.
5. It also has [been shown](https://academic.oup.com/aje/article/188/1/250/5090958) to produce more unbiased and efficient estimates than other inclusive weighting approaches like _inverse probability weighting (IPW)_ (regardless if trimming was used). It also has practical benefits over IPW in that weights are bounded between 0 and 1, foregoing the need for (arbitrary) trimming.

The overlap weighting process actually starts the same as PS matching: by modeling the treatment group as a function of various factors that may be confounding the observed (unadjusted) treatment effect. It's what is done with the scores (probabilities) that differentiate the methods. 

# Table of Contents

* [Defining the overlap weight](#definition)
* [Heart disease example](#example)
  + [Making an artificial treatment](#artificial)
  + [Fitting the propensity score model](#modelfit)

# How is it defined? {#definition}

It's actually quite simple. Suppose you have treatments A & B (note this does extend to more treatments as well), and you build a propensity score model (using logistic regression) to estimate the probability that each observation belongs to a particular treatment group given a collection of covariate values (_Note: We should strive for selecting covariates that we believe are associated with both the treatment group membership and the outcome of interest_):

$$P(T_i = A|X_i) \approx \frac{1}{1 + e^{-X_i\hat{\beta}}} = \hat{p_i}$$
where $\hat{\beta}$ are the estimated regression coefficients and $X_i$, $T_i$ are the covariate values and treatment group for the $i^{th}$ observation, respectively. Then $OW_i$, the _overlap weight_ for the $i^{th}$ observation, is calculated by:

$$
\begin{equation}
OW_i=
    \begin{cases}
        1-\hat{p_i} & \text{if } T_i = A\\
        \hat{p_i} & \text{if } T_i = B\\
    \end{cases}
\end{equation}
$$
Now we do end up normalizing these weights so that they sum to one (1) _within_ each treatment group. But here's some intuition about what's happening: it shifts the focus of the subsequent outcome model to observations that are most likely in _either_ treatment group (i.e., $P(T_i = A|X_i)$ near 0.5) (technically it focuses _most_ on observations that are "misclassified", but since these probabilities are derived on the data set in which the model was built, we probably won't see many of those). 

I think this makes a lot of sense. When we're trying to evaluate the effectiveness of a treatment, we want to focus most heavily on those who could be candidates for either one, and less so on those who were bound for one of the treatment options (because of their specific characteristics). The beautiful thing here is that we do this without throwing away any information; smoothly and proportionately weighting each observation just the amount that we should.

# Can I have an example? {#example}

Sure. Let's first load some packages to get started.

```{r, message = FALSE}
library(tidyverse)
library(plotly)
```

We're going to be creating an artificial example using the `cheese::heart_disease` data set to estimate the effect of some treatment on the likelihood of heart disease. 

```{r}
dat <- cheese::heart_disease
print(dat, n = 5)
```

## Making an artificial treatment {#artificial}

There is not actually a treatment column in this data set, so we'll make one. Let's assume the following:

$$logit(P(T_i|X_i)) = 0.75Age_{std} + 0.5Male$$
That is, the probability of being _treated_ ($T_i$) is linearly related to the (standardized) age and sex of the patient. Further, older patients and male patients are more likely to be treated. 

```{r}
dat <- 
  dat %>%
  
  # Add the true propensity score
  mutate(
    PS = 1 / (1 + exp(-(.75 * ((Age - mean(Age))/sd(Age)) + .5 * (Sex == "Male"))))
  ) 
```

Let's look at the _true_ propensity score distribution (note these are never known).

```{r}
dat %>%
  plot_ly(
    x = ~PS,
    type = "histogram"
  ) %>%
  layout(
    xaxis = list(title = "True Propensity Score (PS)", tickformat = "2%"),
    yaxis = list(title = "Patients")
  )
```

Now, we'll generate the treatment indicator for each patient by taking a random draw from a Bernoulli distribution:

```{r}
set.seed(123)
dat <- 
  dat %>%
  
  # Assign treatment
  mutate(
    Treated = rbinom(n = nrow(dat), size = 1, prob = PS)
  )
```

Of the `r nrow(dat)` patients, `r paste0(sum(dat$Treated), " (", round(mean(dat$Treated) * 100, 1), "%)")` were treated. Let's look at the observed rate of treatment by age and sex.

```{r}
ggplotly(
  dat %>%
    
    # Make age groups
    mutate(
      Age = Hmisc::cut2(Age, g = 5)
    ) %>%
    
    # For each group
    group_by(Age, Sex) %>%
    
    # Compute the rate
    summarise(
      Patients = n(),
      Treated = sum(Treated),
      .groups = "drop"
    ) %>%
    
    # Make the proportion
    mutate(
      Rate = Treated / Patients
    ) %>%
    
    # Make a plot
    ggplot(
      aes(
        x = Age,
        y = Rate,
        color = Sex,
        group = Sex,
        text = 
          paste0(
            "Patients: ", Patients,
            "<br>Treated: ", Treated,
            "<br>Percent Treated: ", round(Rate * 100, 1), "%"
          )
      )
    ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(labels = scales::percent) +
    ylab("Percent Treated (%)"),
  tooltip = "text"
)

```

As expected, we see an increasing treatment rate with age, as well as for male patients (this is only expected because we defined it this way; in practice, this graph might give us the evidence to use age and sex in a propensity score model). 

**Important**: _Note that we did not relate the treatment to likelihood of heart disease (our outcome), so there is actually no effect. Any effect we do see is completely explained by confounding due to age and sex._

## Fitting the propensity score model {#modelfit}

As described [above](#definition), we just need to fit a logistic regression model, regressing the treatment variable on age and sex.

```{r}
ps_mod <- glm(Treated ~ scale(Age) + Sex, data = dat, family = "binomial")
summary(ps_mod)
```

Note that we centered and scaled `Age` which resulted in basically the same model we [specified](#artificial) (which we would hope it does).

## Extracting the overlap weights {#extractingweights}

To get the weights, we have to extract the fitted probabilities of treatment for each observation, and then apply the [formula](#definition). We'll also take one extra step and normalize the weights within each treatment group so they sum to 1.

```{r}
dat <-
  dat %>%
  
  # Compute the weights
  mutate(
    p_hat = predict(ps_mod, type = "response"),
    OW = 
      case_when(
        Treated == 1 ~ 1 - p_hat,
        TRUE ~ p_hat
      )
  ) %>%
  
  # Normalize in each group
  group_by(Treated) %>%
  mutate(
    OW_norm = OW / sum(OW)
  ) %>%
  ungroup()

dat %>%
  plot_ly(
    x = ~OW_norm,
    y = ~factor(Treated),
    color = ~factor(Treated),
    type = "box"
  )
```

We can use the `PSweight` package to verify that we computed the weights correctly.

## Estimate treatment effect

* Verify standardized differences are zero (compared to unweighted)
* Plot the treatment effect before and after weighting.

# Explaining the weights 

The [example](#example) above just gave the steps to run the procedure and get an estimated treatment effect. However, in practice, it is much more complex than this (mostly because we already knew the answer!). There are many ways we can think to summarize the weights as an exploratory step to better understand the impact of the weighting procedure.

* Intuition about who it focuses on, proportional down weights
* Show differences in outcome weighted and unweighted
* Show standarized differences of covariates (some not included in PS model as well) (use plotly)
* Show ways to assess cumulative contribution of cases and impact on other covariates
* Assess redundancy