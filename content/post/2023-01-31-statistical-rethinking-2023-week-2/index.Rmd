---
title: 'Statistical Rethinking 2023: Week 2'
author: Alex Zajichek
date: '2023-01-31'
slug: statistical-rethinking-2023-week-2
categories: []
tags: []
subtitle: 'Chapter 4'
summary: ''
authors: []
lastmod: '2023-01-31T09:38:46-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

# Table of Contents

3. [Geocentric Models](#lecture3)
4. [Categories and Curves](#lecture4)

```{r setup}
# Load some packages
library(tidyverse)
```

# 3. Geocentric Models {#lecture3}

Week #    Lecture #   Chapter(s)    Week End    Notes Taken
------    ---------   ----------    --------    -----------
2         3           4             1/13/2023   1/10/2023

## Summary

We don't actually need data to construct a model. Our prior distributions, which account for our baseline knowledge about what reasonable values for unknown parameters may be, can produce estimates on their own. A bare minimum strategy is to choose these such that before seeing data, the output of our model produces scientifically reasonable results--there is no reason to allow our model to produce results that we know cannot happen. Then, our data can be introduced to help guide the parameters to an area of focus. In this sense (thinking of the example of points bumping around in parameter space), the data we collect is really just a tool for our model--the model is the central focus, the data just helps the model go to where it needs to go. Also, the idea that there are no correct priors and that priors are just (normalized) posteriors from previous data, make the idea of Bayesian updating very intuitive. It will be interesting to see in coming lectures how we can extend this linear model framework to more "real life" problems with observational data that have potentially tens or hundreds or thousands of potential drivers, and strategies for accounting for the most important ones. Obviously these basic examples are great to build a foundation, but it seems like a huge (sometimes impossible) hurdle to have the time and resources to be able to fully vet out expert-driven causal diagrams and generative models that fully account for all the things, especially in fast-paced environments when everyone is just so busy and there are so many projects to attend to. I'd imagine this is one of the reasons why frequentist analysis persists so much (at least in medical research), because it's the way it's been done and therefore you can get more things done faster, even though in an ideal state a Bayesian approach _is_ the right way to go. Definitely something I've thought about time and time again--how can we balance the rigor and detail needed to construct the appropriate models to achieve better inference while still being efficient with peoples' time? Part of it probably has to do with proving to stakeholders that the inference gained from the "quicker" way is less informative (or just plain wrong) compared to the more involved approach.

## Notes

* Statistical models can attain arbitrarily accurate predictions without having any explanation or accurate structure (i.e., the model is just plain wrong, but happens to produce accurate predictions at the right time)
  + Example of this is a previous explanation of orbit pattern of Mars: assuming Earth at the center (geocentric), Mars orbits around Earth but also it's own local orbit (epi-cycles). Using this model, they got very accurate predictions, but this mechanism is completely wrong.
  + Orbits are actually elliptical and around the sun, not Earth
  + Even though the first one predicts accurately, because the structure/mechanism is wrong, it doesn't extend or generalize to other things. However, the correct mechanism is able to explain orbit patterns of all planets in the solar system.
* Linear regression is a large class of statistical golems
  + **Geogentric**: describes associations, makes good predictions; mechanistically always wrong (but useful), very good approximation; meaning doesn't depend on the model, depends on an external causal model. Nothing wrong with it unless you actually believe it is the true mechanism.
  + **Gaussian**: Abstracts away from detail of general error model; mechanistically silent. General argument about symmetry of error. 

**Gaussian**

* Example: Flip coin, each person take a step to left or right depending on heads/tails, measure distance from center; makes a normal distribution. Why?
  + There are more ways for a sequence of coin tosses to get you close to the middle than there are to get you to the left or right
  + Many natural processes attract to this behavior because it is adding together small differences
* Two arguments:
  + Generative: summed fluctuations tend towards normal. Ex. growth--added fluctuations over time, same age weight tends to be gaussian
  + Inferential: estimating mean/variance. Best to use since least informative (maximum entropy)
* Variable does not need to be normally distributed for normal model to be useful. Machine for estimating mean/variance. Contains the least assumptions. (central limit theorem)

**Skills/Goals for Lecture**

1. Learn a standardized language for representing models (generative and statistical)
2. Calculate posteriors with multiple unknown parameters
3. How to construct and understand linear models; how to construct posterior predictions from them

**Reminder of the owl**

1. State a clear question; descriptive, causal, anything; but needs to be clear
2. Sketch causal assumptions using DAGs; good way for non-theorists to realize they have a lot of subject knowledge and can get it on paper
3. Define a generative model; generates synthetic observations
4. Use generative model to build estimator; causal/generative assumptions embedded
5. Test, analyze
6. Profit: we realize our model was useful, or terrible; either way we gain something

**Describing models**

1. Lists variables
2. Define each variable as a deterministic or distributional function of other variables

**Exercise**

1. Goal: Describe the association between adult weight and height
2. Height causes weight H-->W<--(U) (unobserved influences on body weight)
3. Generative/scientific model: $W=f(H,U)$, $W=\beta H + U$
```{r}
sim_weight <-
  function(H,b,sd) {
    U <- rnorm(length(H),0,sd)
    W<-b*H + U
    return(W)
  }
# Generate height
H <- runif(200,130,170)
W <- sim_weight(H, b=.5, sd= 5)
plot(W~H,col=2, lwd = 3)
```

$$W_i=\beta H_i + U_i$$
$$U_i \sim Normal(0,\sigma)$$
$$H_i \sim Uniform(130, 170)$$
4. Statistical model (estimator)

* We want to estimate how the average weight changes with height.

$$E(W_i|H_i)=\alpha + \beta H_i$$

* Posterior distribution

$$P(\alpha, \beta, \sigma|H_i,W_i) = \frac{P(W_i|H_i,\alpha,\beta,\sigma)P(\alpha,\beta,\sigma)}{Z}$$

* Gives the posterior probability of a specific regression line
  + Likelihood: Number of ways we could produce $W_i$, given a line 
  + Prior: The previous posterior distribution; normalized number of ways previous data could have been produced.

$$W_i \sim Normal(\mu_i, \sigma)$$
$$\mu_i = \alpha + \beta H_i$$

* Generally more useful to look at the lines (parameter implications together), instead of individual parameters
 
* Quadratic approximation
  + Approximate the posterior distribution using a multivariate Gaussian distribution
  + Use the `quap` function in the `rethinking` package

**Prior Predictive Distribution**

* Should express scientific knowledge, but _softly_
* We can make the model make predictions without using data
* Not make ranges that represent the data, but rather just those that make sense based on current knowledge
* Account for basic reasonable constraints: In general, patients with more weight have more height, and the weight is less than the height, so $\beta$ is probably between $[0,1]$.
* Use these to define some lines based on the assumptions
```{r}
n <- 1000
a <- rnorm(n,0,10)
b <- runif(n,0,1)
plot(NULL,xlim=c(130,170),ylim=c(50,90),xlab="height(cm)",ylab="Weight(kg)")
for (j in 1:50) abline(a=a[j],b=b[j],lwd=2,col=2)
```

* Some of these are probably not plausible (e.g., high height with low weight). Slopes look good but not intercept
* We can adjust as needed to create what makes sense
* There are no correct priors; only scientifically justifiable priors

5. Validate Model

* Bare minimum to test statistical model
* Not because you wrote it, more so to make sure your model works

6. Analyze data

* Plug in your data set into your process
* Parameters are not independent, can't interpret as such
* Push out posterior predictions

# 4. Categories and Curves {#lecture4}

Week #    Lecture #   Chapter(s)    Week End    Notes Taken
------    ---------   ----------    --------    -----------
2         4           4             1/13/2023   1/11/2023

## Summary

The idea of _total_ vs. _direct_ effects is about specifying the statistical model that will allow you to observe the complete effect (i.e., including differences that could be explained by something else in the model) compared to parsing out differences explained by the variable after adjusting for effects explained through other variables. In the lecture example, the total causal effect of sex on weight was determined by using a (Bayesian) intercept-only model, which showed considerable difference is mean weights between male/female. However, when assessing the direct causal effect, a parameter was added to fit separate slopes for male/female in order to block out the effect of sex on weight that is observed through other causes (in this case, height), such that the resulting estimator looked at mean differences in weight _at each height_--the posterior distribution for this difference yielded little to no direct effect, indicating that most of the difference in weight between male/females is due to height differences. Another interesting aspect of this lecture was how to think about which way an arrow should go when drawing the causal diagram. You should think of the interventions we are willing to consider, and which make logical sense. For example, we drew $H \rightarrow W$ because, given a height, it makes sense to employ interventions (such as weight loss program, exercise, etc.) that could presumably impact the resulting weight, but it doesn't make a lot of sense to think of trying to change someone's height given their weight. Also, declaring something as a _cause_ of something, generally you first want to think about whether an intervention can be employed, but if not can still make sense if it is a proxy for something else (e.g., age encapsulates time, among many other things that presumably do cause height). We can use flexible curves to fit things (e.g., splines), but we want to make sure we vet out any erroneous areas where estimates don't make sense, and add necessary restrictions to alleviate. So far, these lectures have given great optimism and excitement for how to approach modeling. I want to be confident in the models I produce, and I think the generative framework is the right approach to be able to believe in the results you are producing. I see so much published research from observational data that declare something statistically significant for a given research hypothesis and say "we adjusted for all these confounders". Even if I feel fine about the math/statistical procedure, I'm always skeptical about the conclusions that are drawn from it, and quite frankly, don't feel like it means much at all for really making a decision--there are just too many limitations about all sorts of things. The generative approach gives the tools and rigor to be much more confident in the results, and if we can be more demanding of that rigor, time and energy, it should yield more benefit in the long run. I'd rather spend more time getting to a confident conclusion than just pumping out results.

### A check for understanding

During (and after) the lecture, it took me a while to gain intuition about what was happening in the generative simulation for the model:

```{r}
DiagrammeR::grViz("
  digraph graph2 {
    graph [layout = dot, rankdir = LR]
    node [style = filled]
    a [label = 'Sex']
    b [label = 'Height']
    c [label = 'Weight']
    a -> c
    a -> b
    b -> c
  }
", height = 200, width = 350)
```

The code was written in the following way:

```{r}
# S = 1 female, S = 2 male
sim_HW <- function(S,b,a) {
  N <- length(S)
  H <- ifelse(S==1,150,160) + rnorm(N,0,5)
  W <- a[S] + b[S]*H + rnorm(N,0,5)
  data.frame(S,H,W)
}

# Generate data
set.seed(123)
S <- rbinom(100,1,.5) + 1
dat <- sim_HW(S, b=c(.5,.6), a=c(0,0))
head(dat)
```

First, the indexing used here `b[S]` was odd because `b` is a vector of length 2, and `S` is a vector of length `r length(S)`. But all it is doing is making a vector of length `r length(S)` by looking up the index of `b` at each spot (since `S` is either 1 or 2). I didn't know you could index like that in `R` but I guess you learn something everyday. Anyway, that was not the real thing that confused me.

In the lecture, he states that the `a` term represents the _direct_ effect of sex on weight, and the `b` term represents the _indirect_ effect (i.e., proportionality/slope for each sex). It's clear that there are separate lines created for each sex, and you can see the form of an intercept and slope for each one. In my mind, I'm thinking this has to be similar to an interaction in the model, but it wasn't intuitive to me how this really played out and/or there was something different going on here. After some thought on a notepad, it is exactly what I was thinking--just a linear model with an interaction term between sex and height, though it is reparameterized a little to create the _symmetry_ of effects as discussed in the lecture. Anyway, here is how it translates:

Currently, we have that the sex indicators are as follows:

$$S = 1 (female), 2(male)$$
Then, the effect of height on weight for each sex is as follows:

$$b=(b_{S_1},b_{S_2}) = (0.5, 0.6)$$
Finally, the intercept within each line is:

$$a=(a_{S_1},a_{S_2})=(0,0)$$
This leads to:

$$W_{S_1} = a_{S_1} + b_{S_1}H + \epsilon_i = .5H+\epsilon_i$$
$$W_{S_2} = a_{S_2} + b_{S_1}H + \epsilon_i = .6H + \epsilon_i$$
We could think of this as a single model equation with four (4) regression coefficients looking like the following:

$$W = \beta_1 S_1 + \beta_2 S_2 + \beta_3 H \times S_1 + \beta_4 H \times S_2 + \epsilon_i$$
where

$$S_1 = 1 \text{ if female; 0 otherwise}$$
$$S_2 = 1 \text{ if male; 0 otherwise}$$
There is no intercept term in the model. Instead, there are _symmetric_ parameterizations for males and females, instead of making the effects relative to one another (which, as mentioned in the lecture, makes it more intuitive to make priors for). The design matrix for this model would then look something like:

```{r}
tribble(
  ~S1, ~S2, ~H_S1, ~H_S2, ~W,
  1, 0, 150, 0, 80,
  0, 1, 0, 160, 90,
  0, 1, 0, 140, 70,
  1, 0, 165, 0, 75
)
```

Basically, every time `S1` is `1` (female), then `S2` is `0` (male), as well as the corresponding value for `H`. 

_I may have to rethink what I just wrote a bit as the the $S_1$ and $S_2$ columns are actually completely redundant, so not sure if this is right yet_

So how would we parameterize this in a more classical regression model? If we just rewrite a few things. Let:

$$S=(0(female), 1(male))$$
Then we assume the model is:

$$W = \beta_0+\beta_1 S+\beta_2 H+\beta_3 S\times H + \epsilon_i$$
If we translate parameter values from the example,

$$\beta_0 = 0 \hskip.1in \beta_1 = 0$$
$$\beta_2 = .5 \hskip.1in \beta_3 = .6-.5 = .1$$
We then get:

$$
\begin{equation} 
\begin{split}
W_{S=0(female)}
&= \beta_0 + \beta_1(0) + \beta_2H+\beta_3 0 \times H + \epsilon_i \\
&= 0 + 0(0) + .5H + .1(0 \times H) + \epsilon_i \\
&= 0.5H + \epsilon_i
\end{split}
\end{equation}
$$

$$
\begin{equation} 
\begin{split}
W_{S=1(male)}
&= \beta_0 + \beta_1(1) + \beta_2H+\beta_3 1 \times H + \epsilon_i \\
&= 0 + 0(1) + .5H + .1(1 \times H) + \epsilon_i \\
&= 0.5H + 0.1H + \epsilon_i \\
&= 0.6H + \epsilon_i
\end{split}
\end{equation}
$$

This makes it much more clear why the _direct_ effect is zero, since the main effect of sex is zero in this model. We only see an effect from sex through the interaction with height, which is what is known as the _indirect_ effect.

## Homework

1. From the `Howell1` dataset, consider only the people younger than 13 years old. Estimate the causal association between age and weight. Assume age influences weight through two paths. First, age influences height, and height influences weight. Second, age directly influences weight through age-related changes in muscle growth and body proportions. Draw the DAG that represents these causal relationships. And then write a generative simulation that takes age as an input and simulates height and weight, obeying the relationships in the DAG.

First, we'll import the dataset directly from the package's [Github repository](https://github.com/rmcelreath/rethinking). We will filter to those < 13 years old, and convert height to inches, and weight to lbs:

```{r}
howell1 <-
  read_delim(
    file = "https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv",
    delim = ";"
  ) %>%
  
  # Keep those younger than 13
  filter(age < 13) %>%
  
  # Convert the units (more intuitive for me)
  mutate(
    height = height / 2.54,
    weight = weight * 2.205
  )
howell1
```

Next, we can write the causal diagram as described:

```{r}
DiagrammeR::grViz("
  digraph graph2 {
    graph [layout = dot, rankdir = LR]
    node [style = filled]
    a [label = 'Age']
    b [label = 'Height']
    c [label = 'Weight']
    a -> c
    a -> b
    b -> c
  }
", height = 200, width = 350)
```

If we let

$$A=Age \hskip.1in H=Height \hskip.1in W=Weight$$

Then, we assume that:

$$H = f_H(A) \hskip.2in W = f_W(A,H)$$
Finally, we can write the generative simulation to produce synthetic data governed by the DAG:

```{r}
# Simulating synthetic children
sim_children <-
  function(N) {
    
    # 1. Generate uniform ages
    A <- runif(N, 0, 13)
    
    # 2. Generate heights as a linear combination of age
    H <- 22 + 2*A + rnorm(N, mean = 0, sd = 1)
    
    # 3. Generate weights as a linear combination of age and height
    W <- .8*H + rnorm(N, mean = 0, sd = .5)
    
    # Make a data frame
    tibble(A, H, W)
  }
```

We first generate ages uniformly from 0 to 13, so

$$A \sim Uniform(0,13)$$

Then, we generate heights from a normal distribution with means that are linearly related to the age.

$$H \sim Normal(\mu =22 + 2 \times A, \sigma = 3)$$
Notice the intercept term to ensure a positive height for children who are 0 years old. _Note: A distribution like the Gamma may be better here to ensure we don't get negative heights. For younger ages, I would assume that the distribution of heights has a little right-skew. In any case, we'll move forward with the Normal distribution here for simplicity sake._

Then we generate the weights as a linear function of the observed age and heights. 

$$W \sim Normal(\mu=.8H, \sigma = .5)$$
We make the assumption that there is a linear relationship between weight with age and height, and that for any age, the increase in mean weight per inch increase in height is the same. In fact, the effect for age is 0 since it is observed through height.

```{r}
# Simulate some children
set.seed(123)
simmed_children <- sim_children(200)
plot(simmed_children)
```


2. Use a linear regression to estimate the **total** causal effect of each year of growth on weight.

3. Now suppose the causal association between age and weight might be different between boys and girls. Use a single linear regression, with a categorical variable for sex, to estimate the total causal effect of age on weight separately for boys and girls. How do boys and girls differ? Provide one or more posterior contrasts as a summary.

## Notes

* The linear regression can approximate anything, so we need to design it with the causal model in mind
* Generative models + multiple estimands, we'll have multiple estimands
* Need post-processing of posterior distribution to gain inference of joint distributino
* We require categories, splines, etc. to build causal estimators
* Need to _stratify_ by category to get at the estimands we want (separate lines)

**Example**

* Extend example above to include patient sex, age
* Need to determine how height, weight, sex are causally related (add to DAG), and statistically related
* To determine which way the arrows go, think about the interventions you're willing to consider
* Don't have to draw them, but the implied unobserved causes of each variable are implied
  + These are ignorable _unless shared_ across variables
  + Ex. temperature is a cause of sex and weight in some species
  
* What is the causal effect of S on W?
  + Accounts for direct and indirect effect
  + We can also ask what is the direct causal effect of S on W?
  + These questions require different models
  
* Generally want to assign the same prior for parameters for each category level (below)
  + Using indexing is advantageous because you have symmetry such that all parameters can get the same prior, they are all interpreted the same within their levels
  + Using indicators makes parameters relative to other levels, which causes you have to put priors on other parameters because it is an adjustment parameter (one is an average, one is an adjustment to an average)

$$W_i \sim Normal(\mu_i, \sigma) \hskip.1in \mu_i=\alpha_{S[i]}$$
$$\alpha = [\alpha_1, \alpha_2] \hskip.1in \alpha_j \sim Normal(60,10)$$
**Total Causal Effect**

* Simulate one data set of all males, another of all females, look at the average difference in weight
  + This is the actual causal effect
  + Then you can generate a random data set, run the modeling process, and then ensure that the model provides the expected estimate
* Look at the posterior distribution of the mean difference, and randomly draw samples from the individual posteriors and compute the differences to answer questions like "what is the probability that a randomly selected male will be heavier than a randomly selected female?"
* This was basically just an intercept-only model for sex, and the effect due to height would be captured in that difference

**Direct Effect**

* How do we partial out the indirect effect of Height (block it)?
  + Stratify by height to block the association between S and W that is transmitted through H
* Difference in intercept, the indirect is slope differences
* Here, the model allows for separate slopes by sex, so we can tease out the impact of height
  + Center the height to make the interpretation of the intercept be the average
  + Makes priors more intuitive, and computation easier

$$W_i \sim Normal(\mu_i, \sigma) \hskip.1in \mu_i=\alpha_{S[i]} + \beta_{S[i]}(H_i-\bar{H})$$
$$\alpha=[\alpha_1,\alpha_2] \hskip.1in \beta=[\beta_1,\beta_2]$$

* In this case, nearly all the total effect of sex on weight is explained through height (the direct effect (posterior of the difference between weights at each height) is nearly 0 at all heights)

**Curve Fitting**

* We use linear models to do this; i.e., it's not mechanistic, but we use it wisely
* Strategies
  + Polynomials: Don't do it; no local smoothing, only global; learn to much from data in regions that lie far away 
    + It's not worth having a model that looks OK for most of the data that we know is completely erroneous (e.g., parabola at some point shows babies get heavier as their height decreases, which we know is wrong); even though this is a small portation of observations, it's still knowingly wrong, so why use it?
  + Splines & GAMs: Not as bad as polynomials; add total many locally trained terms

**Splines**

* Flexible curve that will find trends
* B-splines are linear models containing additive terms with synthetic variables
  + Think of it as a collection of individual curves (basis functions), but the weight of each basis function is non-zero at only particular areas of x, and spline is the sum of the curves at a particular point

$$\mu_i = \alpha + w_1B_{i,1} + w_2B_{i,2} + ...$$

* Ideal model for age/height would be to account for what we know about human biology: infant, toddler, adolescent, adult. In the first 3, we expect only upward growth, so we should constrain.

**Full Luxury Bayes**

* Equivalent approach is to use one model for entire causal sample
* Then run simulations from overall system to get answers to specific queries
