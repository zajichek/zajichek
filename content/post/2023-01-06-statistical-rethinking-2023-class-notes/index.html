---
title: Statistical Rethinking 2023 Class Notes
author: Alex Zajichek
date: '2023-01-06'
slug: statistical-rethinking-2023-class-notes
categories: []
tags: []
subtitle: 'Summaries, interpretations, and examples'
summary: ''
authors: []
lastmod: '2023-01-06T09:37:10-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/htmlwidgets/htmlwidgets.js"></script>
<script src="{{< blogdown/postref >}}index_files/viz/viz.js"></script>
<link href="{{< blogdown/postref >}}index_files/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="{{< blogdown/postref >}}index_files/grViz-binding/grViz.js"></script>


<p>This document is intended to be a repository for my (raw, unedited) notes, interpretations, examples, and summaries from the <a href="https://github.com/rmcelreath/stat_rethinking_2023">Statistical Rethinking 2023</a> course (which Richard McElreath has graciously made available for free (!) covering <a href="https://xcelab.net/rm/statistical-rethinking/">his book</a>). I’m not actually enrolled in the course, but just casually following the lectures and material as they are released on a weekly basis. I have a strong interest in learning and incorporating Bayesian analysis and causal principles into my work, and this seemed like a great opportunity to build a foundation for that.</p>
<pre class="r"><code># Load some packages
library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.4.0      ✔ purrr   1.0.0 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.3      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<div id="table-of-contents" class="section level1">
<h1>Table of Contents</h1>
<ol style="list-style-type: decimal">
<li><a href="#lecture1">Science Before Statistics</a></li>
<li><a href="#lecture2">Garden of Forking Data</a></li>
<li><a href="#lecture3">Geocentric Models</a></li>
<li><a href="#lecture4">Categories and Curves</a></li>
</ol>
</div>
<div id="lecture1" class="section level1">
<h1>1. Science Before Statistics</h1>
<table>
<thead>
<tr class="header">
<th>Week #</th>
<th>Lecture #</th>
<th>Chapter(s)</th>
<th>Week End</th>
<th>Notes Taken</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>1</td>
<td>1/6/2023</td>
<td>1/3/2023</td>
</tr>
</tbody>
</table>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>This course focus is on scientific modeling via causal inference, which is focused on identifying causes in <em>observational</em> data. Causal Inference requires us to consider the mechanism of a phenomenon, and think about not only which variables cause other variables, but in what order–subject matter expertise is of utmost importance, and we don’t really depend on the data at hand until the very end of our inference process. Causal modeling must become the foundation to do analysis by–we can’t just do simple statistics in one project and then think about causal modeling in another–samples are from populations and there are causes associated with why we observed the sample we did, even if we’re answering very basic questions. Also, <em>Bayesian</em> modeling as a means to performing causal inference is not due to philosophical reasons (e.g., frequentist vs. Bayesian), it’s more so because a Bayesian framework provides the most natural tools to employ the specified causal model (i.e., if the frequentist model made sense for answering the causal question, we’d use it). The generative aspect of Bayesian modeling is one aspect in particular that makes it very inviting to represent the causal model in a statistical framework, and apply distributions. Finally, coding is not just a means to employ the math, but rather needs to be treated as part of the product, therefore employing software engineering principles, having documentation, making things reproducible. These things need to be employed if you really want to advance knowledge with confidence.</p>
</div>
<div id="notes" class="section level2">
<h2>Notes</h2>
<p><strong>Overview</strong></p>
<ul>
<li>Most interested in Causal Inference, focusing on the <em>science</em> before the <em>statistics</em></li>
<li>We must be able to talk about causes to obtain scientific knowledge, why else would we do it?</li>
<li>Causes can’t be extracted from data; must come from knowledge, assumptions</li>
</ul>
<p><strong>What is Causal Inference?</strong></p>
<ul>
<li>It is more than associations; associations are bi-directional, and correlation is only a basic measure of association;</li>
<li>It is all about intervention, directionality, and the <em>prediction</em> of the consequence of changing one variable on another (asking <em>what-if?</em>)</li>
</ul>
<p><strong>Causal Imputation</strong></p>
<ul>
<li>This is about being able to construct <em>counterfactual</em> outcomes</li>
<li>Asking the question, <em>what if I had done something else?</em></li>
<li>We only observe a single outcome, but we want to know what would have happened had a certain intervention not taken place</li>
</ul>
<p><strong>Directed Acyclic Graph (DAG)</strong></p>
<ul>
<li>Nothing more than an abstraction about which variables cause which other variables</li>
<li>Shows the direction at which variables cause each other, but doesn’t specify <em>how</em> (i.e., effect shapes, etc.)</li>
<li>We can use this to know which things to control for, answer hypothetical interventions, under the assumption that the model is true</li>
<li>It provides a tool to answer very specific questions (queries); not necessarily all questions lead to the same statistical model, but the appropriate statistical model can be derived from the causal model depending on the question</li>
<li><em>Intuition Pumps</em>: Gets the researcher to think about mechanism; great way to problem solve with SME’s without looking at the data (which is how it should be)</li>
</ul>
<p><strong>Golems (statistical models)</strong></p>
<ul>
<li>Metaphor for what a statistical model is; it’s a very useful machine that will do what it’s asked very well, but has no wisdom or forethought</li>
<li>Does not know the intent of the task</li>
<li>Statistical models are just objective tools, but we need causal models to know how and when certain models are actually appropriate</li>
</ul>
<p><strong>Statistical Models</strong></p>
<ul>
<li>Having a flowchart of tests is not useful, except maybe in the <em>experimental</em> setting (remember we’re talking observational data)</li>
<li>Statistical models/tests don’t make a clear relationship between the research and the data; it’s just math</li>
</ul>
<p><strong>Hypotheses &amp; Models</strong></p>
<ul>
<li>We need <em>generative</em> causal models that are guided by the DAG’s</li>
<li>We need <em>estimands</em> that are statistical models justified by the generative models (how do we quantify what we’re after?)</li>
<li>Introduce real data at the end–this is the easy part</li>
</ul>
<p><strong>Justifying Controls</strong></p>
<ul>
<li>Cannot just control for everything in your dataset like is done so much in current research (e.g., colliders have undesired effect)</li>
<li>Need the causal model (DAG) to be able to deduce what should be controlled for based on the specific question that is asked</li>
<li><em>Adjustment Set:</em> The variables determined appropriate to control for for a particular query</li>
</ul>
<p><strong>Why Bayesian?</strong></p>
<ul>
<li>Bayesian happens to be the easiest approach for generative models; it’s not because we’re stuck in a philosophical debate</li>
<li>Easiest way to take the scientific structure of the assumed model and generate it, since it naturally has <em>direction</em> (i.e., priors)</li>
<li>In most cases, Bayes can be appropriate (sometimes not–cut cake with chainsaw)
<ul>
<li>Measurement error, missing data, latent variables, regularization</li>
</ul></li>
<li>It is <em>practical</em>, not <em>philosophical</em></li>
</ul>
<p><strong>Owls</strong></p>
<ul>
<li>Classic joke: Step 1 = Draw two circles, Step 2 = draw remaining owl
<ul>
<li>Programming and technical things tend to be taught this way, but we want to avoid this and document all the intermediate steps</li>
</ul></li>
<li>We need to have an explicit workflow with clear steps</li>
<li>We need to treat coding/scripting seriously, not just a means to something (apply software engineering principles, documentation, quality control)</li>
<li>Understand what you are doing, document your work and reduce error, have a respectable scientific workflow, be professional and organized to maintain <em>reproducible</em> scientific knowledge, otherwise it’s all bullshit</li>
<li>Workflow
<ol style="list-style-type: decimal">
<li>Theoretical Estimand (what are we trying to do?)</li>
<li>Scientific (Causal) Model (DAG + Generative)</li>
<li>Use 1 &amp; 2 to build appropriate statistical model</li>
<li>Simulate from 2 to validate that 3 yields 1</li>
<li>Analyze the actual data</li>
</ol></li>
</ul>
</div>
</div>
<div id="lecture2" class="section level1">
<h1>2. Garden of Forking Data</h1>
<table>
<thead>
<tr class="header">
<th>Week #</th>
<th>Lecture #</th>
<th>Chapter(s)</th>
<th>Week End</th>
<th>Notes Taken</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2</td>
<td>2, 3</td>
<td>1/6/2023</td>
<td>1/6/2023</td>
</tr>
</tbody>
</table>
<div id="summary-1" class="section level2">
<h2>Summary</h2>
<p>This scientific modeling framework provides an <em>objective</em> process to incorporate <em>subjective</em> (expert, scientfic) knowledge into the modeling process, enabling us to incorporate all of the uncertainty associated with those processes, predicated on the assumption of the causal model. Further, one of the key takeaways was that <em>samples do not need to be representative of the population for us to provide good estimates</em>. This is profound because generally we are taught the opposite, but because of the process, we can explicitly account for how we know/assume the data was generated, and use that information to create a good estimate of the quantity we are interested in. This is much more <em>practical</em> than the assumptions that are made in a typical frequentist analysis–which tend to be blindly made which ironically makes them more wrong than the “subjective” information in the generative approach. We can then use sampling of our posterior distribution(s) to answer questions about what might happen if we do another experiment, etc. (e.g., what if we take 10 more samples?). Instead of relying on asymptotics for the sampling distribution of a statistic (frequentist), we can just take samples from the posterior for any complex quantity of interest and get the uncertainty surrounding that. This is especially important once we are dealing with analytically intractable posteriors that don’t have closed form solutions. Instead of needing expert-level calculus knowledge for such problem, we just have to follow the same workflow as in this basic problem. After years of frequentist modeling, that is always full of limitations and disatisfaction in the results, this approach will lead to much more rewarding scientific discovery and confidence in the conclusions of research.</p>
<div id="my-check-for-understanding" class="section level3">
<h3>My check for understanding</h3>
<p>Let’s go through and reproduce some of the content/concepts from slides but using our own explanation, implementation and interpretation along the way.</p>
<div id="what-is-the-objective" class="section level4">
<h4>1. What is the objective?</h4>
<p>The main question asked in the demonstration was <em>what proportion of the globe is water?</em>. Thus, the quantity we are interested in is a single quantity: the <em>true</em> proportion of of the globe that is water.</p>
</div>
<div id="what-is-the-sampling-strategy" class="section level4">
<h4>2. What is the sampling strategy?</h4>
<p>We want to collect data to try to answer the question of interest. This will be done by spinning the globe and dropping a pin at a random location to indicate if it is either land or water. Some initial assumptions are</p>
<ul>
<li>All points on the globe are equally-likely to be selected</li>
<li>Any given point on the globe is either land or water (only two possibilities)</li>
<li>There is no measurement error associated with indicating if the selected point was land or water</li>
</ul>
</div>
<div id="what-is-the-generative-model" class="section level4">
<h4>3. What is the generative model?</h4>
<p>We want to consider the different variables at play here as it relates to any observed sample we get as a result of the sampling strategy. First and foremost, the primary <em>unknown</em> parameter is:</p>
<p><span class="math display">\[p=\text{proportion of water on the globe}\]</span></p>
<p>The other two at play (under this simplistic model) are:</p>
<p><span class="math display">\[N=\text{Number of globe spins} \hskip.5in W=\text{Number of spins resulting in water}\]</span>
<em>Note that the number of spins resulting in land is just <span class="math inline">\(N-W\)</span></em></p>
<p>With the variables defined, the next step is determine how these variables relate to each other. We’ll use the following DAG:</p>
<pre class="r"><code>DiagrammeR::grViz(&quot;
  digraph graph2 {
    graph [layout = dot, rankdir = LR]
    node [style = filled]
    a [label = &#39;p&#39;, fillcolor = yellow]
    b [label = &#39;N&#39;]
    c [label = &#39;W&#39;]
    a -&gt; c
    b -&gt; c
  }
&quot;, height = 200, width = 350)</code></pre>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<div id="htmlwidget-1" style="width:350px;height:200px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"\n  digraph graph2 {\n    graph [layout = dot, rankdir = LR]\n    node [style = filled]\n    a [label = \"p\", fillcolor = yellow]\n    b [label = \"N\"]\n    c [label = \"W\"]\n    a -> c\n    b -> c\n  }\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 1: Unobserved quantities are highlighted in yellow
</p>
</div>
<p>This assumes that the number of water spins observed in our sample is determined by:</p>
<ol style="list-style-type: decimal">
<li>The true proportion of water on the globe</li>
<li>The total number of spins of the globe made (samples)</li>
</ol>
</div>
<div id="what-is-the-statistical-modelestimation-procedure" class="section level4">
<h4>4. What is the statistical model/estimation procedure?</h4>
<p>Let’s suppose we execute the sampling procedure which yields the following response vector:</p>
<pre class="r"><code>observed_sample &lt;- c(1, 0, 1, 1, 1, 0, 1, 0, 1) # 1 = water; 0 = land
W &lt;- sum(observed_sample) # Number of water samples
N &lt;- length(observed_sample) # Number of spins
W; N</code></pre>
<pre><code>## [1] 6</code></pre>
<pre><code>## [1] 9</code></pre>
<p>We just need to <em>count</em> all of the ways that this sample could have arose across all of the different possibilities of <span class="math inline">\(p\)</span>, and then estimate <span class="math inline">\(p\)</span> as that of where the sample was most likely to have occurred.</p>
<div id="basic-incorrect-solution-with-finite-possibilities" class="section level5">
<h5>Basic (incorrect) solution with finite possibilities</h5>
<p>We know that there are infinitely many possibilities for <span class="math inline">\(p\)</span>. Let’s first go through this assuming the globe is that of a 4-sided die, such that each side is land or water, implying the only possibilities are <span class="math inline">\(p \in (0,.25,.50,.75,1)\)</span>. For each possible value of <span class="math inline">\(p\)</span>, what is number of ways we could have observed our sequence of data? (thinking of the generative process, starting with <span class="math inline">\(N\)</span> and <span class="math inline">\(p\)</span>).</p>
<p>First of all, we can set our <em>possible</em> set of parameter values, and the number of “sides” of the globe this implies (i.e., we’re saying that there are only 4 sides and each one is either Water or Land, so we have a limited number of <span class="math inline">\(p\)</span> values that could occur).</p>
<pre class="r"><code># Set possible values for p
p &lt;- c(0, .25, .5, .75, 1)

# Number of sides of globe
sides &lt;- length(p) - 1</code></pre>
<p>For each of the 5 possible values of <span class="math inline">\(p\)</span>, how many combinations are there that produce our observed sequence of data?</p>
<pre class="r"><code># Number of ways to observe sample for each p (this is the count of the possible sequences of indicators)
ways &lt;- (sides*p)^W * (sides*(1-p))^(N-W)
ways</code></pre>
<pre><code>## [1]   0  27 512 729   0</code></pre>
<p>Now, of those possibilities, which was the most likely to occur?</p>
<pre class="r"><code># Posterior probability
posterior_prob &lt;- ways / sum(ways)
cbind(p, ways, posterior_prob)</code></pre>
<pre><code>##         p ways posterior_prob
## [1,] 0.00    0     0.00000000
## [2,] 0.25   27     0.02129338
## [3,] 0.50  512     0.40378549
## [4,] 0.75  729     0.57492114
## [5,] 1.00    0     0.00000000</code></pre>
<p>It looks like <span class="math inline">\(p=0.75\)</span> was the most likely value of those that are possible.</p>
<p>What is key to note about the posterior probabilities is that they are relative to the total across all values of <span class="math inline">\(p\)</span>. We simply found all of the raw counts associated with each <span class="math inline">\(p\)</span> and then normalized them by the total to get the posterior probability. But this process was <em>exactly</em> the same thing as finding the <em>likelihood</em> of the data:</p>
<p><span class="math display">\[Likelihood = \prod_{i=1}^NP(X=x|p)\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the binary indicator from a single globe spin.</p>
<p>If we just look at all the <em>possible</em> sequences of indicators that could have occurred:</p>
<pre class="r"><code># Total possible sequences of indicators (each one could be a 1 or a 0)
total_possible_sequences &lt;- sides^N
total_possible_sequences</code></pre>
<pre><code>## [1] 262144</code></pre>
<p>And then divide our original combination counts by that, we’ll get <em>exactly</em> the likelihood of the data:</p>
<pre class="r"><code># Divide the total number of combinations we could have saw our sample, by the total number of possibilities
likelihood &lt;- ways / total_possible_sequences
likelihood</code></pre>
<pre><code>## [1] 0.0000000000 0.0001029968 0.0019531250 0.0027809143 0.0000000000</code></pre>
<p>However, as stated above, this will <em>not</em> change the resulting posterior distribution because the number we divided by was just a normalizing constant:</p>
<pre class="r"><code>likelihood / sum(likelihood)</code></pre>
<pre><code>## [1] 0.00000000 0.02129338 0.40378549 0.57492114 0.00000000</code></pre>
<p>So, we could also think of this problem in a different light (although it’s the SAME) and get the same result:</p>
<ol style="list-style-type: decimal">
<li>We could think of each observed value as an (unfair) coin flip (according to the value of <span class="math inline">\(p\)</span>) and calculate the likelihood of the sequence of flips (which is actually what we already did, but this is more of the “traditional” way to think about it):</li>
</ol>
<pre class="r"><code># Likelihood of sequence of observed sample
likelihood2 &lt;- p^W * (1-p)^(N-W)
likelihood</code></pre>
<pre><code>## [1] 0.0000000000 0.0001029968 0.0019531250 0.0027809143 0.0000000000</code></pre>
<pre class="r"><code># Compute posterior
likelihood2 / sum(likelihood2) # Same as before</code></pre>
<pre><code>## [1] 0.00000000 0.02129338 0.40378549 0.57492114 0.00000000</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>We could also think of this as finding the likelihood of observing the <em>total</em> number of water spins since each flip is <em>independent</em>. This is also the same as before, except we’re accounting for all of the combinations to observe the total number of water flips, not just the particular sequence:</li>
</ol>
<pre class="r"><code># Make the normalizing constant
normalizing_constant &lt;- factorial(N) / (factorial(W)*factorial(N-W))

# Multiply the likelihood by the normalizing constant by the likelihood to get the true probability of the observed sample for each value of p
probability &lt;- normalizing_constant * likelihood
probability</code></pre>
<pre><code>## [1] 0.000000000 0.008651733 0.164062500 0.233596802 0.000000000</code></pre>
<pre class="r"><code># Compute the posterior
probability / sum(probability)</code></pre>
<pre><code>## [1] 0.00000000 0.02129338 0.40378549 0.57492114 0.00000000</code></pre>
<p>Note that the normalizing constant had no effect on the posterior, but it did calculate the correct probabilities of the observed sample. In fact, this was just a Binomial distribution:</p>
<pre class="r"><code># What is the probability of observing W water values in a sample of N globe spins for each p?
dbinom(x = W, size = N, prob = p)</code></pre>
<pre><code>## [1] 0.000000000 0.008651733 0.164062500 0.233596802 0.000000000</code></pre>
<p>That is, the probability distribution for the number of water samples is:</p>
<p><span class="math display">\[W|p \sim Binomial(N, p)\]</span>
<span class="math display">\[
\begin{equation}
\begin{split}
P(W|p)
&amp;= \binom{N}{W}p^W(1-p)^{N-W} \\
&amp;= \frac{N!}{W!(N-W)!}p^W(1-p)^{(N-W)} \\
\end{split}
\end{equation}
\]</span></p>
<p>So what is going on here? We are after the distribution of probability weights associated with each possible value of <span class="math inline">\(p\)</span> (which is what the posterior distribution is). In mathematical notation, we’re just applying Bayes’ formula:</p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
P(p|sample)
&amp; = \frac{P(p)P(sample|p)}{P(sample)} \\
&amp; = \frac{P(p)P(W|p)}{P(W)} \\
&amp; = \frac{P(p)P(W|p)}{P(W \cap p = 0) + ... + P(W \cap p = 1)} \\
&amp; = \frac{P(p)P(W|p)}{P(p=0)P(W|p=0) + ... + P(p=1)P(W|p=1)} \\
\end{split}
\end{equation}
\]</span>
Each value of <span class="math inline">\(p\)</span> is equally-likely to occur (<em>uniform prior</em>), so we can factor out that term:</p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
\text{(from previous)}
&amp; = \frac{P(W|p)}{P(W|p=0) + ... + P(W|p=1)} \\
(binomials) &amp;= \frac{\binom{N}{W}p^W(1-p)^{(N-W)}}{\binom{N}{W}0^W(1-0)^{(N-W)} + ... + \binom{N}{W}1^W(1-1)^{(N-W)}} \\
&amp; = \frac{p^W(1-p)^{(N-W)}}{0^W(1-0)^{(N-W)} + ... + 1^W(1-1)^{(N-W)}} \\
&amp; = \frac{p^W(1-p)^{(N-W)}}{\text{Normalizing constant}} \\
\end{split}
\end{equation}
\]</span>
As you can see, the combination term also factors out, and the basic structure we’re left with is the <em>likelihood</em> piece that was found in <em>all three (3)</em> variations above: <span class="math inline">\(p^W(1-p)^{(N-W)}\)</span>. So when computing the posterior probability, they are relative to only terms dependent on the parameter of interest, so doesn’t matter if we use the counts, base likelihood, or the probability distribution–they are all the SAME. The counting process and the “forking data” approach is simply a means to breakdown the process of what’s happening behind the scenes in the math, so instead of just saying “do this integral” or “compute this product of the likelihood”, you’re picking apart each step of that process to gain intuition about what is happening. I’d imagine this is exactly the point of the Owl reference in the prior lecture.</p>
</div>
<div id="full-solution-p-is-a-continuous-value" class="section level5">
<h5>Full solution: <em>p</em> is a continuous value</h5>
<p>As mentioned before, the actual proportion of water on the globe can be any number between zero and one (<span class="math inline">\(p \in [0,1]\)</span>), meaning that there are “infinite” sides to the globe. The derivation at the end of the previous section illustrates that the posterior distribution for <span class="math inline">\(p\)</span> not restricted to any particular set of values. If we pick up where we left off:</p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
P(p|data)
&amp; = \frac{p^W(1-p)^{(N-W)}}{\text{Normalizing constant}} \\
\end{split}
\end{equation}
\]</span>
All we would need to do for the continuous version of <span class="math inline">\(p\)</span> to make the posterior a formal probability distribution is to find the normalizing constant such that the integral over all possible values of <span class="math inline">\(p\)</span> equals 1. Formally, with respect to <span class="math inline">\(p\)</span>,</p>
<p><span class="math display">\[\int_0^1 \frac{p^W(1-p)^{N-W}}{Constant} = 1\]</span>
This will ensure that the probabilities across all possible values of <span class="math inline">\(p\)</span> sums to one. However, it doesn’t actually matter that we find that constant necessarily, because the posterior probability is just <em>relative</em> to the range of values of <span class="math inline">\(p\)</span>. So all that really matters is:</p>
<p><span class="math display">\[P(p|data) \propto p^W(1-p)^{N-W}\]</span>
We can then plug in our data and graph the resulting distribution to make inferences about <span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[P(p|data) \propto p^6(1-p)^3\]</span></p>
<pre class="r"><code>tibble(
  p = seq(0, 1, .01), # Approximate the range of p values
  posterior = p^W*(1-p)^(N-W) # Compute the posterior
) %&gt;%
  
  # Make a plot
  ggplot() +
  geom_line(
    aes(
      x = p,
      y = posterior
    )
  ) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>After using our sample of 9, the probability weight for <span class="math inline">\(p\)</span> tends to focus near 0.70. Note that the scale of the y-axis was removed to emphasize that it doesn’t really matter what it is. We would just need to be able to calculate the area under the curve to be able to assign real probabilities to questions like <em>“what is the probability that the proportion of water is less than 0.5?”</em>.</p>
<p><em>Note: In some cases, if we used used a different priors on <span class="math inline">\(p\)</span> (e.g., Beta), the posterior will turn out to be an identifiable distribution which we know the normalizing constant.</em></p>
</div>
<div id="updating-the-posterior" class="section level5">
<h5>Updating the posterior</h5>
<p>So when we talk “Bayesian updates” or updating the posterior distribution, what does this mean? Since the point of it is to be able to update a model with new information, my gut used to tell me that we were somehow adding our current knowledge about the parameter into the new <em>prior</em> distribution, and then updating the new posterior with an updated prior and only using new data in the likelihood. While in a way this might be the right way to think about it (i.e., if I have a posterior right now, isn’t that the most current knowledge about the parameter, so if I want to collect more data, wouldn’t I want to use knowledge up to this point as the prior instead of reverting back to the original prior and just adding more data to the collective sample?), in these examples we were doing something different: we’re just seeing how the posterior changes as more data is added to the sample (i.e., observed sequence of data points).</p>
<p>Let’s start with just focusing on the basic example (i.e., 4 sided-globe) for now. We just need to loop through the observed sample, and calculate the posterior probabilities for each value of <span class="math inline">\(p\)</span> as a new observation comes in:</p>
<pre class="r"><code># Set the prior probability (uniform over the possibly choices)
prior &lt;- rep(1 / length(p), length(p))

# Set the current posterior as the prior (before any data collected)
last_posterior &lt;- prior

# Make result set
results &lt;- tibble()

# For each value in the observed sample 
for(i in 1:N) {
  
  # 1. Get the sub-sample
  sub_sample &lt;- observed_sample[1:i]
  
  # 2. Compute metrics (the number of water samples, and the total number of spins)
  W_temp &lt;- sum(sub_sample)
  N_temp &lt;- length(sub_sample)
  
  # 3. Compute the likelihood for each p
  temp_likelihood &lt;- p^W_temp * (1 - p)^(N_temp - W_temp)
  
  # 4. Posterior
  temp_posterior &lt;- temp_likelihood / sum(temp_likelihood)
  
  # 5. Add to results
  results &lt;-
    results %&gt;%
    bind_rows(
      tibble(
        sample = i,
        sequence = paste(sub_sample, collapse = &quot;,&quot;),
        p,
        likelihood = temp_likelihood,
        current = temp_posterior,
        last = last_posterior
      )
    )
  
  # Set the new last posterior
  last_posterior &lt;- temp_posterior

}

results %&gt;%
  
  # Send down the rows
  pivot_longer(
    cols = c(last, current)
  ) %&gt;%
  
  # Make a plot
  ggplot() +
  geom_col(
    aes(
      x = factor(p),
      y = value,
      fill = name
    ),
    color = &quot;black&quot;,
    alpha = .75,
    width = .25,
    position = &quot;identity&quot;
  ) +
  facet_wrap(
    ~paste0(&quot;Spin: &quot;, factor(sample), &quot; \nSample: &quot;, sequence)
  ) +
  theme(
    legend.position = &quot;top&quot;,
    panel.background = element_blank(),
    panel.grid.major.y = element_line(colour = &quot;gray&quot;)
  ) +
  xlab(&quot;p&quot;) +
  ylab(&quot;Posterior Probability&quot;) +
  labs(
    fill = &quot;Posterior&quot;
  ) +
  scale_fill_manual(
    values = c(&quot;blue&quot;, &quot;darkgray&quot;)
  ) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The blue bars show the posterior probability for each possible value of <span class="math inline">\(p\)</span> <em>after</em> the newest observation was made, and the gray bars show it <em>before</em> the newest observation was made. This illustrates the incremental impact of adding more data to the sample on the resulting posterior distribution.</p>
<p>We can apply this same process to the <em>continuous</em> (correct) possible set of values for <span class="math inline">\(p\)</span> (in fact, we’ll create the curves by performing the exact same procedure to a larger, discrete set of values but make the display appear continuous):</p>
<pre class="r"><code># Approximate the set of inifinite p-values by a large set of discrete ones
p_continuous &lt;- seq(0, 1, .01)

# Set the prior probability (uniform over the possibly choices)
prior &lt;- rep(1 / length(p_continuous), length(p_continuous))

# Set the current posterior as the prior (before any data collected)
last_posterior &lt;- prior

# Make result set
results &lt;- tibble()

# For each value in the observed sample 
for(i in 1:N) {
  
  # 1. Get the sub-sample
  sub_sample &lt;- observed_sample[1:i]
  
  # 2. Compute metrics (the number of water samples, and the total number of spins)
  W_temp &lt;- sum(sub_sample)
  N_temp &lt;- length(sub_sample)
  
  # 3. Compute the likelihood for each p
  temp_likelihood &lt;- p_continuous^W_temp * (1 - p_continuous)^(N_temp - W_temp)
  
  # 4. Posterior
  temp_posterior &lt;- temp_likelihood / sum(temp_likelihood)
  
  # 5. Add to results
  results &lt;-
    results %&gt;%
    bind_rows(
      tibble(
        sample = i,
        sequence = paste(sub_sample, collapse = &quot;,&quot;),
        p_continuous,
        likelihood = temp_likelihood,
        current = temp_posterior,
        last = last_posterior
      )
    )
  
  # Set the new last posterior
  last_posterior &lt;- temp_posterior
  
}

results %&gt;%
  
  # Send down the rows
  pivot_longer(
    cols = c(last, current)
  ) %&gt;%
  
  # Make a plot
  ggplot() +
  geom_area(
    aes(
      x = p_continuous,
      y = value,
      fill = name
    ),
    color = &quot;black&quot;,
    alpha = .65,
    position = &quot;identity&quot;
  ) +
  facet_wrap(
    ~paste0(&quot;Spin: &quot;, factor(sample), &quot; \nSample: &quot;, sequence)
  ) +
  theme(
    legend.position = &quot;top&quot;,
    panel.background = element_blank(),
    panel.grid.major.y = element_line(colour = &quot;gray&quot;),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  ) +
  xlab(&quot;p&quot;) +
  ylab(&quot;Posterior Probability&quot;) +
  labs(
    fill = &quot;Posterior&quot;
  ) +
  scale_fill_manual(
    values = c(&quot;blue&quot;, &quot;darkgray&quot;)
  ) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Again, these curves are actually approximated here. In practice, we would need to calculate the area underneath the curve to get exact answers about probabilities. <em>Note: We could parameterize the Beta distribution to get the normalizing constant for calculating the actual posterior probabilities that fits this distribution.</em></p>
</div>
</div>
</div>
</div>
<div id="homework" class="section level2">
<h2>Homework</h2>
<ol style="list-style-type: decimal">
<li>Suppose the globe tossing data had turned out to be 4 water and 11 land. Construct the posterior distribution.</li>
</ol>
<p>We’ll cheat a little bit and use the approach of using a large, discrete list of possible values for <span class="math inline">\(p\)</span>, and plot it as if it is continuous (we could also just use the Beta distribution). With that, all we need to do is change the values of <span class="math inline">\(W\)</span> and <span class="math inline">\(N\)</span> and use the same code as above.</p>
<pre class="r"><code>W_new &lt;- 4
N_new &lt;- 15
tibble(
  p = seq(0, 1, .01), # Approximate the range of p values
  posterior = p^W_new*(1-p)^(N_new-W_new) # Compute the posterior
) %&gt;%
  
  # Make a plot
  ggplot() +
  geom_line(
    aes(
      x = p,
      y = posterior
    )
  ) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li>Using the posterior distribution from (1), compute the posterior predictive distribution for the next 5 tosses of the same globe.</li>
</ol>
<p>Okay here I’ll finally acknowledge that the posterior can be written as a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta</a> distribution, which gives us the normalizing constant needed to make it a real probability distribution (i.e., the area sums to 1). It has the following <em>probability density function (PDF)</em>:</p>
<p><span class="math display">\[f(x|\alpha, \beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\]</span>
where <span class="math inline">\(x \in [0,1]\)</span>, <span class="math inline">\(\alpha, \beta &gt; 0\)</span>, and <span class="math inline">\(\Gamma(n) = (n-1)!\)</span>. If we make the following reparameterizations from our set of variables:</p>
<p><span class="math display">\[p=x\]</span>
<span class="math display">\[W = \alpha - 1\]</span>
<span class="math display">\[N-W = \beta - 1\]</span>
we get:</p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
f(p|W,N)
&amp; = \frac{\Gamma(W+1+N-W+1)}{\Gamma(W+1)\Gamma(N-W+1)}p^W(1-p)^{N-W} \\
&amp; = \frac{\Gamma(N+2)}{\Gamma(W+1)\Gamma(N-W+1)}p^W(1-p)^{N-W} \\
&amp; = \frac{(N+1)!}{W!(N-W)!}p^W(1-p)^{N-W} \\
\end{split}
\end{equation}
\]</span></p>
<p><em>Note that since <span class="math inline">\(p\)</span> is continuous, this function represents the probability density at any particular value of <span class="math inline">\(p\)</span>. To get any positive probability, we must integrate this function over a range of <span class="math inline">\(p\)</span> values. Hence the <span class="math inline">\(f\)</span> notation.</em></p>
<p>Let’s quickly plug in <span class="math inline">\(W=4\)</span> and <span class="math inline">\(N=15\)</span> to confirm (at least visually) that this function produces the same posterior we made to answer the last question. We’ll do this by evaluating the density of the derived Beta distribution at range of possible <span class="math inline">\(p\)</span> values. <em>Note we’ll have to use the parameterizations for the Beta distribution that are built into <code>R</code></em>.</p>
<pre class="r"><code># Reparameterize
alpha &lt;- W_new + 1
beta &lt;- N_new - W_new + 1

# Make a data frame
tibble(
  p = seq(0, 1, .01), # Approximate the range of p values (same as before)
  posterior = dbeta(p, shape1 = alpha, shape2 = beta) # Compute the actual posterior density
) %&gt;%
  
  # Make a plot
  ggplot() +
  geom_line(
    aes(
      x = p,
      y = posterior
    )
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>From inspection, it appears that they are essentially the same curves. <em>Note that this time I kept the y-axis labels because this is the density for the actual probability distribution</em>.</p>
<p>So back to the question: how do we find the posterior predictive distribution for the next 5 globe spins? Well, if the globe is spun 5 more times, then we can get water on 0, 1, 2, 3, 4, or all 5 spins. Our quest is to figure out the likelihood of each of those possible outcomes based on what we currently know about <span class="math inline">\(p\)</span> (i.e., the posterior distribution). To do this, we’ll use our posterior distribution to run a simulation of the experiment using the following steps:</p>
<ol style="list-style-type: lower-roman">
<li>Select a random value of <span class="math inline">\(p\)</span> from the posterior</li>
<li>Draw a random <span class="math inline">\(binomial\)</span> realization where <span class="math inline">\(N=5\)</span></li>
<li>Repeat steps i-ii 10000 times</li>
<li>Graph the results</li>
</ol>
<pre class="r"><code># Set some parameters
set.seed(123)
n_experiment &lt;- 5 # Number of new spins we&#39;re going to conduct
s &lt;- 10000 # Number of simulations

# 1. Draw random values of p from the posterior
p_rand &lt;- rbeta(n = s, shape1 = alpha, shape2 = beta) # Same parameters as before

# 2. For each p, run a binomial experiment (represents samples of W)
w_rand &lt;- rbinom(n = s, size = n_experiment, prob = p_rand)

# Make a data frame
tibble(
  w = w_rand
) %&gt;%
  
  # For each w
  group_by(w) %&gt;%
  
  # Compute the total
  summarise(
    count = n()
  ) %&gt;%
  
  # Add proportion
  mutate(
    proportion = count / sum(count)
  ) %&gt;%
  
  # Make a plot
  ggplot(
    aes(
      x = factor(w)
    )
  ) +
  geom_col(
    aes(
      y = proportion
    )
  ) +
  geom_text(
    aes(
      y = proportion,
      label = paste0(round(proportion*100,1), &quot;%&quot;)
    ),
    vjust = -.1
  ) +
  scale_y_continuous(
    labels = scales::percent
  ) +
  xlab(&quot;W&quot;) +
  ylab(&quot;Probability&quot;) +
  labs(
    title = paste0(&quot;Posterior Predictive Distribution for &quot;, n_experiment, &quot; more spins.&quot;)
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Another way to think about what we’re doing here is:</p>
<ol style="list-style-type: lower-roman">
<li>Find the collection of <em>density</em> values for all <span class="math inline">\(p\)</span> in the posterior distribution</li>
<li>Find the probability distribution of the possible outcomes from a Binomial distribution where <span class="math inline">\(N=5\)</span> for all possible values of <span class="math inline">\(p\)</span> (i.e., independent of our posterior)</li>
<li>Take the average probability value for each possible outcome in (ii) over all values of <span class="math inline">\(p\)</span>, <em>weighted</em> by the posterior density in (i)</li>
</ol>
<pre class="r"><code># Make a set of p representive of its domain
p_alt &lt;- seq(0, 1, .001) # Supposed to represent continuous p

# 1. Posterior density values for each p
posterior_alt &lt;- dbeta(p_alt, shape1 = alpha, shape2 = beta)

# 2. Probability distribution for each outcome for each p
possible_outcomes &lt;- 0:n_experiment
likelihood_alt &lt;- 
  possible_outcomes %&gt;% 
  map_df(
    ~
      tibble(
        binomial_probability = dbinom(x = .x, size = n_experiment, prob = p_alt),
        p = p_alt,
        outcome = .x
      )
  )

# 3. Get the posterior predictive distribution
posterior_predictive_distribution &lt;-
  likelihood_alt %&gt;%
  
  # Join to attach the posterior density weight to each value of p
  inner_join(
    y = 
      tibble(
        p = p_alt,
        posterior_density = posterior_alt
      ),
    by = &quot;p&quot;
  ) %&gt;%
  
  # For each possible outcome
  group_by(outcome) %&gt;%
  
  # Compute the weighted-average probability
  summarise(
    posterior_probability = sum(binomial_probability * posterior_density) / sum(posterior_density)
  )

# 4. Make a plot
posterior_predictive_distribution %&gt;%
  ggplot(
    aes(
      x = factor(outcome)
    )
  ) +
  geom_col(
    aes(
      y = posterior_probability
    )
  ) +
  geom_text(
    aes(
      y = posterior_probability,
      label = paste0(round(posterior_probability*100,1), &quot;%&quot;)
    ),
    vjust = -.1
  ) +
  scale_y_continuous(
    labels = scales::percent
  ) +
  xlab(&quot;W&quot;) +
  ylab(&quot;Probability&quot;) +
  labs(
    title = paste0(&quot;Posterior Predictive Distribution for &quot;, n_experiment, &quot; more spins.&quot;)
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>The distributions from the two approaches are merely identical (slight differences due to simulation variability and inexact integration).</p>
<ol start="3" style="list-style-type: decimal">
<li>Use the posterior predictive distribution from (2) to calculate the probability of 3 or more water samples in the next 5 tosses.</li>
</ol>
<p>Using the posterior predictive distribution above, we can look at the percent of simulations that resulted in 3 or more water samples:</p>
<pre class="r"><code>mean(w_rand &gt;= 3)</code></pre>
<pre><code>## [1] 0.1771</code></pre>
<p>So there is a 0.177 probability of 3 or more water samples in the next 5 tosses. However, this point estimate is not totally sufficient because we haven’t reported any uncertainty associated with it. Since we know that <span class="math inline">\(W|p \sim Binomial(n,p)\)</span>, the <span class="math inline">\(P(W&gt;=3|p,N)\)</span> is already determined. In this case, we can just calculate it for each random value of <span class="math inline">\(p\)</span> sampled from the posterior:</p>
<pre class="r"><code># Compute the binomial probability
prob_binom &lt;- 1 - pbinom(q = 2, size = n_experiment, prob = p_rand)

# Make a plot
ggplot() +
  geom_density(
    aes(
      x = prob_binom
    )
  ) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  ) +
  xlab(&quot;P(W&gt;=3|N=5)&quot;) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>This one has been stumping me a bit, and I’m not totally confident this is the correct result. Another way I was thinking about it was similar the alternative in (2), namely that we are finding the <span class="math inline">\(P(W&gt;=3|N=5)\)</span> for each value of <span class="math inline">\(p\)</span>, and then weighting that by the posterior density of <span class="math inline">\(p\)</span>. However, it seems like we should be sampling from the predictive distribution in (2) somehow, but if we do that it seems like the precision of our estimates would then just be determined by the number of simulations we run, not the data, which also doesn’t make sense.</p>
</div>
<div id="notes-1" class="section level2">
<h2>Notes</h2>
<p><strong>Goal: Estimate the percent of the globe that is covered in water</strong></p>
<ul>
<li>Think of spinning the globe and stopping on a point and repeating many times</li>
<li>How do we use that collection of points to come up with an estimate? That’s the goal of today’s lecture</li>
<li>First thought is just indicate each time whether land or water appear as the point; however, how does the shape of the globe impact the likelihood that I will come up with land or water on a “random” toss? Has to do with sampling strategy</li>
</ul>
<ol style="list-style-type: decimal">
<li>Define a generative model</li>
</ol>
<ul>
<li>Think conceptually about scientifically how the sample was produced (how do variables influence one another)</li>
<li>Variables: Things we <em>want</em> to observe/estimate or things we actually do observe</li>
</ul>
<p><span class="math display">\[\bf{p} = \text{proportion of water}\hskip.5inW=\text{water observations}\]</span>
<span class="math display">\[N = \text{number of tosses}\hskip.5inL=\text{land observations}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Define a specific estimand</li>
</ol>
<p>Were interested in the true proportion of water <strong>p</strong></p>
<ol start="3" style="list-style-type: decimal">
<li>Design a statistical way to produce estimate</li>
</ol>
<ul>
<li>How are these related to each other?
<ul>
<li>N influences W and L (the more tosses leads to change on other variables)</li>
<li>p also influences W and L (i.e., the true proportion dictates the number of water observations and land observations)</li>
<li>The DAG shows relationships, but not what the relationships <em>are</em>. We can say <span class="math inline">\(W,L=f(p,N)\)</span>; what is <span class="math inline">\(f\)</span>?</li>
</ul></li>
<li>Assume a model (e.g., <span class="math inline">\(p\)</span> = .25, then count likely the sample was under that model, do that for all possible models)</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Test (3) using (1)</li>
</ol>
<pre class="r"><code>sim_globe &lt;-
  function(p = .7, N = 9) {
    sample(
      c(&quot;W&quot;,&quot;L&quot;), # Possible observations
      size = N, # Number of tosses
      prob = c(p, 1-p), # The probability of each possible observation
      replace = TRUE)
  }
sim_globe()</code></pre>
<pre><code>## [1] &quot;W&quot; &quot;W&quot; &quot;W&quot; &quot;W&quot; &quot;W&quot; &quot;W&quot; &quot;W&quot; &quot;W&quot; &quot;W&quot;</code></pre>
<pre class="r"><code>replicate(sim_globe(p =.5, N=9), n=10)</code></pre>
<pre><code>##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
##  [1,] &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  
##  [2,] &quot;L&quot;  &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  
##  [3,] &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  
##  [4,] &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  
##  [5,] &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;L&quot;  
##  [6,] &quot;W&quot;  &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  &quot;W&quot;  
##  [7,] &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;L&quot;  &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  
##  [8,] &quot;W&quot;  &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  &quot;W&quot;  
##  [9,] &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;L&quot;  &quot;W&quot;  &quot;L&quot;  &quot;W&quot;  &quot;W&quot;  &quot;L&quot;</code></pre>
<ul>
<li>Test the intent of the code first</li>
<li>If our procedure doesn’t work when <em>we know</em> the answer, it certainly won’t when we <em>don’t</em> know the answer</li>
</ul>
<p>Infinite sample:</p>
<p><span class="math display">\[p^W(1-p)^L\]</span> Posterior probability:</p>
<p><span class="math display">\[p = \frac{(W+L+1)!}{W!L!}p^W(1-p)^L\]</span></p>
<ul>
<li>This is a <em>Beta</em> distribution, and the likelihood was a <em>Binomial</em>.</li>
<li>The minimum sample size for Bayesian analysis is 1.</li>
<li>The shape of the posterior distribution embodies the sample size</li>
<li>No point estimate, we work with the entire posterior distribution</li>
<li>The distribution <em>is</em> the estimate; always use the entire distribution, never a single point</li>
<li>The fact that an arbitrary interval contains an arbitrary value is not meaningful</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Analyze sample, summarize</li>
</ol>
<ul>
<li>Implications depend on entire posterior</li>
<li>Average over the uncertainty of the posterior</li>
<li>What can we do with the posterior distribution?
<ul>
<li>We can take samples from it, and then do calculations with the samples</li>
</ul></li>
<li>Posterior Prediction
<ul>
<li>Given what we’ve learned, what would happen if we took more samples?</li>
<li>Sampling distribution (predictive distribution) of draws represents the likelihood of each outcome in a new experiment for a particular value</li>
<li>The <em>posterior predictive</em> distribution then represents the entire distribution of the statistic of interest, and contains all the uncertainty around that estimate (analogous to the sampling distribution of a statistic (e.g., mean) in the frequentist paradigm, except this is completely model-driven by the posterior instead of based on asymptotics in the frequentist approach)</li>
<li>Sampling turns calculus into a data summary problem; this is important when models get complex and numerically intractable to compute by hand</li>
</ul></li>
<li>This generative, Bayesian framework is the optimal approach for causal estimation <em>if your model is correct</em>.</li>
<li>It honestly carries out the assumptions we put into it, using logical implications</li>
<li>Quantitative framework/asset that activates our qualitative knowledge as scientists, subject matter experts, etc. Let’s the subjective and objective work together. Subjectivity is expertise.</li>
</ul>
<p><strong>Misclassification</strong></p>
<ul>
<li><p>Use circles around variable in DAG to represent unobserved vs. observed variables</p></li>
<li><p>Imagine the true number of water samples (W) are unobserved (e.g., measurement error, data error, etc.)</p></li>
<li><p>We observe a <em>contaminated</em> W (called W*) that is the <em>misclassified</em> sample</p></li>
<li><p>W* is caused by the <em>measurement process</em> M. We can get get back to the correct posterior distribution for p if we use M through W*.</p></li>
<li><p>The posterior is honest about the uncertaintly induced by the misclassification process</p></li>
<li><p>When there is measurement error, model it instead of ignoring it (same for missing data, compliance, inclusion)</p></li>
<li><p><em>Key point: Samples do not need to be representative of population to provide good estimates, since we can correct them through our causal diagram (modeling the source, sampling process, etc.)</em></p></li>
<li><p>This concept may also arise if, for example, the globe was not spun equally likely for every point to be selected.</p></li>
</ul>
</div>
</div>
<div id="lecture3" class="section level1">
<h1>3. Geocentric Models</h1>
<table>
<thead>
<tr class="header">
<th>Week #</th>
<th>Lecture #</th>
<th>Chapter(s)</th>
<th>Week End</th>
<th>Notes Taken</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>3</td>
<td>4</td>
<td>1/13/2023</td>
<td>1/10/2023</td>
</tr>
</tbody>
</table>
<div id="summary-2" class="section level2">
<h2>Summary</h2>
<p>We don’t actually need data to construct a model. Our prior distributions, which account for our baseline knowledge about what reasonable values for unknown parameters may be, can produce estimates on their own. A bare minimum strategy is to choose these such that before seeing data, the output of our model produces scientifically reasonable results–there is no reason to allow our model to produce results that we know cannot happen. Then, our data can be introduced to help guide the parameters to an area of focus. In this sense (thinking of the example of points bumping around in parameter space), the data we collect is really just a tool for our model–the model is the central focus, the data just helps the model go to where it needs to go. Also, the idea that there are no correct priors and that priors are just (normalized) posteriors from previous data, make the idea of Bayesian updating very intuitive. It will be interesting to see in coming lectures how we can extend this linear model framework to more “real life” problems with observational data that have potentially tens or hundreds or thousands of potential drivers, and strategies for accounting for the most important ones. Obviously these basic examples are great to build a foundation, but it seems like a huge (sometimes impossible) hurdle to have the time and resources to be able to fully vet out expert-driven causal diagrams and generative models that fully account for all the things, especially in fast-paced environments when everyone is just so busy and there are so many projects to attend to. I’d imagine this is one of the reasons why frequentist analysis persists so much (at least in medical research), because it’s the way it’s been done and therefore you can get more things done faster, even though in an ideal state a Bayesian approach <em>is</em> the right way to go. Definitely something I’ve thought about time and time again–how can we balance the rigor and detail needed to construct the appropriate models to achieve better inference while still being efficient with peoples’ time? Part of it probably has to do with proving to stakeholders that the inference gained from the “quicker” way is less informative (or just plain wrong) compared to the more involved approach.</p>
</div>
<div id="notes-2" class="section level2">
<h2>Notes</h2>
<ul>
<li>Statistical models can attain arbitrarily accurate predictions without having any explanation or accurate structure (i.e., the model is just plain wrong, but happens to produce accurate predictions at the right time)
<ul>
<li>Example of this is a previous explanation of orbit pattern of Mars: assuming Earth at the center (geocentric), Mars orbits around Earth but also it’s own local orbit (epi-cycles). Using this model, they got very accurate predictions, but this mechanism is completely wrong.</li>
<li>Orbits are actually elliptical and around the sun, not Earth</li>
<li>Even though the first one predicts accurately, because the structure/mechanism is wrong, it doesn’t extend or generalize to other things. However, the correct mechanism is able to explain orbit patterns of all planets in the solar system.</li>
</ul></li>
<li>Linear regression is a large class of statistical golems
<ul>
<li><strong>Geogentric</strong>: describes associations, makes good predictions; mechanistically always wrong (but useful), very good approximation; meaning doesn’t depend on the model, depends on an external causal model. Nothing wrong with it unless you actually believe it is the true mechanism.</li>
<li><strong>Gaussian</strong>: Abstracts away from detail of general error model; mechanistically silent. General argument about symmetry of error.</li>
</ul></li>
</ul>
<p><strong>Gaussian</strong></p>
<ul>
<li>Example: Flip coin, each person take a step to left or right depending on heads/tails, measure distance from center; makes a normal distribution. Why?
<ul>
<li>There are more ways for a sequence of coin tosses to get you close to the middle than there are to get you to the left or right</li>
<li>Many natural processes attract to this behavior because it is adding together small differences</li>
</ul></li>
<li>Two arguments:
<ul>
<li>Generative: summed fluctuations tend towards normal. Ex. growth–added fluctuations over time, same age weight tends to be gaussian</li>
<li>Inferential: estimating mean/variance. Best to use since least informative (maximum entropy)</li>
</ul></li>
<li>Variable does not need to be normally distributed for normal model to be useful. Machine for estimating mean/variance. Contains the least assumptions. (central limit theorem)</li>
</ul>
<p><strong>Skills/Goals for Lecture</strong></p>
<ol style="list-style-type: decimal">
<li>Learn a standardized language for representing models (generative and statistical)</li>
<li>Calculate posteriors with multiple unknown parameters</li>
<li>How to construct and understand linear models; how to construct posterior predictions from them</li>
</ol>
<p><strong>Reminder of the owl</strong></p>
<ol style="list-style-type: decimal">
<li>State a clear question; descriptive, causal, anything; but needs to be clear</li>
<li>Sketch causal assumptions using DAGs; good way for non-theorists to realize they have a lot of subject knowledge and can get it on paper</li>
<li>Define a generative model; generates synthetic observations</li>
<li>Use generative model to build estimator; causal/generative assumptions embedded</li>
<li>Test, analyze</li>
<li>Profit: we realize our model was useful, or terrible; either way we gain something</li>
</ol>
<p><strong>Describing models</strong></p>
<ol style="list-style-type: decimal">
<li>Lists variables</li>
<li>Define each variable as a deterministic or distributional function of other variables</li>
</ol>
<p><strong>Exercise</strong></p>
<ol style="list-style-type: decimal">
<li>Goal: Describe the association between adult weight and height</li>
<li>Height causes weight H–&gt;W&lt;–(U) (unobserved influences on body weight)</li>
<li>Generative/scientific model: <span class="math inline">\(W=f(H,U)\)</span>, <span class="math inline">\(W=\beta H + U\)</span></li>
</ol>
<pre class="r"><code>sim_weight &lt;-
  function(H,b,sd) {
    U &lt;- rnorm(length(H),0,sd)
    W&lt;-b*H + U
    return(W)
  }
# Generate height
H &lt;- runif(200,130,170)
W &lt;- sim_weight(H, b=.5, sd= 5)
plot(W~H,col=2, lwd = 3)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p><span class="math display">\[W_i=\beta H_i + U_i\]</span>
<span class="math display">\[U_i \sim Normal(0,\sigma)\]</span>
<span class="math display">\[H_i \sim Uniform(130, 170)\]</span>
4. Statistical model (estimator)</p>
<ul>
<li>We want to estimate how the average weight changes with height.</li>
</ul>
<p><span class="math display">\[E(W_i|H_i)=\alpha + \beta H_i\]</span></p>
<ul>
<li>Posterior distribution</li>
</ul>
<p><span class="math display">\[P(\alpha, \beta, \sigma|H_i,W_i) = \frac{P(W_i|H_i,\alpha,\beta,\sigma)P(\alpha,\beta,\sigma)}{Z}\]</span></p>
<ul>
<li>Gives the posterior probability of a specific regression line
<ul>
<li>Likelihood: Number of ways we could produce <span class="math inline">\(W_i\)</span>, given a line</li>
<li>Prior: The previous posterior distribution; normalized number of ways previous data could have been produced.</li>
</ul></li>
</ul>
<p><span class="math display">\[W_i \sim Normal(\mu_i, \sigma)\]</span>
<span class="math display">\[\mu_i = \alpha + \beta H_i\]</span></p>
<ul>
<li><p>Generally more useful to look at the lines (parameter implications together), instead of individual parameters</p></li>
<li><p>Quadratic approximation</p>
<ul>
<li>Approximate the posterior distribution using a multivariate Gaussian distribution</li>
<li>Use the <code>quap</code> function in the <code>rethinking</code> package</li>
</ul></li>
</ul>
<p><strong>Prior Predictive Distribution</strong></p>
<ul>
<li>Should express scientific knowledge, but <em>softly</em></li>
<li>We can make the model make predictions without using data</li>
<li>Not make ranges that represent the data, but rather just those that make sense based on current knowledge</li>
<li>Account for basic reasonable constraints: In general, patients with more weight have more height, and the weight is less than the height, so <span class="math inline">\(\beta\)</span> is probably between <span class="math inline">\([0,1]\)</span>.</li>
<li>Use these to define some lines based on the assumptions</li>
</ul>
<pre class="r"><code>n &lt;- 1000
a &lt;- rnorm(n,0,10)
b &lt;- runif(n,0,1)
plot(NULL,xlim=c(130,170),ylim=c(50,90),xlab=&quot;height(cm)&quot;,ylab=&quot;Weight(kg)&quot;)
for (j in 1:50) abline(a=a[j],b=b[j],lwd=2,col=2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<ul>
<li>Some of these are probably not plausible (e.g., high height with low weight). Slopes look good but not intercept</li>
<li>We can adjust as needed to create what makes sense</li>
<li>There are no correct priors; only scientifically justifiable priors</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Validate Model</li>
</ol>
<ul>
<li>Bare minimum to test statistical model</li>
<li>Not because you wrote it, more so to make sure your model works</li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>Analyze data</li>
</ol>
<ul>
<li>Plug in your data set into your process</li>
<li>Parameters are not independent, can’t interpret as such</li>
<li>Push out posterior predictions</li>
</ul>
</div>
</div>
<div id="lecture4" class="section level1">
<h1>4. Categories and Curves</h1>
<table>
<thead>
<tr class="header">
<th>Week #</th>
<th>Lecture #</th>
<th>Chapter(s)</th>
<th>Week End</th>
<th>Notes Taken</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>4</td>
<td>4</td>
<td>1/13/2023</td>
<td>1/11/2023</td>
</tr>
</tbody>
</table>
<div id="summary-3" class="section level2">
<h2>Summary</h2>
<p>The idea of <em>total</em> vs. <em>direct</em> effects is about specifying the statistical model that will allow you to observe the complete effect (i.e., including differences that could be explained by something else in the model) compared to parsing out differences explained by the variable after adjusting for effects explained through other variables. In the lecture example, the total causal effect of sex on weight was determined by using a (Bayesian) intercept-only model, which showed considerable difference is mean weights between male/female. However, when assessing the direct causal effect, a parameter was added to fit separate slopes for male/female in order to block out the effect of sex on weight that is observed through other causes (in this case, height), such that the resulting estimator looked at mean differences in weight <em>at each height</em>–the posterior distribution for this difference yielded little to no direct effect, indicating that most of the difference in weight between male/females is due to height differences. Another interesting aspect of this lecture was how to think about which way an arrow should go when drawing the causal diagram. You should think of the interventions we are willing to consider, and which make logical sense. For example, we drew <span class="math inline">\(H \rightarrow W\)</span> because, given a height, it makes sense to employ interventions (such as weight loss program, exercise, etc.) that could presumably impact the resulting weight, but it doesn’t make a lot of sense to think of trying to change someone’s height given their weight. Also, declaring something as a <em>cause</em> of something, generally you first want to think about whether an intervention can be employed, but if not can still make sense if it is a proxy for something else (e.g., age encapsulates time, among many other things that presumably do cause height). We can use flexible curves to fit things (e.g., splines), but we want to make sure we vet out any erroneous areas where estimates don’t make sense, and add necessary restrictions to alleviate. So far, these lectures have given great optimism and excitement for how to approach modeling. I want to be confident in the models I produce, and I think the generative framework is the right approach to be able to believe in the results you are producing. I see so much published research from observational data that declare something statistically significant for a given research hypothesis and say “we adjusted for all these confounders”. Even if I feel fine about the math/statistical procedure, I’m always skeptical about the conclusions that are drawn from it, and quite frankly, don’t feel like it means much at all for really making a decision–there are just too many limitations about all sorts of things. The generative approach gives the tools and rigor to be much more confident in the results, and if we can be more demanding of that rigor, time and energy, it should yield more benefit in the long run. I’d rather spend more time getting to a confident conclusion than just pumping out results.</p>
</div>
<div id="homework-1" class="section level2">
<h2>Homework</h2>
<ol style="list-style-type: decimal">
<li><p>From the <code>howell1</code> dataset, consider only the people younger than 13 years old. Estimate the causal association between age and weight. Assume age influences weight through two paths. First, age influences height, and height influences weight. Second, age directly influences weight through age-related changes in muscle growth and body proportions. Draw the DAG that represents these causal relationships. And then write a generative simulation that takes age as an input and simulates height and weight, obeying the relationships in the DAG.</p></li>
<li><p>Use a linear regression to estimate the <strong>total</strong> causal effect of each year of growth on weight.</p></li>
<li><p>Now suppose the causal association between age and weight might be different between boys and girls. Use a single linear regression, with a categorical variable for sex, to estimate the total causal effect of age on weight separately for boys and girls. How do boys and girls differ? Provide one or more posterior contrasts as a summary.</p></li>
</ol>
</div>
<div id="notes-3" class="section level2">
<h2>Notes</h2>
<ul>
<li>The linear regression can approximate anything, so we need to design it with the causal model in mind</li>
<li>Generative models + multiple estimands, we’ll have multiple estimands</li>
<li>Need post-processing of posterior distribution to gain inference of joint distributino</li>
<li>We require categories, splines, etc. to build causal estimators</li>
<li>Need to <em>stratify</em> by category to get at the estimands we want (separate lines)</li>
</ul>
<p><strong>Example</strong></p>
<ul>
<li>Extend example above to include patient sex, age</li>
<li>Need to determine how height, weight, sex are causally related (add to DAG), and statistically related</li>
<li>To determine which way the arrows go, think about the interventions you’re willing to consider</li>
<li>Don’t have to draw them, but the implied unobserved causes of each variable are implied
<ul>
<li>These are ignorable <em>unless shared</em> across variables</li>
<li>Ex. temperature is a cause of sex and weight in some species</li>
</ul></li>
<li>What is the causal effect of S on W?
<ul>
<li>Accounts for direct and indirect effect</li>
<li>We can also ask what is the direct causal effect of S on W?</li>
<li>These questions require different models</li>
</ul></li>
<li>Generally want to assign the same prior for parameters for each category level (below)
<ul>
<li>Using indexing is advantageous because you have symmetry such that all parameters can get the same prior, they are all interpreted the same within their levels</li>
<li>Using indicators makes parameters relative to other levels, which causes you have to put priors on other parameters because it is an adjustment parameter (one is an average, one is an adjustment to an average)</li>
</ul></li>
</ul>
<p><span class="math display">\[W_i \sim Normal(\mu_i, \sigma) \hskip.1in \mu_i=\alpha_{S[i]}\]</span>
<span class="math display">\[\alpha = [\alpha_1, \alpha_2] \hskip.1in \alpha_j \sim Normal(60,10)\]</span>
<strong>Total Causal Effect</strong></p>
<ul>
<li>Simulate one data set of all males, another of all females, look at the average difference in weight
<ul>
<li>This is the actual causal effect</li>
<li>Then you can generate a random data set, run the modeling process, and then ensure that the model provides the expected estimate</li>
</ul></li>
<li>Look at the posterior distribution of the mean difference, and randomly draw samples from the individual posteriors and compute the differences to answer questions like “what is the probability that a randomly selected male will be heavier than a randomly selected female?”</li>
<li>This was basically just an intercept-only model for sex, and the effect due to height would be captured in that difference</li>
</ul>
<p><strong>Direct Effect</strong></p>
<ul>
<li>How do we partial out the indirect effect of Height (block it)?
<ul>
<li>Stratify by height to block the association between S and W that is transmitted through H</li>
</ul></li>
<li>Difference in intercept, the indirect is slope differences</li>
<li>Here, the model allows for separate slopes by sex, so we can tease out the impact of height
<ul>
<li>Center the height to make the interpretation of the intercept be the average</li>
<li>Makes priors more intuitive, and computation easier</li>
</ul></li>
</ul>
<p><span class="math display">\[W_i \sim Normal(\mu_i, \sigma) \hskip.1in \mu_i=\alpha_{S[i]} + \beta_{S[i]}(H_i-\bar{H})\]</span>
<span class="math display">\[\alpha=[\alpha_1,\alpha_2] \hskip.1in \beta=[\beta_1,\beta_2]\]</span></p>
<ul>
<li>In this case, nearly all the total effect of sex on weight is explained through height (the direct effect (posterior of the difference between weights at each height) is nearly 0 at all heights)</li>
</ul>
<p><strong>Curve Fitting</strong></p>
<ul>
<li>We use linear models to do this; i.e., it’s not mechanistic, but we use it wisely</li>
<li>Strategies
<ul>
<li>Polynomials: Don’t do it; no local smoothing, only global; learn to much from data in regions that lie far away
<ul>
<li>It’s not worth having a model that looks OK for most of the data that we know is completely erroneous (e.g., parabola at some point shows babies get heavier as their height decreases, which we know is wrong); even though this is a small portation of observations, it’s still knowingly wrong, so why use it?</li>
</ul></li>
<li>Splines &amp; GAMs: Not as bad as polynomials; add total many locally trained terms</li>
</ul></li>
</ul>
<p><strong>Splines</strong></p>
<ul>
<li>Flexible curve that will find trends</li>
<li>B-splines are linear models containing additive terms with synthetic variables
<ul>
<li>Think of it as a collection of individual curves (basis functions), but the weight of each basis function is non-zero at only particular areas of x, and spline is the sum of the curves at a particular point</li>
</ul></li>
</ul>
<p><span class="math display">\[\mu_i = \alpha + w_1B_{i,1} + w_2B_{i,2} + ...\]</span>
* Ideal model for age/height would be to account for what we know about human biology: infant, toddler, adolescent, adult. In the first 3, we expect only upward growth, so we should constrain.</p>
<p><strong>Full Luxury Bayes</strong></p>
<ul>
<li>Equivalent approach is to use one model for entire causal sample</li>
<li>Then run simulations from overall system to get answers to specific queries</li>
</ul>
</div>
</div>
