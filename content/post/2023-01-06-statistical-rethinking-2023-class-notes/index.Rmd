---
title: Statistical Rethinking 2023 Class Notes
author: Alex Zajichek
date: '2023-01-06'
slug: statistical-rethinking-2023-class-notes
categories: []
tags: []
subtitle: 'Summaries, interpretations, and examples'
summary: ''
authors: []
lastmod: '2023-01-06T09:37:10-06:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

This document is intended to be a repository for my (raw, unedited) notes, interpretations, examples, and summaries from the [Statistical Rethinking 2023](https://github.com/rmcelreath/stat_rethinking_2023) course (which Richard McElreath has graciously made available for free (!) covering [his book](https://xcelab.net/rm/statistical-rethinking/)). I'm not actually enrolled in the course, but just following the lectures and material as they are released on a weekly basis. I have a strong interest in learning and incorporating Bayesian analysis and causal principles into my work, and this seemed like a great opportunity to build a foundation for that.

```{r}
# Load some packages
library(tidyverse)
```


# Table of Contents

1. [Science Before Statistics](#lecture1)
2. [Garden of Forking Data](#lecture2)
3. [Basic Regression](#lecture3)
4. [Not-so-basic Regression](#lecture4)

# 1. Science Before Statistics {#lecture1}

Week #    Lecture #   Chapter(s)    Week End    Notes Taken
------    ---------   ----------    --------    -----------
1         1           1             1/6/2023    1/3/2023

## Summary

This course focus is on scientific modeling via causal inference, which is focused on identifying causes in *observational* data. Causal Inference requires us to consider the mechanism of a phenomenon, and think about not only which variables cause other variables, but in what order--subject matter expertise is of utmost importance, and we don't really depend on the data at hand until the very end of our inference process. Causal modeling must become the foundation to do analysis by--we can't just do simple statistics in one project and then think about causal modeling in another--samples are from populations and there are causes associated with why we observed the sample we did, even if we're answering very basic questions. Also, *Bayesian* modeling as a means to performing causal inference is not due to philosophical reasons (e.g., frequentist vs. Bayesian), it's more so because a Bayesian framework provides the most natural tools to employ the specified causal model (i.e., if the frequentist model made sense for answering the causal question, we'd use it). The generative aspect of Bayesian modeling is one aspect in particular that makes it very inviting to represent the causal model in a statistical framework, and apply distributions. Finally, coding is not just a means to employ the math, but rather needs to be treated as part of the product, therefore employing software engineering principles, having documentation, making things reproducible. These things need to be employed if you really want to advance knowledge with confidence.

## Notes

**Overview**

-   Most interested in Causal Inference, focusing on the *science* before the *statistics*
-   We must be able to talk about causes to obtain scientific knowledge, why else would we do it?
-   Causes can't be extracted from data; must come from knowledge, assumptions

**What is Causal Inference?**

-   It is more than associations; associations are bi-directional, and correlation is only a basic measure of association;
-   It is all about intervention, directionality, and the *prediction* of the consequence of changing one variable on another (asking *what-if?*)

**Causal Imputation**

-   This is about being able to construct *counterfactual* outcomes
-   Asking the question, *what if I had done something else?*
-   We only observe a single outcome, but we want to know what would have happened had a certain intervention not taken place

**Directed Acyclic Graph (DAG)**

-   Nothing more than an abstraction about which variables cause which other variables
-   Shows the direction at which variables cause each other, but doesn't specify *how* (i.e., effect shapes, etc.)
-   We can use this to know which things to control for, answer hypothetical interventions, under the assumption that the model is true
-   It provides a tool to answer very specific questions (queries); not necessarily all questions lead to the same statistical model, but the appropriate statistical model can be derived from the causal model depending on the question
-   *Intuition Pumps*: Gets the researcher to think about mechanism; great way to problem solve with SME's without looking at the data (which is how it should be)

**Golems (statistical models)**

-   Metaphor for what a statistical model is; it's a very useful machine that will do what it's asked very well, but has no wisdom or forethought
-   Does not know the intent of the task
-   Statistical models are just objective tools, but we need causal models to know how and when certain models are actually appropriate

**Statistical Models**

-   Having a flowchart of tests is not useful, except maybe in the *experimental* setting (remember we're talking observational data)
-   Statistical models/tests don't make a clear relationship between the research and the data; it's just math

**Hypotheses & Models**

-   We need *generative* causal models that are guided by the DAG's
-   We need *estimands* that are statistical models justified by the generative models (how do we quantify what we're after?)
-   Introduce real data at the end--this is the easy part

**Justifying Controls**

-   Cannot just control for everything in your dataset like is done so much in current research (e.g., colliders have undesired effect)
-   Need the causal model (DAG) to be able to deduce what should be controlled for based on the specific question that is asked
-   *Adjustment Set:* The variables determined appropriate to control for for a particular query

**Why Bayesian?**

-   Bayesian happens to be the easiest approach for generative models; it's not because we're stuck in a philosophical debate
-   Easiest way to take the scientific structure of the assumed model and generate it, since it naturally has *direction* (i.e., priors)
-   In most cases, Bayes can be appropriate (sometimes not--cut cake with chainsaw)
    -   Measurement error, missing data, latent variables, regularization
-   It is *practical*, not *philosophical*

**Owls**

-   Classic joke: Step 1 = Draw two circles, Step 2 = draw remaining owl
    -   Programming and technical things tend to be taught this way, but we want to avoid this and document all the intermediate steps
-   We need to have an explicit workflow with clear steps
-   We need to treat coding/scripting seriously, not just a means to something (apply software engineering principles, documentation, quality control)
-   Understand what you are doing, document your work and reduce error, have a respectable scientific workflow, be professional and organized to maintain *reproducible* scientific knowledge, otherwise it's all bullshit
-   Workflow
    1.  Theoretical Estimand (what are we trying to do?)
    2.  Scientific (Causal) Model (DAG + Generative)
    3.  Use 1 & 2 to build appropriate statistical model
    4.  Simulate from 2 to validate that 3 yields 1
    5.  Analyze the actual data

# 2. Garden of Forking Data {#lecture2}

Week #    Lecture #   Chapter(s)    Week End    Notes Taken
------    ---------   ----------    --------    -----------
1         2           2, 3          1/6/2023    1/6/2023

## Summary

This scientific modeling framework provides an _objective_ process to incorporate _subjective_ (expert, scientfic) knowledge into the modeling process, enabling us to incorporate all of the uncertainty associated with those processes, predicated on the assumption of the causal model. Further, one of the key takeaways was that _samples do not need to be representative of the population for us to provide good estimates_. This is profound because generally we are taught the opposite, but because of the process, we can explicitly account for how we know/assume the data was generated, and use that information to create a good estimate of the quantity we are interested in. This is much more _practical_ than the assumptions that are made in a typical frequentist analysis--which tend to be blindly made which ironically makes them more wrong than the "subjective" information in the generative approach. We can then use sampling of our posterior distribution(s) to answer questions about what might happen if we do another experiment, etc. (e.g., what if we take 10 more samples?). Instead of relying on asymptotics for the sampling distribution of a statistic (frequentist), we can just take samples from the posterior for any complex quantity of interest and get the uncertainty surrounding that. This is especially important once we are dealing with analytically intractable posteriors that don't have closed form solutions. Instead of needing expert-level calculus knowledge for such problem, we just have to follow the same workflow as in this basic problem. After years of frequentist modeling, that is always full of limitations and disatisfaction in the results, this approach will lead to much more rewarding scientific discovery and confidence in the conclusions of research.

### My check for understanding

Let's go through and reproduce some of the content/concepts from slides but using our own explanation, implementation and interpretation along the way. 

#### 1. What is the objective?

The main question asked in the demonstration was _what proportion of the globe is water?_. Thus, the quantity we are interested in is a single quantity: the _true_ proportion of of the globe that is water. 

#### 2. What is the sampling strategy?

We want to collect data to try to answer the question of interest. This will be done by spinning the globe and dropping a pin at a random location to indicate if it is either land or water. Some initial assumptions are

* All points on the globe are equally-likely to be selected
* Any given point on the globe is either land or water (only two possibilities)
* There is no measurement error associated with indicating if the selected point was land or water

#### 3. What is the generative model?

We want to consider the different variables at play here as it relates to any observed sample we get as a result of the sampling strategy. First and foremost, the primary _unknown_ parameter is:

$$p=\text{proportion of water on the globe}$$

The other two at play (under this simplistic model) are:

$$N=\text{Number of globe spins} \hskip.5in W=\text{Number of spins resulting in water}$$
_Note that the number of spins resulting in land is just $N-W$_

With the variables defined, the next step is determine how these variables relate to each other. We'll use the following DAG:

```{r, fig.cap="Unobserved quantities are highlighted in yellow"}
DiagrammeR::grViz("
  digraph graph2 {
    graph [layout = dot, rankdir = LR]
    node [style = filled]
    a [label = 'p', fillcolor = yellow]
    b [label = 'N']
    c [label = 'W']
    a -> c
    b -> c
  }
", height = 200, width = 350)
```

This assumes that the number of water spins observed in our sample is determined by:

1. The true proportion of water on the globe
2. The total number of spins of the globe made (samples)

#### 4. What is the statistical model/estimation procedure?

Let's suppose we execute the sampling procedure which yields the following response vector:

```{r}
observed_sample <- c(1, 0, 1, 1, 1, 0, 1, 0, 1) # 1 = water; 0 = land
W <- sum(observed_sample) # Number of water samples
N <- length(observed_sample) # Number of spins
W; N
```

We just need to _count_ all of the ways that this sample could have arose across all of the different possibilities of $p$, and then estimate $p$ as that of where the sample was most likely to have occurred.

##### Basic (incorrect) solution with finite possibilities

We know that there are infinitely many possibilities for $p$. Let's first go through this assuming the globe is that of a 4-sided die, such that each side is land or water, implying the only possibilities are $p \in (0,.25,.50,.75,1)$. For each possible value of $p$, what is number of ways we could have observed our sequence of data? (thinking of the generative process, starting with $N$ and $p$).

First of all, we can set our _possible_ set of parameter values, and the number of "sides" of the globe this implies (i.e., we're saying that there are only 4 sides and each one is either Water or Land, so we have a limited number of $p$ values that could occur).
```{r}
# Set possible values for p
p <- c(0, .25, .5, .75, 1)

# Number of sides of globe
sides <- length(p) - 1

```

For each of the `r length(p)` possible values of $p$, how many combinations are there that produce our observed sequence of data?

```{r}
# Number of ways to observe sample for each p (this is the count of the possible sequences of indicators)
ways <- (sides*p)^W * (sides*(1-p))^(N-W)
ways
```

Now, of those possibilities, which was the most likely to occur?

```{r}
# Posterior probability
posterior_prob <- ways / sum(ways)
cbind(p, ways, posterior_prob)
```

It looks like $p=0.75$ was the most likely value of those that are possible.

What is key to note about the posterior probabilities is that they are relative to the total across all values of $p$. We simply found all of the raw counts associated with each $p$ and then normalized them by the total to get the posterior probability. But this process was _exactly_ the same thing as finding the _likelihood_ of the data:

$$Likelihood = \prod_{i=1}^NP(X=x|p)$$

where $X$ is the binary indicator from a single globe spin. 

If we just look at all the _possible_ sequences of indicators that could have occurred:

```{r}
# Total possible sequences of indicators (each one could be a 1 or a 0)
total_possible_sequences <- sides^N
total_possible_sequences
```

And then divide our original combination counts by that, we'll get _exactly_ the likelihood of the data:

```{r}
# Divide the total number of combinations we could have saw our sample, by the total number of possibilities
likelihood <- ways / total_possible_sequences
likelihood
```

However, as stated above, this will _not_ change the resulting posterior distribution because the number we divided by was just a normalizing constant:

```{r}
likelihood / sum(likelihood)
```

So, we could also think of this problem in a different light (although it's the SAME) and get the same result:

1. We could think of each observed value as an (unfair) coin flip (according to the value of $p$) and calculate the likelihood of the sequence of flips (which is actually what we already did, but this is more of the "traditional" way to think about it):

```{r}
# Likelihood of sequence of observed sample
likelihood2 <- p^W * (1-p)^(N-W)
likelihood

# Compute posterior
likelihood2 / sum(likelihood2) # Same as before
```

2. We could also think of this as finding the likelihood of observing the _total_ number of water spins since each flip is _independent_. This is also the same as before, except we're accounting for all of the combinations to observe the total number of water flips, not just the particular sequence:

```{r}
# Make the normalizing constant
normalizing_constant <- factorial(N) / (factorial(W)*factorial(N-W))

# Multiply the likelihood by the normalizing constant by the likelihood to get the true probability of the observed sample for each value of p
probability <- normalizing_constant * likelihood
probability

# Compute the posterior
probability / sum(probability)
```

Note that the normalizing constant had no effect on the posterior, but it did calculate the correct probabilities of the observed sample. In fact, this was just a Binomial distribution:

```{r}
# What is the probability of observing W water values in a sample of N globe spins for each p?
dbinom(x = W, size = N, prob = p)
```

That is, the probability distribution for the number of water samples is:

$$W|p \sim Binomial(N, p)$$
$$
\begin{equation} 
\begin{split}
P(W|p) 
&= \binom{N}{W}p^W(1-p)^{N-W} \\
&= \frac{N!}{W!(N-W)!}p^W(1-p)^{(N-W)} \\
\end{split}
\end{equation}
$$

So what is going on here? We are after the distribution of probability weights associated with each possible value of $p$ (which is what the posterior distribution is). In mathematical notation, we're just applying Bayes' formula:

$$
\begin{equation} 
\begin{split}
P(p|sample) 
& = \frac{P(p)P(sample|p)}{P(sample)} \\
& = \frac{P(p)P(W|p)}{P(W)} \\
& = \frac{P(p)P(W|p)}{P(W \cap p = 0) + ... + P(W \cap p = 1)} \\
& = \frac{P(p)P(W|p)}{P(p=0)P(W|p=0) + ... + P(p=1)P(W|p=1)} \\
\end{split}
\end{equation}
$$
Each value of $p$ is equally-likely to occur (_uniform prior_), so we can factor out that term:

$$
\begin{equation} 
\begin{split}
\text{(from previous)}
& = \frac{P(W|p)}{P(W|p=0) + ... + P(W|p=1)} \\
(binomials) &= \frac{\binom{N}{W}p^W(1-p)^{(N-W)}}{\binom{N}{W}0^W(1-0)^{(N-W)} + ... + \binom{N}{W}1^W(1-1)^{(N-W)}} \\
& = \frac{p^W(1-p)^{(N-W)}}{0^W(1-0)^{(N-W)} + ... + 1^W(1-1)^{(N-W)}} \\
& = \frac{p^W(1-p)^{(N-W)}}{\text{Normalizing constant}} \\
\end{split}
\end{equation}
$$
As you can see, the combination term also factors out, and the basic structure we're left with is the _likelihood_ piece that was found in _all three (3)_ variations above: $p^W(1-p)^{(N-W)}$. So when computing the posterior probability, they are relative to only terms dependent on the parameter of interest, so doesn't matter if we use the counts, base likelihood, or the probability distribution--they are all the SAME. The counting process and the "forking path" approach is simply a means to breakdown the process of what's happening behind the scenes in the math, so instead of just saying "do this integral" or "compute this product of the likelihood", you're picking apart each step of that process to gain intuition about what is happening. I'd imagine this is exactly the point of the Owl reference in the prior lecture.

##### Full solution: _p_ is a continuous value

As mentioned before, the actual proportion of water on the globe can be any number between zero and one ($p \in [0,1]$), meaning that there are "infinite" sides to the globe. The derivation at the end of the previous section illustrates that the posterior distribution for $p$ not restricted to any particular set of values. If we pick up where we left off:

$$
\begin{equation} 
\begin{split}
P(p|data)
& = \frac{p^W(1-p)^{(N-W)}}{\text{Normalizing constant}} \\
\end{split}
\end{equation}
$$
All we would need to do for the continuous version of $p$ to make the posterior a formal probability distribution is to find the normalizing constant such that the integral over all possible values of $p$ equals 1. Formally, with respect to $p$,

$$\int_0^1 \frac{p^W(1-p)^{N-W}}{Constant} = 1$$
This will ensure that the probabilities across all possible values of $p$ sums to one. However, it doesn't actually matter that we find that constant necessarily, because the posterior probability is just _relative_ to the range of values of $p$. So all that really matters is:

$$P(p|data) \propto p^W(1-p)^{N-W}$$
We can then plug in our data and graph the resulting distribution to make inferences about $p$. 

$$P(p|data) \propto p^6(1-p)^3$$
```{r}
tibble(
  p = seq(0, 1, .01), # Approximate the range of p values
  posterior = p^W*(1-p)^(N-W) # Compute the posterior
) %>%
  
  # Make a plot
  ggplot() +
  geom_line(
    aes(
      x = p,
      y = posterior
    )
  ) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  )
```

After using our sample of `r N`, the probability weight for $p$ tends to focus near 0.70. Note that the scale of the y-axis was removed to emphasize that it doesn't really matter what it is. We would just need to be able to calculate the area under the curve to be able to assign real probabilities to questions like _"what is the probability that the proportion of water is less than 0.5?"_.

_Note: In some cases, if we used used a different priors on $p$ (e.g., Beta), the posterior will turn out to be an identifiable distribution which we know the normalizing constant._

##### Updating the posterior

So when we talk "Bayesian updates" or updating the posterior distribution, what does this mean? Since the point of it is to be able to update a model with new information, my gut used to tell me that we were somehow adding our current knowledge about the parameter into the new _prior_ distribution, and then updating the new posterior with an updated prior and only using new data in the likelihood. While in a way this might be the right way to think about it (i.e., if I have a posterior right now, isn't that the most current knowledge about the parameter, so if I want to collect more data, wouldn't I want to use knowledge up to this point as the prior instead of reverting back to the original prior and just adding more data to the collective sample?), in these examples we were doing something different: we're just seeing how the posterior changes as more data is added to the sample (i.e., observed sequence of data points).

Let's start with just focusing on the basic example (i.e., 4 sided-globe) for now. We just need to loop through the observed sample, and calculate the posterior probabilities for each value of $p$ as a new observation comes in:

```{r}

# Set the prior probability (uniform over the possibly choices)
prior <- rep(1 / length(p), length(p))

# Set the current posterior as the prior (before any data collected)
last_posterior <- prior

# Make result set
results <- tibble()

# For each value in the observed sample 
for(i in 1:N) {
  
  # 1. Get the sub-sample
  sub_sample <- observed_sample[1:i]
  
  # 2. Compute metrics (the number of water samples, and the total number of spins)
  W_temp <- sum(sub_sample)
  N_temp <- length(sub_sample)
  
  # 3. Compute the likelihood for each p
  temp_likelihood <- p^W_temp * (1 - p)^(N_temp - W_temp)
  
  # 4. Posterior
  temp_posterior <- temp_likelihood / sum(temp_likelihood)
  
  # 5. Add to results
  results <-
    results %>%
    bind_rows(
      tibble(
        sample = i,
        sequence = paste(sub_sample, collapse = ","),
        p,
        likelihood = temp_likelihood,
        current = temp_posterior,
        last = last_posterior
      )
    )
  
  # Set the new last posterior
  last_posterior <- temp_posterior

}

results %>%
  
  # Send down the rows
  pivot_longer(
    cols = c(last, current)
  ) %>%
  
  # Make a plot
  ggplot() +
  geom_col(
    aes(
      x = factor(p),
      y = value,
      fill = name
    ),
    color = "black",
    alpha = .75,
    width = .25,
    position = "identity"
  ) +
  facet_wrap(
    ~paste0("Spin: ", factor(sample), " \nSample: ", sequence)
  ) +
  theme(
    legend.position = "top",
    panel.background = element_blank(),
    panel.grid.major.y = element_line(colour = "gray")
  ) +
  xlab("p") +
  ylab("Posterior Probability") +
  labs(
    fill = "Posterior"
  ) +
  scale_fill_manual(
    values = c("blue", "darkgray")
  ) 

```

The blue bars show the posterior probability for each possible value of $p$ _after_ the newest observation was made, and the gray bars show it _before_ the newest observation was made. This illustrates the incremental impact of adding more data to the sample on the resulting posterior distribution.

We can apply this same process to the _continuous_ (correct) possible set of values for $p$ (in fact, we'll create the curves by performing the exact same procedure to a larger, discrete set of values but make the display appear continuous):

```{r}
# Approximate the set of inifinite p-values by a large set of discrete ones
p_continuous <- seq(0, 1, .01)

# Set the prior probability (uniform over the possibly choices)
prior <- rep(1 / length(p_continuous), length(p_continuous))

# Set the current posterior as the prior (before any data collected)
last_posterior <- prior

# Make result set
results <- tibble()

# For each value in the observed sample 
for(i in 1:N) {
  
  # 1. Get the sub-sample
  sub_sample <- observed_sample[1:i]
  
  # 2. Compute metrics (the number of water samples, and the total number of spins)
  W_temp <- sum(sub_sample)
  N_temp <- length(sub_sample)
  
  # 3. Compute the likelihood for each p
  temp_likelihood <- p_continuous^W_temp * (1 - p_continuous)^(N_temp - W_temp)
  
  # 4. Posterior
  temp_posterior <- temp_likelihood / sum(temp_likelihood)
  
  # 5. Add to results
  results <-
    results %>%
    bind_rows(
      tibble(
        sample = i,
        sequence = paste(sub_sample, collapse = ","),
        p_continuous,
        likelihood = temp_likelihood,
        current = temp_posterior,
        last = last_posterior
      )
    )
  
  # Set the new last posterior
  last_posterior <- temp_posterior
  
}

results %>%
  
  # Send down the rows
  pivot_longer(
    cols = c(last, current)
  ) %>%
  
  # Make a plot
  ggplot() +
  geom_area(
    aes(
      x = p_continuous,
      y = value,
      fill = name
    ),
    color = "black",
    alpha = .65,
    position = "identity"
  ) +
  facet_wrap(
    ~paste0("Spin: ", factor(sample), " \nSample: ", sequence)
  ) +
  theme(
    legend.position = "top",
    panel.background = element_blank(),
    panel.grid.major.y = element_line(colour = "gray"),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  ) +
  xlab("p") +
  ylab("Posterior Probability") +
  labs(
    fill = "Posterior"
  ) +
  scale_fill_manual(
    values = c("blue", "darkgray")
  ) 
```

Again, these curves are actually approximated here. In practice, we would need to calculate the area underneath the curve to get exact answers about probabilities. _Note: We could parameterize the Beta distribution to get the normalizing constant for calculating the actual posterior probabilities that fits this distribution._

## Homework

1. Suppose the globe tossing data had turned out to be 4 water and 11 land. Construct the posterior distribution.

We'll cheat a little bit and use the approach of using a large, discrete list of possible values for $p$, and plot it as if it is continuous (we could also just use the Beta distribution). With that, all we need to do is change the values of $W$ and $N$ and use the same code as above.

```{r}
W_new <- 4
N_new <- 15
tibble(
  p = seq(0, 1, .01), # Approximate the range of p values
  posterior = p^W_new*(1-p)^(N_new-W_new) # Compute the posterior
) %>%
  
  # Make a plot
  ggplot() +
  geom_line(
    aes(
      x = p,
      y = posterior
    )
  ) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  )
```

2. Using the posterior distribution from (1), compute the posterior predictive distribution for the next 5 tosses of the same globe. 

Okay here I'll finally acknowledge that the posterior can be written as a [Beta](https://en.wikipedia.org/wiki/Beta_distribution) distribution, which gives us the normalizing constant needed to make it a real probability distribution (i.e., the area sums to 1). It has the following _probability density function (PDF)_:

$$f(x|\alpha, \beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}$$
where $x \in [0,1]$, $\alpha, \beta > 0$, and $\Gamma(n) = (n-1)!$. If we make the following reparameterizations from our set of variables:

$$p=x$$
$$W = \alpha - 1$$
$$N-W = \beta - 1$$
we get:

$$
\begin{equation} 
\begin{split}
f(p|W,N)
& = \frac{\Gamma(W+1+N-W+1)}{\Gamma(W+1)\Gamma(N-W+1)}p^W(1-p)^{N-W} \\
& = \frac{\Gamma(N+2)}{\Gamma(W+1)\Gamma(N-W+1)}p^W(1-p)^{N-W} \\
& = \frac{(N+1)!}{W!(N-W)!}p^W(1-p)^{N-W} \\
\end{split}
\end{equation}
$$

_Note that since $p$ is continuous, this function represents the probability density at any particular value of $p$. To get any positive probability, we must integrate this function over a range of $p$ values. Hence the $f$ notation._

Let's quickly plug in $W=4$ and $N=15$ to confirm (at least visually) that this function produces (approximately) the same posterior we made to answer the last question (note we'll have to use the parameterization built into `R`. We'll do this by sampling 10000 observations from this distribution:

```{r}
# Reparameterize
alpha <- W_new + 1
beta <- N_new - W_new + 1

# Make a data frame
tibble(
  p = seq(0, 1, .01), # Approximate the range of p values (same as before)
  posterior = dbeta(p, shape1 = alpha, shape2 = beta) # Compute the actual posterior density
) %>%
  
  # Make a plot
  ggplot() +
  geom_line(
    aes(
      x = p,
      y = posterior
    )
  )

```

From inspection, it appears that they are essentially the same curves. _Note that this time I kept the y-axis labels because this is the density for the actual probability distribution_.

So back to the question: how do we find the posterior predictive distribution for the next 5 globe spins? Well, if the globe is spun 5 more times, then we can get water on 0, 1, 2, 3, 4, or all 5 spins. Our quest is to figure out the likelihood of each of those possible outcomes based on what we currently know about $p$ (i.e., the posterior distribution). To do this, we'll use our posterior distribution to run a simulation of the experiment using the following steps:

i. Select a random value of $p$ from the posterior
ii. Draw a random $binomial$ realization where $N=5$
iii. Repeat steps i-ii 10000 times
iv. Graph the results

```{r}
# Set some parameters
set.seed(123)
n_experiment <- 5 # Number of new spins we're going to conduct
s <- 10000 # Number of simulations

# 1. Draw random values of p from the posterior
p_rand <- rbeta(n = s, shape1 = alpha, shape2 = beta) # Same parameters as before

# 2. For each p, run a binomial experiment (represents samples of W)
w_rand <- rbinom(n = s, size = n_experiment, prob = p_rand)

# Make a data frame
tibble(
  w = w_rand
) %>%
  
  # For each w
  group_by(w) %>%
  
  # Compute the total
  summarise(
    count = n()
  ) %>%
  
  # Add proportion
  mutate(
    proportion = count / sum(count)
  ) %>%
  
  # Make a plot
  ggplot(
    aes(
      x = factor(w)
    )
  ) +
  geom_col(
    aes(
      y = proportion
    )
  ) +
  geom_text(
    aes(
      y = proportion,
      label = paste0(round(proportion*100,1), "%")
    ),
    vjust = -.1
  ) +
  scale_y_continuous(
    labels = scales::percent
  ) +
  xlab("W") +
  ylab("Probability") +
  labs(
    title = paste0("Posterior Predictive Distribution for ", n_experiment, " more spins.")
  )
```

3. Use the posterior predictive distribution from (2) to calculate the probability of 3 or more water samples in the next 5 tosses.

Using the posterior predictive distribution above, we can look at the percent of simulations that resulted in 3 or more water samples:

```{r}
mean(w_rand >= 3)
```

So there is a `r round(mean(w_rand>=3), 3)` probability of 3 or more water samples in the next 5 tosses. However, this point estimate is not totally sufficient because we haven't reported any uncertainty associated with it. Since we know that $W|p \sim Binomial(n,p)$, the $P(W>=3|p,N)$ is already determined. In this case, we can just calculate it for each random value of $p$ sampled from the posterior:

```{r}
# Compute the binomial probability
prob_binom <- 1 - pbinom(q = 2, size = n_experiment, prob = p_rand)

# Make a plot
ggplot() +
  geom_density(
    aes(
      x = prob_binom
    )
  ) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  ) +
  xlab("P(W>=3|N=5)") 
```

## Notes

**Goal: Estimate the percent of the globe that is covered in water**

-   Think of spinning the globe and stopping on a point and repeating many times
-   How do we use that collection of points to come up with an estimate? That's the goal of today's lecture
-   First thought is just indicate each time whether land or water appear as the point; however, how does the shape of the globe impact the likelihood that I will come up with land or water on a "random" toss? Has to do with sampling strategy

1.  Define a generative model

-   Think conceptually about scientifically how the sample was produced (how do variables influence one another)
-   Variables: Things we *want* to observe/estimate or things we actually do observe

$$\bf{p} = \text{proportion of water}\hskip.5inW=\text{water observations}$$ 
$$N = \text{number of tosses}\hskip.5inL=\text{land observations}$$

2.  Define a specific estimand

Were interested in the true proportion of water **p**

3.  Design a statistical way to produce estimate

-   How are these related to each other?
    -   N influences W and L (the more tosses leads to change on other variables)
    -   p also influences W and L (i.e., the true proportion dictates the number of water observations and land observations)
    -   The DAG shows relationships, but not what the relationships *are*. We can say $W,L=f(p,N)$; what is $f$?
-   Assume a model (e.g., $p$ = .25, then count likely the sample was under that model, do that for all possible models)

4.  Test (3) using (1)

```{r}
sim_globe <-
  function(p = .7, N = 9) {
    sample(
      c("W","L"), # Possible observations
      size = N, # Number of tosses
      prob = c(p, 1-p), # The probability of each possible observation
      replace = TRUE)
  }
sim_globe()
replicate(sim_globe(p =.5, N=9), n=10)

```

-   Test the intent of the code first
-   If our procedure doesn't work when *we know* the answer, it certainly won't when we *don't* know the answer

Infinite sample:

$$p^W(1-p)^L$$ Posterior probability:

$$p = \frac{(W+L+1)!}{W!L!}p^W(1-p)^L$$ 

* This is a *Beta* distribution, and the likelihood was a *Binomial*.
* The minimum sample size for Bayesian analysis is 1.
* The shape of the posterior distribution embodies the sample size
* No point estimate, we work with the entire posterior distribution
* The distribution *is* the estimate; always use the entire distribution, never a single point
* The fact that an arbitrary interval contains an arbitrary value is not meaningful

5.  Analyze sample, summarize

* Implications depend on entire posterior
* Average over the uncertainty of the posterior
* What can we do with the posterior distribution?
  + We can take samples from it, and then do calculations with the samples
* Posterior Prediction
  + Given what we've learned, what would happen if we took more samples?
  + Sampling distribution (predictive distribution) of draws represents the likelihood of each outcome in a new experiment for a particular value
  + The _posterior predictive_ distribution then represents the entire distribution of the statistic of interest, and contains all the uncertainty around that estimate (analogous to the sampling distribution of a statistic (e.g., mean) in the frequentist paradigm, except this is completely model-driven by the posterior instead of based on asymptotics in the frequentist approach)
  + Sampling turns calculus into a data summary problem; this is important when models get complex and numerically intractable to compute by hand

* This generative, Bayesian framework is the optimal approach for causal estimation _if your model is correct_.
* It honestly carries out the assumptions we put into it, using logical implications
* Quantitative framework/asset that activates our qualitative knowledge as scientists, subject matter experts, etc. Let's the subjective and objective work together. Subjectivity is expertise.

**Misclassification**

* Use circles around variable in DAG to represent unobserved vs. observed variables

* Imagine the true number of water samples (W) are unobserved (e.g., measurement error, data error, etc.)
* We observe a _contaminated_ W (called W*) that is the _misclassified_ sample
* W* is caused by the _measurement process_ M. We can get get back to the correct posterior distribution for p if we use M through W*.
* The posterior is honest about the uncertaintly induced by the misclassification process
* When there is measurement error, model it instead of ignoring it (same for missing data, compliance, inclusion)
* _Key point: Samples do not need to be representative of population to provide good estimates, since we can correct them through our causal diagram (modeling the source, sampling process, etc.)_
* This concept may also arise if, for example, the globe was not spun equally likely for every point to be selected.

# 3. Basic Regression {#lecture3}

Week #    Lecture #   Chapter(s)    Week End    Notes Taken
------    ---------   ----------    --------    -----------
2         3           4             1/13/2023   

## Summary

## Notes

# 4. Not-so-basic Regression {#lecture4}

Week #    Lecture #   Chapter(s)    Week End    Notes Taken
------    ---------   ----------    --------    -----------
2         4           4             1/13/2023   

## Summary

## Notes