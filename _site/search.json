[
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 12, 2018\n\n\nAn assessment of non-traditional regression models for count data\n\n\nAlex Zajichek\n\n\n\n\nFeb 27, 2025\n\n\nThe value of open-source: Leveraging free, code-first tools to iterate toward advanced analytics\n\n\nAlex Zajichek\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/the-value-of-open-source/index.html",
    "href": "presentations/the-value-of-open-source/index.html",
    "title": "The value of open-source: Leveraging free, code-first tools to iterate toward advanced analytics",
    "section": "",
    "text": "In progress\n\nSlides\n\n\nLink to slideshow"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Alex Zajichek",
    "section": "",
    "text": "Statistician and data scientist in Central Wisconsin interested in statistical analysis and software development. Need statistical help? Visit my consultancy page."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Alex Zajichek",
    "section": "Experience",
    "text": "Experience\nCleveland Clinic | Research Data Scientist | Feb 2023 - Present\nAspirus Health | Data Scientist | Nov 2020 - Jun 2023\nSentry Insurance | Advanced Analytics Modeler | Dec 2019 - Nov 2020\nCleveland Clinic | Biostatistician | Aug 2017 - Dec 2019"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Alex Zajichek",
    "section": "Education",
    "text": "Education\nUniversity of Iowa | MS in Statistics | Iowa City, IA | Aug 2015 - May 2017\nUniversity of Wisconsin - La Crosse | BS in Statistics | La Crosse, WI | Sep 2011 - Jun 2015"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zajichek Stats",
    "section": "",
    "text": "A blog on statistical practice, programming, history, philosophy, and data science.\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow to reconcile the regression equation from spline terms\n\n\n\n\n\n\nRegression\n\n\n\nUsing the {rms::rcs} function\n\n\n\n\n\nNov 25, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nCan you have a model without data?\n\n\n\n\n\n\nBayesian Statistics\n\n\n\nYes, by being a Bayesian.\n\n\n\n\n\nOct 29, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nYou should have a data science blog\n\n\n\n\n\n\nDeployment\n\n\nLearning\n\n\nSoftware Development\n\n\n\nEasy and free are only a couple benefits.\n\n\n\n\n\nSep 25, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nLow cost ways to build and deploy analytical web apps\n\n\n\n\n\n\nWeb applications\n\n\nDeployment\n\n\n\nAn overview of some server options\n\n\n\n\n\nAug 20, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nA prediction system for managing the hospital readmission risk pool\n\n\n\n\n\n\nHealthcare\n\n\nModeling\n\n\nPrediction\n\n\n\nProposed framework\n\n\n\n\n\nJul 26, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nHow do you assess the proportional-odds assumption? Directly.\n\n\n\n\n\n\nRegression\n\n\n\nA simple visual check.\n\n\n\n\n\nJul 19, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nA simple example why statistical significance is insufficient for action\n\n\n\n\n\n\nStatistical Significance\n\n\nDecision Making\n\n\n\nIt ignores the basic question: “How much?”\n\n\n\n\n\nJun 19, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\n5 ways to help ensure success of a statistical project\n\n\n\n\n\n\nProject Management\n\n\n\nHow can we increase the likelihood that things will go as expected?\n\n\n\n\n\nMay 16, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nQuantum entanglement from a statistician’s perspective\n\n\n\n\n\n\nQuantum\n\n\nProbability\n\n\n\nI’m a total beginner, but this stuff is interesting\n\n\n\n\n\nFeb 16, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Creation of Classical Statistics\n\n\n\n\n\n\nHistory\n\n\nPhilosophy\n\n\n\nFisher was in fact, a genius\n\n\n\n\n\nFeb 10, 2024\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical significance is…insignificant\n\n\n\n\n\n\nHistory\n\n\nPhilosophy\n\n\nResearch\n\n\n\n“A cheap way to get marketable results” -William Kruskal\n\n\n\n\n\nDec 22, 2023\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nThe overlap weight in survival analysis\n\n\n\n\n\n\nCausal Inference\n\n\nSurvival Analysis\n\n\nPropensity Scores\n\n\nWeighting\n\n\n\nA simulation review\n\n\n\n\n\nOct 2, 2023\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nRemoving higher-order aggregation text in {reactable}\n\n\n\n\n\n\nTables\n\n\nJavascript\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nNesting with {tidyr}\n\n\n\n\n\n\nData Wrangling\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nSome Takeaways from Effective Data Storytelling\n\n\n\n\n\n\nCommunication\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nThe Evasive Spline\n\n\n\n\n\n\nEstimation\n\n\nModeling\n\n\n\nBasis Functions & Splines\n\n\n\n\n\nJan 13, 2023\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\n(Re)deriving a Bernoulli MLE\n\n\n\n\n\n\nEstimation\n\n\n\nTrying to remember calculus\n\n\n\n\n\nJan 12, 2023\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Rethinking 2023 Class Notes\n\n\n\n\n\n\nBayesian Statistics\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nA couple useful JavaScript aggregation and formatting functions for {reactable}\n\n\n\n\n\n\nHealthcare\n\n\nTables\n\n\nJavascript\n\n\n\nEmbedding tables into your analytical HTML document\n\n\n\n\n\nJul 31, 2022\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a filterable map with {leaflet} and {crosstalk} for hospital readmissions\n\n\n\n\n\n\nHealthcare\n\n\nMaps\n\n\nJavascript\n\n\n\nFor runtime-free analytical content delivery\n\n\n\n\n\nJun 18, 2022\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nExploring COM Poisson regression\n\n\n\n\n\n\nRegression\n\n\n\nA method for underdispersed count data\n\n\n\n\n\nDec 28, 2021\n\n\nAlex Zajichek\n\n\n\n\n\n\n\n\n\n\n\n\nA look at collider bias\n\n\n\n\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\nJul 15, 2021\n\n\nAlex Zajichek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html",
    "href": "post/statistical-rethinking-2023-class-notes/index.html",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "",
    "text": "This document is intended to be a repository for my (raw, unedited) notes, interpretations, examples, and summaries from the Statistical Rethinking 2023 course (which Richard McElreath has graciously made available for free (!) covering his book). I’m not actually enrolled in the course, but just casually following the lectures and material. I have a strong interest in learning and incorporating Bayesian analysis and causal principles into my work, and this seemed like a great opportunity to build a foundation for that."
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#summary",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#summary",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Summary",
    "text": "Summary\nThis course focus is on scientific modeling via causal inference, which is focused on identifying causes in observational data. Causal Inference requires us to consider the mechanism of a phenomenon, and think about not only which variables cause other variables, but in what order–subject matter expertise is of utmost importance, and we don’t really depend on the data at hand until the very end of our inference process. Causal modeling must become the foundation to do analysis by–we can’t just do simple statistics in one project and then think about causal modeling in another–samples are from populations and there are causes associated with why we observed the sample we did, even if we’re answering very basic questions. Also, Bayesian modeling as a means to performing causal inference is not due to philosophical reasons (e.g., frequentist vs. Bayesian), it’s more so because a Bayesian framework provides the most natural tools to employ the specified causal model (i.e., if the frequentist model made sense for answering the causal question, we’d use it). The generative aspect of Bayesian modeling is one aspect in particular that makes it very inviting to represent the causal model in a statistical framework, and apply distributions. Finally, coding is not just a means to employ the math, but rather needs to be treated as part of the product, therefore employing software engineering principles, having documentation, making things reproducible. These things need to be employed if you really want to advance knowledge with confidence."
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#notes",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#notes",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Notes",
    "text": "Notes\nOverview\n\nMost interested in Causal Inference, focusing on the science before the statistics\nWe must be able to talk about causes to obtain scientific knowledge, why else would we do it?\nCauses can’t be extracted from data; must come from knowledge, assumptions\n\nWhat is Causal Inference?\n\nIt is more than associations; associations are bi-directional, and correlation is only a basic measure of association;\nIt is all about intervention, directionality, and the prediction of the consequence of changing one variable on another (asking what-if?)\n\nCausal Imputation\n\nThis is about being able to construct counterfactual outcomes\nAsking the question, what if I had done something else?\nWe only observe a single outcome, but we want to know what would have happened had a certain intervention not taken place\n\nDirected Acyclic Graph (DAG)\n\nNothing more than an abstraction about which variables cause which other variables\nShows the direction at which variables cause each other, but doesn’t specify how (i.e., effect shapes, etc.)\nWe can use this to know which things to control for, answer hypothetical interventions, under the assumption that the model is true\nIt provides a tool to answer very specific questions (queries); not necessarily all questions lead to the same statistical model, but the appropriate statistical model can be derived from the causal model depending on the question\nIntuition Pumps: Gets the researcher to think about mechanism; great way to problem solve with SME’s without looking at the data (which is how it should be)\n\nGolems (statistical models)\n\nMetaphor for what a statistical model is; it’s a very useful machine that will do what it’s asked very well, but has no wisdom or forethought\nDoes not know the intent of the task\nStatistical models are just objective tools, but we need causal models to know how and when certain models are actually appropriate\n\nStatistical Models\n\nHaving a flowchart of tests is not useful, except maybe in the experimental setting (remember we’re talking observational data)\nStatistical models/tests don’t make a clear relationship between the research and the data; it’s just math\n\nHypotheses & Models\n\nWe need generative causal models that are guided by the DAG’s\nWe need estimands that are statistical models justified by the generative models (how do we quantify what we’re after?)\nIntroduce real data at the end–this is the easy part\n\nJustifying Controls\n\nCannot just control for everything in your dataset like is done so much in current research (e.g., colliders have undesired effect)\nNeed the causal model (DAG) to be able to deduce what should be controlled for based on the specific question that is asked\nAdjustment Set: The variables determined appropriate to control for for a particular query\n\nWhy Bayesian?\n\nBayesian happens to be the easiest approach for generative models; it’s not because we’re stuck in a philosophical debate\nEasiest way to take the scientific structure of the assumed model and generate it, since it naturally has direction (i.e., priors)\nIn most cases, Bayes can be appropriate (sometimes not–cut cake with chainsaw)\n\nMeasurement error, missing data, latent variables, regularization\n\nIt is practical, not philosophical\n\nOwls\n\nClassic joke: Step 1 = Draw two circles, Step 2 = draw remaining owl\n\nProgramming and technical things tend to be taught this way, but we want to avoid this and document all the intermediate steps\n\nWe need to have an explicit workflow with clear steps\nWe need to treat coding/scripting seriously, not just a means to something (apply software engineering principles, documentation, quality control)\nUnderstand what you are doing, document your work and reduce error, have a respectable scientific workflow, be professional and organized to maintain reproducible scientific knowledge, otherwise it’s all bullshit\nWorkflow\n\nTheoretical Estimand (what are we trying to do?)\nScientific (Causal) Model (DAG + Generative)\nUse 1 & 2 to build appropriate statistical model\nSimulate from 2 to validate that 3 yields 1\nAnalyze the actual data"
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#summary-1",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#summary-1",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Summary",
    "text": "Summary\nThis scientific modeling framework provides an objective process to incorporate subjective (expert, scientfic) knowledge into the modeling process, enabling us to incorporate all of the uncertainty associated with those processes, predicated on the assumption of the causal model. Further, one of the key takeaways was that samples do not need to be representative of the population for us to provide good estimates. This is profound because generally we are taught the opposite, but because of the process, we can explicitly account for how we know/assume the data was generated, and use that information to create a good estimate of the quantity we are interested in. This is much more practical than the assumptions that are made in a typical frequentist analysis–which tend to be blindly made which ironically makes them more wrong than the “subjective” information in the generative approach. We can then use sampling of our posterior distribution(s) to answer questions about what might happen if we do another experiment, etc. (e.g., what if we take 10 more samples?). Instead of relying on asymptotics for the sampling distribution of a statistic (frequentist), we can just take samples from the posterior for any complex quantity of interest and get the uncertainty surrounding that. This is especially important once we are dealing with analytically intractable posteriors that don’t have closed form solutions. Instead of needing expert-level calculus knowledge for such problem, we just have to follow the same workflow as in this basic problem. After years of frequentist modeling, that is always full of limitations and disatisfaction in the results, this approach will lead to much more rewarding scientific discovery and confidence in the conclusions of research.\n\nA check for understanding\nLet’s go through and reproduce some of the content/concepts from slides but using our own explanation, implementation and interpretation along the way.\n\n1. What is the objective?\nThe main question asked in the demonstration was what proportion of the globe is water?. Thus, the quantity we are interested in is a single quantity: the true proportion of of the globe that is water.\n\n\n2. What is the sampling strategy?\nWe want to collect data to try to answer the question of interest. This will be done by spinning the globe and dropping a pin at a random location to indicate if it is either land or water. Some initial assumptions are\n\nAll points on the globe are equally-likely to be selected\nAny given point on the globe is either land or water (only two possibilities)\nThere is no measurement error associated with indicating if the selected point was land or water\n\n\n\n3. What is the generative model?\nWe want to consider the different variables at play here as it relates to any observed sample we get as a result of the sampling strategy. First and foremost, the primary unknown parameter is:\n\\[p=\\text{proportion of water on the globe}\\]\nThe other two at play (under this simplistic model) are:\n\\[N=\\text{Number of globe spins} \\hskip.5in W=\\text{Number of spins resulting in water}\\] Note that the number of spins resulting in land is just \\(N-W\\)\nWith the variables defined, the next step is determine how these variables relate to each other. We’ll use the following DAG:\n\n\n\n\n\nflowchart LR\n  A(p) --&gt; C(W)\n  B(N) --&gt; C\n\n\n\n\n\n\nThis assumes that the number of water spins observed in our sample is determined by:\n\nThe true proportion of water on the globe\nThe total number of spins of the globe made (samples)\n\n\n\n4. What is the statistical model/estimation procedure?\nLet’s suppose we execute the sampling procedure which yields the following response vector:\n\n\nCode\nobserved_sample &lt;- c(1, 0, 1, 1, 1, 0, 1, 0, 1) # 1 = water; 0 = land\nW &lt;- sum(observed_sample) # Number of water samples\nN &lt;- length(observed_sample) # Number of spins\nW; N\n\n\n[1] 6\n\n\n[1] 9\n\n\nWe just need to count all of the ways that this sample could have arose across all of the different possibilities of \\(p\\), and then estimate \\(p\\) as that of where the sample was most likely to have occurred.\n\nBasic (incorrect) solution with finite possibilities\nWe know that there are infinitely many possibilities for \\(p\\). Let’s first go through this assuming the globe is that of a 4-sided die, such that each side is land or water, implying the only possibilities are \\(p \\in (0,.25,.50,.75,1)\\). For each possible value of \\(p\\), what is number of ways we could have observed our sequence of data? (thinking of the generative process, starting with \\(N\\) and \\(p\\)).\nFirst of all, we can set our possible set of parameter values, and the number of “sides” of the globe this implies (i.e., we’re saying that there are only 4 sides and each one is either Water or Land, so we have a limited number of \\(p\\) values that could occur).\n\n\nCode\n# Set possible values for p\np &lt;- c(0, .25, .5, .75, 1)\n\n# Number of sides of globe\nsides &lt;- length(p) - 1\n\n\nFor each of the 5 possible values of \\(p\\), how many combinations are there that produce our observed sequence of data?\n\n\nCode\n# Number of ways to observe sample for each p (this is the count of the possible sequences of indicators)\nways &lt;- (sides*p)^W * (sides*(1-p))^(N-W)\nways\n\n\n[1]   0  27 512 729   0\n\n\nNow, of those possibilities, which was the most likely to occur?\n\n\nCode\n# Posterior probability\nposterior_prob &lt;- ways / sum(ways)\ncbind(p, ways, posterior_prob)\n\n\n        p ways posterior_prob\n[1,] 0.00    0     0.00000000\n[2,] 0.25   27     0.02129338\n[3,] 0.50  512     0.40378549\n[4,] 0.75  729     0.57492114\n[5,] 1.00    0     0.00000000\n\n\nIt looks like \\(p=0.75\\) was the most likely value of those that are possible.\nWhat is key to note about the posterior probabilities is that they are relative to the total across all values of \\(p\\). We simply found all of the raw counts associated with each \\(p\\) and then normalized them by the total to get the posterior probability. But this process was exactly the same thing as finding the likelihood of the data:\n\\[Likelihood = \\prod_{i=1}^NP(X=x|p)\\]\nwhere \\(X\\) is the binary indicator from a single globe spin.\nIf we just look at all the possible sequences of indicators that could have occurred:\n\n\nCode\n# Total possible sequences of indicators (each one could be a 1 or a 0)\ntotal_possible_sequences &lt;- sides^N\ntotal_possible_sequences\n\n\n[1] 262144\n\n\nAnd then divide our original combination counts by that, we’ll get exactly the likelihood of the data:\n\n\nCode\n# Divide the total number of combinations we could have saw our sample, by the total number of possibilities\nlikelihood &lt;- ways / total_possible_sequences\nlikelihood\n\n\n[1] 0.0000000000 0.0001029968 0.0019531250 0.0027809143 0.0000000000\n\n\nHowever, as stated above, this will not change the resulting posterior distribution because the number we divided by was just a normalizing constant:\n\n\nCode\nlikelihood / sum(likelihood)\n\n\n[1] 0.00000000 0.02129338 0.40378549 0.57492114 0.00000000\n\n\nSo, we could also think of this problem in a different light (although it’s the SAME) and get the same result:\n\nWe could think of each observed value as an (unfair) coin flip (according to the value of \\(p\\)) and calculate the likelihood of the sequence of flips (which is actually what we already did, but this is more of the “traditional” way to think about it):\n\n\n\nCode\n# Likelihood of sequence of observed sample\nlikelihood2 &lt;- p^W * (1-p)^(N-W)\nlikelihood\n\n\n[1] 0.0000000000 0.0001029968 0.0019531250 0.0027809143 0.0000000000\n\n\nCode\n# Compute posterior\nlikelihood2 / sum(likelihood2) # Same as before\n\n\n[1] 0.00000000 0.02129338 0.40378549 0.57492114 0.00000000\n\n\n\nWe could also think of this as finding the likelihood of observing the total number of water spins since each flip is independent. This is also the same as before, except we’re accounting for all of the combinations to observe the total number of water flips, not just the particular sequence:\n\n\n\nCode\n# Make the normalizing constant\nnormalizing_constant &lt;- factorial(N) / (factorial(W)*factorial(N-W))\n\n# Multiply the likelihood by the normalizing constant by the likelihood to get the true probability of the observed sample for each value of p\nprobability &lt;- normalizing_constant * likelihood\nprobability\n\n\n[1] 0.000000000 0.008651733 0.164062500 0.233596802 0.000000000\n\n\nCode\n# Compute the posterior\nprobability / sum(probability)\n\n\n[1] 0.00000000 0.02129338 0.40378549 0.57492114 0.00000000\n\n\nNote that the normalizing constant had no effect on the posterior, but it did calculate the correct probabilities of the observed sample. In fact, this was just a Binomial distribution:\n\n\nCode\n# What is the probability of observing W water values in a sample of N globe spins for each p?\ndbinom(x = W, size = N, prob = p)\n\n\n[1] 0.000000000 0.008651733 0.164062500 0.233596802 0.000000000\n\n\nThat is, the probability distribution for the number of water samples is:\n\\[W|p \\sim Binomial(N, p)\\] \\[\n\\begin{equation}\n\\begin{split}\nP(W|p)\n&= \\binom{N}{W}p^W(1-p)^{N-W} \\\\\n&= \\frac{N!}{W!(N-W)!}p^W(1-p)^{(N-W)} \\\\\n\\end{split}\n\\end{equation}\n\\]\nSo what is going on here? We are after the distribution of probability weights associated with each possible value of \\(p\\) (which is what the posterior distribution is). In mathematical notation, we’re just applying Bayes’ formula:\n\\[\n\\begin{equation}\n\\begin{split}\nP(p|sample)\n& = \\frac{P(p)P(sample|p)}{P(sample)} \\\\\n& = \\frac{P(p)P(W|p)}{P(W)} \\\\\n& = \\frac{P(p)P(W|p)}{P(W \\cap p = 0) + ... + P(W \\cap p = 1)} \\\\\n& = \\frac{P(p)P(W|p)}{P(p=0)P(W|p=0) + ... + P(p=1)P(W|p=1)} \\\\\n\\end{split}\n\\end{equation}\n\\] Each value of \\(p\\) is equally-likely to occur (uniform prior), so we can factor out that term:\n\\[\n\\begin{equation}\n\\begin{split}\n\\text{(from previous)}\n& = \\frac{P(W|p)}{P(W|p=0) + ... + P(W|p=1)} \\\\\n(binomials) &= \\frac{\\binom{N}{W}p^W(1-p)^{(N-W)}}{\\binom{N}{W}0^W(1-0)^{(N-W)} + ... + \\binom{N}{W}1^W(1-1)^{(N-W)}} \\\\\n& = \\frac{p^W(1-p)^{(N-W)}}{0^W(1-0)^{(N-W)} + ... + 1^W(1-1)^{(N-W)}} \\\\\n& = \\frac{p^W(1-p)^{(N-W)}}{\\text{Normalizing constant}} \\\\\n\\end{split}\n\\end{equation}\n\\] As you can see, the combination term also factors out, and the basic structure we’re left with is the likelihood piece that was found in all three (3) variations above: \\(p^W(1-p)^{(N-W)}\\). So when computing the posterior probability, they are relative to only terms dependent on the parameter of interest, so doesn’t matter if we use the counts, base likelihood, or the probability distribution–they are all the SAME. The counting process and the “forking data” approach is simply a means to breakdown the process of what’s happening behind the scenes in the math, so instead of just saying “do this integral” or “compute this product of the likelihood”, you’re picking apart each step of that process to gain intuition about what is happening. I’d imagine this is exactly the point of the Owl reference in the prior lecture.\n\n\nFull solution: p is a continuous value\nAs mentioned before, the actual proportion of water on the globe can be any number between zero and one (\\(p \\in [0,1]\\)), meaning that there are “infinite” sides to the globe. The derivation at the end of the previous section illustrates that the posterior distribution for \\(p\\) not restricted to any particular set of values. If we pick up where we left off:\n\\[\n\\begin{equation}\n\\begin{split}\nP(p|data)\n& = \\frac{p^W(1-p)^{(N-W)}}{\\text{Normalizing constant}} \\\\\n\\end{split}\n\\end{equation}\n\\] All we would need to do for the continuous version of \\(p\\) to make the posterior a formal probability distribution is to find the normalizing constant such that the integral over all possible values of \\(p\\) equals 1. Formally, with respect to \\(p\\),\n\\[\\int_0^1 \\frac{p^W(1-p)^{N-W}}{Constant} = 1\\] This will ensure that the probabilities across all possible values of \\(p\\) sums to one. However, it doesn’t actually matter that we find that constant necessarily, because the posterior probability is just relative to the range of values of \\(p\\). So all that really matters is:\n\\[P(p|data) \\propto p^W(1-p)^{N-W}\\] We can then plug in our data and graph the resulting distribution to make inferences about \\(p\\).\n\\[P(p|data) \\propto p^6(1-p)^3\\]\n\n\nCode\ntibble(\n  p = seq(0, 1, .01), # Approximate the range of p values\n  posterior = p^W*(1-p)^(N-W) # Compute the posterior\n) %&gt;%\n  \n  # Make a plot\n  ggplot() +\n  geom_line(\n    aes(\n      x = p,\n      y = posterior\n    )\n  ) +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nAfter using our sample of 9, the probability weight for \\(p\\) tends to focus near 0.70. Note that the scale of the y-axis was removed to emphasize that it doesn’t really matter what it is. We would just need to be able to calculate the area under the curve to be able to assign real probabilities to questions like “what is the probability that the proportion of water is less than 0.5?”.\nNote: In some cases, if we used used a different priors on \\(p\\) (e.g., Beta), the posterior will turn out to be an identifiable distribution which we know the normalizing constant.\n\n\nUpdating the posterior\nSo when we talk “Bayesian updates” or updating the posterior distribution, what does this mean? Since the point of it is to be able to update a model with new information, my gut used to tell me that we were somehow adding our current knowledge about the parameter into the new prior distribution, and then updating the new posterior with an updated prior and only using new data in the likelihood. While in a way this might be the right way to think about it (i.e., if I have a posterior right now, isn’t that the most current knowledge about the parameter, so if I want to collect more data, wouldn’t I want to use knowledge up to this point as the prior instead of reverting back to the original prior and just adding more data to the collective sample?), in these examples we were doing something different: we’re just seeing how the posterior changes as more data is added to the sample (i.e., observed sequence of data points).\nLet’s start with just focusing on the basic example (i.e., 4 sided-globe) for now. We just need to loop through the observed sample, and calculate the posterior probabilities for each value of \\(p\\) as a new observation comes in:\n\n\nCode\n# Set the prior probability (uniform over the possibly choices)\nprior &lt;- rep(1 / length(p), length(p))\n\n# Set the current posterior as the prior (before any data collected)\nlast_posterior &lt;- prior\n\n# Make result set\nresults &lt;- tibble()\n\n# For each value in the observed sample \nfor(i in 1:N) {\n  \n  # 1. Get the sub-sample\n  sub_sample &lt;- observed_sample[1:i]\n  \n  # 2. Compute metrics (the number of water samples, and the total number of spins)\n  W_temp &lt;- sum(sub_sample)\n  N_temp &lt;- length(sub_sample)\n  \n  # 3. Compute the likelihood for each p\n  temp_likelihood &lt;- p^W_temp * (1 - p)^(N_temp - W_temp)\n  \n  # 4. Posterior\n  temp_posterior &lt;- temp_likelihood / sum(temp_likelihood)\n  \n  # 5. Add to results\n  results &lt;-\n    results %&gt;%\n    bind_rows(\n      tibble(\n        sample = i,\n        sequence = paste(sub_sample, collapse = \",\"),\n        p,\n        likelihood = temp_likelihood,\n        current = temp_posterior,\n        last = last_posterior\n      )\n    )\n  \n  # Set the new last posterior\n  last_posterior &lt;- temp_posterior\n\n}\n\nresults %&gt;%\n  \n  # Send down the rows\n  pivot_longer(\n    cols = c(last, current)\n  ) %&gt;%\n  \n  # Make a plot\n  ggplot() +\n  geom_col(\n    aes(\n      x = factor(p),\n      y = value,\n      fill = name\n    ),\n    color = \"black\",\n    alpha = .75,\n    width = .25,\n    position = \"identity\"\n  ) +\n  facet_wrap(\n    ~paste0(\"Spin: \", factor(sample), \" \\nSample: \", sequence)\n  ) +\n  theme(\n    legend.position = \"top\",\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(colour = \"gray\")\n  ) +\n  xlab(\"p\") +\n  ylab(\"Posterior Probability\") +\n  labs(\n    fill = \"Posterior\"\n  ) +\n  scale_fill_manual(\n    values = c(\"blue\", \"darkgray\")\n  ) \n\n\n\n\n\n\n\n\n\nThe blue bars show the posterior probability for each possible value of \\(p\\) after the newest observation was made, and the gray bars show it before the newest observation was made. This illustrates the incremental impact of adding more data to the sample on the resulting posterior distribution.\nWe can apply this same process to the continuous (correct) possible set of values for \\(p\\) (in fact, we’ll create the curves by performing the exact same procedure to a larger, discrete set of values but make the display appear continuous):\n\n\nCode\n# Approximate the set of inifinite p-values by a large set of discrete ones\np_continuous &lt;- seq(0, 1, .01)\n\n# Set the prior probability (uniform over the possibly choices)\nprior &lt;- rep(1 / length(p_continuous), length(p_continuous))\n\n# Set the current posterior as the prior (before any data collected)\nlast_posterior &lt;- prior\n\n# Make result set\nresults &lt;- tibble()\n\n# For each value in the observed sample \nfor(i in 1:N) {\n  \n  # 1. Get the sub-sample\n  sub_sample &lt;- observed_sample[1:i]\n  \n  # 2. Compute metrics (the number of water samples, and the total number of spins)\n  W_temp &lt;- sum(sub_sample)\n  N_temp &lt;- length(sub_sample)\n  \n  # 3. Compute the likelihood for each p\n  temp_likelihood &lt;- p_continuous^W_temp * (1 - p_continuous)^(N_temp - W_temp)\n  \n  # 4. Posterior\n  temp_posterior &lt;- temp_likelihood / sum(temp_likelihood)\n  \n  # 5. Add to results\n  results &lt;-\n    results %&gt;%\n    bind_rows(\n      tibble(\n        sample = i,\n        sequence = paste(sub_sample, collapse = \",\"),\n        p_continuous,\n        likelihood = temp_likelihood,\n        current = temp_posterior,\n        last = last_posterior\n      )\n    )\n  \n  # Set the new last posterior\n  last_posterior &lt;- temp_posterior\n  \n}\n\nresults %&gt;%\n  \n  # Send down the rows\n  pivot_longer(\n    cols = c(last, current)\n  ) %&gt;%\n  \n  # Make a plot\n  ggplot() +\n  geom_area(\n    aes(\n      x = p_continuous,\n      y = value,\n      fill = name\n    ),\n    color = \"black\",\n    alpha = .65,\n    position = \"identity\"\n  ) +\n  facet_wrap(\n    ~paste0(\"Spin: \", factor(sample), \" \\nSample: \", sequence)\n  ) +\n  theme(\n    legend.position = \"top\",\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(colour = \"gray\"),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  xlab(\"p\") +\n  ylab(\"Posterior Probability\") +\n  labs(\n    fill = \"Posterior\"\n  ) +\n  scale_fill_manual(\n    values = c(\"blue\", \"darkgray\")\n  ) \n\n\n\n\n\n\n\n\n\nAgain, these curves are actually approximated here. In practice, we would need to calculate the area underneath the curve to get exact answers about probabilities. Note: We could parameterize the Beta distribution to get the normalizing constant for calculating the actual posterior probabilities that fits this distribution."
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#homework",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#homework",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Homework",
    "text": "Homework\n\nSuppose the globe tossing data had turned out to be 4 water and 11 land. Construct the posterior distribution.\n\nWe’ll cheat a little bit and use the approach of using a large, discrete list of possible values for \\(p\\), and plot it as if it is continuous (we could also just use the Beta distribution). With that, all we need to do is change the values of \\(W\\) and \\(N\\) and use the same code as above.\n\n\nCode\nW_new &lt;- 4\nN_new &lt;- 15\ntibble(\n  p = seq(0, 1, .01), # Approximate the range of p values\n  posterior = p^W_new*(1-p)^(N_new-W_new) # Compute the posterior\n) %&gt;%\n  \n  # Make a plot\n  ggplot() +\n  geom_line(\n    aes(\n      x = p,\n      y = posterior\n    )\n  ) +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\nUsing the posterior distribution from (1), compute the posterior predictive distribution for the next 5 tosses of the same globe.\n\nOkay here I’ll finally acknowledge that the posterior can be written as a Beta distribution, which gives us the normalizing constant needed to make it a real probability distribution (i.e., the area sums to 1). It has the following probability density function (PDF):\n\\[f(x|\\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\] where \\(x \\in [0,1]\\), \\(\\alpha, \\beta &gt; 0\\), and \\(\\Gamma(n) = (n-1)!\\). If we make the following reparameterizations from our set of variables:\n\\[p=x\\] \\[W = \\alpha - 1\\] \\[N-W = \\beta - 1\\] we get:\n\\[\n\\begin{equation}\n\\begin{split}\nf(p|W,N)\n& = \\frac{\\Gamma(W+1+N-W+1)}{\\Gamma(W+1)\\Gamma(N-W+1)}p^W(1-p)^{N-W} \\\\\n& = \\frac{\\Gamma(N+2)}{\\Gamma(W+1)\\Gamma(N-W+1)}p^W(1-p)^{N-W} \\\\\n& = \\frac{(N+1)!}{W!(N-W)!}p^W(1-p)^{N-W} \\\\\n\\end{split}\n\\end{equation}\n\\]\nNote that since \\(p\\) is continuous, this function represents the probability density at any particular value of \\(p\\). To get any positive probability, we must integrate this function over a range of \\(p\\) values. Hence the \\(f\\) notation.\nLet’s quickly plug in \\(W=4\\) and \\(N=15\\) to confirm (at least visually) that this function produces the same posterior we made to answer the last question. We’ll do this by evaluating the density of the derived Beta distribution at range of possible \\(p\\) values. Note we’ll have to use the parameterizations for the Beta distribution that are built into R.\n\n\nCode\n# Reparameterize\nalpha &lt;- W_new + 1\nbeta &lt;- N_new - W_new + 1\n\n# Make a data frame\ntibble(\n  p = seq(0, 1, .01), # Approximate the range of p values (same as before)\n  posterior = dbeta(p, shape1 = alpha, shape2 = beta) # Compute the actual posterior density\n) %&gt;%\n  \n  # Make a plot\n  ggplot() +\n  geom_line(\n    aes(\n      x = p,\n      y = posterior\n    )\n  )\n\n\n\n\n\n\n\n\n\nFrom inspection, it appears that they are essentially the same curves. Note that this time I kept the y-axis labels because this is the density for the actual probability distribution.\nSo back to the question: how do we find the posterior predictive distribution for the next 5 globe spins? Well, if the globe is spun 5 more times, then we can get water on 0, 1, 2, 3, 4, or all 5 spins. Our quest is to figure out the likelihood of each of those possible outcomes based on what we currently know about \\(p\\) (i.e., the posterior distribution). To do this, we’ll use our posterior distribution to run a simulation of the experiment using the following steps:\n\nSelect a random value of \\(p\\) from the posterior\nDraw a random \\(binomial\\) realization where \\(N=5\\)\nRepeat steps i-ii 10000 times\nGraph the results\n\n\n\nCode\n# Set some parameters\nset.seed(123)\nn_experiment &lt;- 5 # Number of new spins we're going to conduct\ns &lt;- 10000 # Number of simulations\n\n# 1. Draw random values of p from the posterior\np_rand &lt;- rbeta(n = s, shape1 = alpha, shape2 = beta) # Same parameters as before\n\n# 2. For each p, run a binomial experiment (represents samples of W)\nw_rand &lt;- rbinom(n = s, size = n_experiment, prob = p_rand)\n\n# Make a data frame\ntibble(\n  w = w_rand\n) %&gt;%\n  \n  # For each w\n  group_by(w) %&gt;%\n  \n  # Compute the total\n  summarise(\n    count = n()\n  ) %&gt;%\n  \n  # Add proportion\n  mutate(\n    proportion = count / sum(count)\n  ) %&gt;%\n  \n  # Make a plot\n  ggplot(\n    aes(\n      x = factor(w)\n    )\n  ) +\n  geom_col(\n    aes(\n      y = proportion\n    )\n  ) +\n  geom_text(\n    aes(\n      y = proportion,\n      label = paste0(round(proportion*100,1), \"%\")\n    ),\n    vjust = -.1\n  ) +\n  scale_y_continuous(\n    labels = scales::percent\n  ) +\n  xlab(\"W\") +\n  ylab(\"Probability\") +\n  labs(\n    title = paste0(\"Posterior Predictive Distribution for \", n_experiment, \" more spins.\")\n  )\n\n\n\n\n\n\n\n\n\nAnother way to think about what we’re doing here is:\n\nFind the collection of density values for all \\(p\\) in the posterior distribution\nFind the probability distribution of the possible outcomes from a Binomial distribution where \\(N=5\\) for all possible values of \\(p\\) (i.e., independent of our posterior)\nTake the average probability value for each possible outcome in (ii) over all values of \\(p\\), weighted by the posterior density in (i)\n\n\n\nCode\n# Make a set of p representive of its domain\np_alt &lt;- seq(0, 1, .001) # Supposed to represent continuous p\n\n# 1. Posterior density values for each p\nposterior_alt &lt;- dbeta(p_alt, shape1 = alpha, shape2 = beta)\n\n# 2. Probability distribution for each outcome for each p\npossible_outcomes &lt;- 0:n_experiment\nlikelihood_alt &lt;- \n  possible_outcomes %&gt;% \n  map_df(\n    ~\n      tibble(\n        binomial_probability = dbinom(x = .x, size = n_experiment, prob = p_alt),\n        p = p_alt,\n        outcome = .x\n      )\n  )\n\n# 3. Get the posterior predictive distribution\nposterior_predictive_distribution &lt;-\n  likelihood_alt %&gt;%\n  \n  # Join to attach the posterior density weight to each value of p\n  inner_join(\n    y = \n      tibble(\n        p = p_alt,\n        posterior_density = posterior_alt\n      ),\n    by = \"p\"\n  ) %&gt;%\n  \n  # For each possible outcome\n  group_by(outcome) %&gt;%\n  \n  # Compute the weighted-average probability\n  summarise(\n    posterior_probability = sum(binomial_probability * posterior_density) / sum(posterior_density)\n  )\n\n# 4. Make a plot\nposterior_predictive_distribution %&gt;%\n  ggplot(\n    aes(\n      x = factor(outcome)\n    )\n  ) +\n  geom_col(\n    aes(\n      y = posterior_probability\n    )\n  ) +\n  geom_text(\n    aes(\n      y = posterior_probability,\n      label = paste0(round(posterior_probability*100,1), \"%\")\n    ),\n    vjust = -.1\n  ) +\n  scale_y_continuous(\n    labels = scales::percent\n  ) +\n  xlab(\"W\") +\n  ylab(\"Probability\") +\n  labs(\n    title = paste0(\"Posterior Predictive Distribution for \", n_experiment, \" more spins.\")\n  )\n\n\n\n\n\n\n\n\n\nThe distributions from the two approaches are merely identical (slight differences due to simulation variability and inexact integration).\n\nUse the posterior predictive distribution from (2) to calculate the probability of 3 or more water samples in the next 5 tosses.\n\nUsing the posterior predictive distribution above, we can look at the percent of simulations that resulted in 3 or more water samples:\n\n\nCode\nmean(w_rand &gt;= 3)\n\n\n[1] 0.1771\n\n\nSo there is a 0.177 probability of 3 or more water samples in the next 5 tosses. However, this point estimate is not totally sufficient because we haven’t reported any uncertainty associated with it. Since we know that \\(W|p \\sim Binomial(n,p)\\), the \\(P(W&gt;=3|p,N)\\) is already determined. In this case, we can just calculate it for each random value of \\(p\\) sampled from the posterior:\n\n\nCode\n# Compute the binomial probability\nprob_binom &lt;- 1 - pbinom(q = 2, size = n_experiment, prob = p_rand)\n\n# Make a plot\nggplot() +\n  geom_density(\n    aes(\n      x = prob_binom\n    )\n  ) +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  xlab(\"P(W&gt;=3|N=5)\") \n\n\n\n\n\n\n\n\n\nThis one has been stumping me a bit, and I’m not totally confident this is the correct result. Another way I was thinking about it was similar the alternative in (2), namely that we are finding the \\(P(W&gt;=3|N=5)\\) for each value of \\(p\\), and then weighting that by the posterior density of \\(p\\). However, it seems like we should be sampling from the predictive distribution in (2) somehow, but if we do that it seems like the precision of our estimates would then just be determined by the number of simulations we run, not the data, which also doesn’t make sense."
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#notes-1",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#notes-1",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Notes",
    "text": "Notes\nGoal: Estimate the percent of the globe that is covered in water\n\nThink of spinning the globe and stopping on a point and repeating many times\nHow do we use that collection of points to come up with an estimate? That’s the goal of today’s lecture\nFirst thought is just indicate each time whether land or water appear as the point; however, how does the shape of the globe impact the likelihood that I will come up with land or water on a “random” toss? Has to do with sampling strategy\n\n\nDefine a generative model\n\n\nThink conceptually about scientifically how the sample was produced (how do variables influence one another)\nVariables: Things we want to observe/estimate or things we actually do observe\n\n\\[\\bf{p} = \\text{proportion of water}\\hskip.5inW=\\text{water observations}\\] \\[N = \\text{number of tosses}\\hskip.5inL=\\text{land observations}\\]\n\nDefine a specific estimand\n\nWere interested in the true proportion of water p\n\nDesign a statistical way to produce estimate\n\n\nHow are these related to each other?\n\nN influences W and L (the more tosses leads to change on other variables)\np also influences W and L (i.e., the true proportion dictates the number of water observations and land observations)\nThe DAG shows relationships, but not what the relationships are. We can say \\(W,L=f(p,N)\\); what is \\(f\\)?\n\nAssume a model (e.g., \\(p\\) = .25, then count likely the sample was under that model, do that for all possible models)\n\n\nTest (3) using (1)\n\n\n\nCode\nsim_globe &lt;-\n  function(p = .7, N = 9) {\n    sample(\n      c(\"W\",\"L\"), # Possible observations\n      size = N, # Number of tosses\n      prob = c(p, 1-p), # The probability of each possible observation\n      replace = TRUE)\n  }\nsim_globe()\n\n\n[1] \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\n\nCode\nreplicate(sim_globe(p =.5, N=9), n=10)\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] \"W\"  \"L\"  \"W\"  \"W\"  \"L\"  \"W\"  \"L\"  \"W\"  \"L\"  \"W\"  \n [2,] \"L\"  \"L\"  \"L\"  \"W\"  \"L\"  \"L\"  \"W\"  \"L\"  \"L\"  \"W\"  \n [3,] \"L\"  \"L\"  \"W\"  \"W\"  \"W\"  \"W\"  \"W\"  \"W\"  \"W\"  \"W\"  \n [4,] \"W\"  \"W\"  \"W\"  \"L\"  \"W\"  \"L\"  \"L\"  \"W\"  \"W\"  \"W\"  \n [5,] \"W\"  \"L\"  \"W\"  \"W\"  \"L\"  \"W\"  \"W\"  \"W\"  \"W\"  \"L\"  \n [6,] \"W\"  \"W\"  \"L\"  \"W\"  \"L\"  \"W\"  \"L\"  \"L\"  \"W\"  \"W\"  \n [7,] \"L\"  \"L\"  \"W\"  \"W\"  \"W\"  \"W\"  \"L\"  \"L\"  \"L\"  \"W\"  \n [8,] \"W\"  \"L\"  \"L\"  \"W\"  \"L\"  \"W\"  \"W\"  \"W\"  \"W\"  \"W\"  \n [9,] \"W\"  \"L\"  \"W\"  \"L\"  \"L\"  \"W\"  \"L\"  \"W\"  \"W\"  \"L\"  \n\n\n\nTest the intent of the code first\nIf our procedure doesn’t work when we know the answer, it certainly won’t when we don’t know the answer\n\nInfinite sample:\n\\[p^W(1-p)^L\\] Posterior probability:\n\\[p = \\frac{(W+L+1)!}{W!L!}p^W(1-p)^L\\]\n\nThis is a Beta distribution, and the likelihood was a Binomial.\nThe minimum sample size for Bayesian analysis is 1.\nThe shape of the posterior distribution embodies the sample size\nNo point estimate, we work with the entire posterior distribution\nThe distribution is the estimate; always use the entire distribution, never a single point\nThe fact that an arbitrary interval contains an arbitrary value is not meaningful\n\n\nAnalyze sample, summarize\n\n\nImplications depend on entire posterior\nAverage over the uncertainty of the posterior\nWhat can we do with the posterior distribution?\n\nWe can take samples from it, and then do calculations with the samples\n\nPosterior Prediction\n\nGiven what we’ve learned, what would happen if we took more samples?\nSampling distribution (predictive distribution) of draws represents the likelihood of each outcome in a new experiment for a particular value\nThe posterior predictive distribution then represents the entire distribution of the statistic of interest, and contains all the uncertainty around that estimate (analogous to the sampling distribution of a statistic (e.g., mean) in the frequentist paradigm, except this is completely model-driven by the posterior instead of based on asymptotics in the frequentist approach)\nSampling turns calculus into a data summary problem; this is important when models get complex and numerically intractable to compute by hand\n\nThis generative, Bayesian framework is the optimal approach for causal estimation if your model is correct.\nIt honestly carries out the assumptions we put into it, using logical implications\nQuantitative framework/asset that activates our qualitative knowledge as scientists, subject matter experts, etc. Let’s the subjective and objective work together. Subjectivity is expertise.\n\nMisclassification\n\nUse circles around variable in DAG to represent unobserved vs. observed variables\nImagine the true number of water samples (W) are unobserved (e.g., measurement error, data error, etc.)\nWe observe a contaminated W (called W*) that is the misclassified sample\nW* is caused by the measurement process M. We can get get back to the correct posterior distribution for p if we use M through W*.\nThe posterior is honest about the uncertaintly induced by the misclassification process\nWhen there is measurement error, model it instead of ignoring it (same for missing data, compliance, inclusion)\nKey point: Samples do not need to be representative of population to provide good estimates, since we can correct them through our causal diagram (modeling the source, sampling process, etc.)\nThis concept may also arise if, for example, the globe was not spun equally likely for every point to be selected."
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#summary-2",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#summary-2",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Summary",
    "text": "Summary\nWe don’t actually need data to construct a model. Our prior distributions, which account for our baseline knowledge about what reasonable values for unknown parameters may be, can produce estimates on their own. A bare minimum strategy is to choose these such that before seeing data, the output of our model produces scientifically reasonable results–there is no reason to allow our model to produce results that we know cannot happen. Then, our data can be introduced to help guide the parameters to an area of focus. In this sense (thinking of the example of points bumping around in parameter space), the data we collect is really just a tool for our model–the model is the central focus, the data just helps the model go to where it needs to go. Also, the idea that there are no correct priors and that priors are just (normalized) posteriors from previous data, make the idea of Bayesian updating very intuitive. It will be interesting to see in coming lectures how we can extend this linear model framework to more “real life” problems with observational data that have potentially tens or hundreds or thousands of potential drivers, and strategies for accounting for the most important ones. Obviously these basic examples are great to build a foundation, but it seems like a huge (sometimes impossible) hurdle to have the time and resources to be able to fully vet out expert-driven causal diagrams and generative models that fully account for all the things, especially in fast-paced environments when everyone is just so busy and there are so many projects to attend to. I’d imagine this is one of the reasons why frequentist analysis persists so much (at least in medical research), because it’s the way it’s been done and therefore you can get more things done faster, even though in an ideal state a Bayesian approach is the right way to go. Definitely something I’ve thought about time and time again–how can we balance the rigor and detail needed to construct the appropriate models to achieve better inference while still being efficient with peoples’ time? Part of it probably has to do with proving to stakeholders that the inference gained from the “quicker” way is less informative (or just plain wrong) compared to the more involved approach."
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#notes-2",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#notes-2",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Notes",
    "text": "Notes\n\nStatistical models can attain arbitrarily accurate predictions without having any explanation or accurate structure (i.e., the model is just plain wrong, but happens to produce accurate predictions at the right time)\n\nExample of this is a previous explanation of orbit pattern of Mars: assuming Earth at the center (geocentric), Mars orbits around Earth but also it’s own local orbit (epi-cycles). Using this model, they got very accurate predictions, but this mechanism is completely wrong.\nOrbits are actually elliptical and around the sun, not Earth\nEven though the first one predicts accurately, because the structure/mechanism is wrong, it doesn’t extend or generalize to other things. However, the correct mechanism is able to explain orbit patterns of all planets in the solar system.\n\nLinear regression is a large class of statistical golems\n\nGeogentric: describes associations, makes good predictions; mechanistically always wrong (but useful), very good approximation; meaning doesn’t depend on the model, depends on an external causal model. Nothing wrong with it unless you actually believe it is the true mechanism.\nGaussian: Abstracts away from detail of general error model; mechanistically silent. General argument about symmetry of error.\n\n\nGaussian\n\nExample: Flip coin, each person take a step to left or right depending on heads/tails, measure distance from center; makes a normal distribution. Why?\n\nThere are more ways for a sequence of coin tosses to get you close to the middle than there are to get you to the left or right\nMany natural processes attract to this behavior because it is adding together small differences\n\nTwo arguments:\n\nGenerative: summed fluctuations tend towards normal. Ex. growth–added fluctuations over time, same age weight tends to be gaussian\nInferential: estimating mean/variance. Best to use since least informative (maximum entropy)\n\nVariable does not need to be normally distributed for normal model to be useful. Machine for estimating mean/variance. Contains the least assumptions. (central limit theorem)\n\nSkills/Goals for Lecture\n\nLearn a standardized language for representing models (generative and statistical)\nCalculate posteriors with multiple unknown parameters\nHow to construct and understand linear models; how to construct posterior predictions from them\n\nReminder of the owl\n\nState a clear question; descriptive, causal, anything; but needs to be clear\nSketch causal assumptions using DAGs; good way for non-theorists to realize they have a lot of subject knowledge and can get it on paper\nDefine a generative model; generates synthetic observations\nUse generative model to build estimator; causal/generative assumptions embedded\nTest, analyze\nProfit: we realize our model was useful, or terrible; either way we gain something\n\nDescribing models\n\nLists variables\nDefine each variable as a deterministic or distributional function of other variables\n\nExercise\n\nGoal: Describe the association between adult weight and height\nHeight causes weight H–&gt;W&lt;–(U) (unobserved influences on body weight)\nGenerative/scientific model: \\(W=f(H,U)\\), \\(W=\\beta H + U\\)\n\n\n\nCode\nsim_weight &lt;-\n  function(H,b,sd) {\n    U &lt;- rnorm(length(H),0,sd)\n    W&lt;-b*H + U\n    return(W)\n  }\n# Generate height\nH &lt;- runif(200,130,170)\nW &lt;- sim_weight(H, b=.5, sd= 5)\nplot(W~H,col=2, lwd = 3)\n\n\n\n\n\n\n\n\n\n\\[W_i=\\beta H_i + U_i\\] \\[U_i \\sim Normal(0,\\sigma)\\] \\[H_i \\sim Uniform(130, 170)\\] 4. Statistical model (estimator)\n\nWe want to estimate how the average weight changes with height.\n\n\\[E(W_i|H_i)=\\alpha + \\beta H_i\\]\n\nPosterior distribution\n\n\\[P(\\alpha, \\beta, \\sigma|H_i,W_i) = \\frac{P(W_i|H_i,\\alpha,\\beta,\\sigma)P(\\alpha,\\beta,\\sigma)}{Z}\\]\n\nGives the posterior probability of a specific regression line\n\nLikelihood: Number of ways we could produce \\(W_i\\), given a line\nPrior: The previous posterior distribution; normalized number of ways previous data could have been produced.\n\n\n\\[W_i \\sim Normal(\\mu_i, \\sigma)\\] \\[\\mu_i = \\alpha + \\beta H_i\\]\n\nGenerally more useful to look at the lines (parameter implications together), instead of individual parameters\nQuadratic approximation\n\nApproximate the posterior distribution using a multivariate Gaussian distribution\nUse the quap function in the rethinking package\n\n\nPrior Predictive Distribution\n\nShould express scientific knowledge, but softly\nWe can make the model make predictions without using data\nNot make ranges that represent the data, but rather just those that make sense based on current knowledge\nAccount for basic reasonable constraints: In general, patients with more weight have more height, and the weight is less than the height, so \\(\\beta\\) is probably between \\([0,1]\\).\nUse these to define some lines based on the assumptions\n\n\n\nCode\nn &lt;- 1000\na &lt;- rnorm(n,0,10)\nb &lt;- runif(n,0,1)\nplot(NULL,xlim=c(130,170),ylim=c(50,90),xlab=\"height(cm)\",ylab=\"Weight(kg)\")\nfor (j in 1:50) abline(a=a[j],b=b[j],lwd=2,col=2)\n\n\n\n\n\n\n\n\n\n\nSome of these are probably not plausible (e.g., high height with low weight). Slopes look good but not intercept\nWe can adjust as needed to create what makes sense\nThere are no correct priors; only scientifically justifiable priors\n\n\nValidate Model\n\n\nBare minimum to test statistical model\nNot because you wrote it, more so to make sure your model works\n\n\nAnalyze data\n\n\nPlug in your data set into your process\nParameters are not independent, can’t interpret as such\nPush out posterior predictions"
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#summary-3",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#summary-3",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Summary",
    "text": "Summary\nThe idea of total vs. direct effects is about specifying the statistical model that will allow you to observe the complete effect (i.e., including differences that could be explained by something else in the model) compared to parsing out differences explained by the variable after adjusting for effects explained through other variables. In the lecture example, the total causal effect of sex on weight was determined by using a (Bayesian) intercept-only model, which showed considerable difference is mean weights between male/female. However, when assessing the direct causal effect, a parameter was added to fit separate slopes for male/female in order to block out the effect of sex on weight that is observed through other causes (in this case, height), such that the resulting estimator looked at mean differences in weight at each height–the posterior distribution for this difference yielded little to no direct effect, indicating that most of the difference in weight between male/females is due to height differences. Another interesting aspect of this lecture was how to think about which way an arrow should go when drawing the causal diagram. You should think of the interventions we are willing to consider, and which make logical sense. For example, we drew \\(H \\rightarrow W\\) because, given a height, it makes sense to employ interventions (such as weight loss program, exercise, etc.) that could presumably impact the resulting weight, but it doesn’t make a lot of sense to think of trying to change someone’s height given their weight. Also, declaring something as a cause of something, generally you first want to think about whether an intervention can be employed, but if not can still make sense if it is a proxy for something else (e.g., age encapsulates time, among many other things that presumably do cause height). We can use flexible curves to fit things (e.g., splines), but we want to make sure we vet out any erroneous areas where estimates don’t make sense, and add necessary restrictions to alleviate. So far, these lectures have given great optimism and excitement for how to approach modeling. I want to be confident in the models I produce, and I think the generative framework is the right approach to be able to believe in the results you are producing. I see so much published research from observational data that declare something statistically significant for a given research hypothesis and say “we adjusted for all these confounders”. Even if I feel fine about the math/statistical procedure, I’m always skeptical about the conclusions that are drawn from it, and quite frankly, don’t feel like it means much at all for really making a decision–there are just too many limitations about all sorts of things. The generative approach gives the tools and rigor to be much more confident in the results, and if we can be more demanding of that rigor, time and energy, it should yield more benefit in the long run. I’d rather spend more time getting to a confident conclusion than just pumping out results.\n\nA check for understanding\nDuring (and after) the lecture, it took me a while to gain intuition about what was happening in the generative simulation for the model:\n\n\n\n\n\nflowchart LR\n  A(Sex) --&gt; C(Weight)\n  A --&gt; B(Height)\n  B --&gt; C\n\n\n\n\n\n\nThe code was written in the following way:\n\n\nCode\n# S = 1 female, S = 2 male\nsim_HW &lt;- function(S,b,a) {\n  N &lt;- length(S)\n  H &lt;- ifelse(S==1,150,160) + rnorm(N,0,5)\n  W &lt;- a[S] + b[S]*H + rnorm(N,0,5)\n  data.frame(S,H,W)\n}\n\n# Generate data\nset.seed(123)\nS &lt;- rbinom(100,1,.5) + 1\ndat &lt;- sim_HW(S, b=c(.5,.6), a=c(0,0))\nhead(dat)\n\n\n  S        H        W\n1 1 151.2666 79.57199\n2 2 159.8573 99.75957\n3 1 149.7856 76.55384\n4 2 166.8430 95.06392\n5 2 158.8711 94.72542\n6 1 157.5824 77.38920\n\n\nFirst, the indexing used here b[S] was odd because b is a vector of length 2, and S is a vector of length 100. But all it is doing is making a vector of length 100 by looking up the index of b at each spot (since S is either 1 or 2). I didn’t know you could index like that in R but I guess you learn something everyday. Anyway, that was not the real thing that confused me.\nIn the lecture, he states that the a term represents the direct effect of sex on weight, and the b term represents the indirect effect (i.e., proportionality/slope for each sex). It’s clear that there are separate lines created for each sex, and you can see the form of an intercept and slope for each one. In my mind, I’m thinking this has to be similar to an interaction in the model, but it wasn’t intuitive to me how this really played out and/or there was something different going on here. After some thought on a notepad, it is exactly what I was thinking–just a linear model with an interaction term between sex and height, though it is reparameterized a little to create the symmetry of effects as discussed in the lecture. Anyway, here is how it translates:\nCurrently, we have that the sex indicators are as follows:\n\\[S = 1 (female), 2(male)\\] Then, the effect of height on weight for each sex is as follows:\n\\[b=(b_{S_1},b_{S_2}) = (0.5, 0.6)\\] Finally, the intercept within each line is:\n\\[a=(a_{S_1},a_{S_2})=(0,0)\\] This leads to:\n\\[W_{S_1} = a_{S_1} + b_{S_1}H + \\epsilon_i = .5H+\\epsilon_i\\] \\[W_{S_2} = a_{S_2} + b_{S_1}H + \\epsilon_i = .6H + \\epsilon_i\\] We could think of this as a single model equation with four (4) regression coefficients looking like the following:\n\\[W = \\beta_1 S_1 + \\beta_2 S_2 + \\beta_3 H \\times S_1 + \\beta_4 H \\times S_2 + \\epsilon_i\\] where\n\\[S_1 = 1 \\text{ if female; 0 otherwise}\\] \\[S_2 = 1 \\text{ if male; 0 otherwise}\\] There is no intercept term in the model. Instead, there are symmetric parameterizations for males and females, instead of making the effects relative to one another (which, as mentioned in the lecture, makes it more intuitive to make priors for). The design matrix for this model would then look something like:\n\n\nCode\ntribble(\n  ~S1, ~S2, ~H_S1, ~H_S2, ~W,\n  1, 0, 150, 0, 80,\n  0, 1, 0, 160, 90,\n  0, 1, 0, 140, 70,\n  1, 0, 165, 0, 75\n)\n\n\n# A tibble: 4 × 5\n     S1    S2  H_S1  H_S2     W\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     0   150     0    80\n2     0     1     0   160    90\n3     0     1     0   140    70\n4     1     0   165     0    75\n\n\nBasically, every time S1 is 1 (female), then S2 is 0 (male), as well as the corresponding value for H.\nI may have to rethink what I just wrote a bit as the the \\(S_1\\) and \\(S_2\\) columns are actually completely redundant, so not sure if this is right yet\nSo how would we parameterize this in a more classical regression model? If we just rewrite a few things. Let:\n\\[S=(0(female), 1(male))\\] Then we assume the model is:\n\\[W = \\beta_0+\\beta_1 S+\\beta_2 H+\\beta_3 S\\times H + \\epsilon_i\\] If we translate parameter values from the example,\n\\[\\beta_0 = 0 \\hskip.1in \\beta_1 = 0\\] \\[\\beta_2 = .5 \\hskip.1in \\beta_3 = .6-.5 = .1\\] We then get:\n\\[\n\\begin{equation}\n\\begin{split}\nW_{S=0(female)}\n&= \\beta_0 + \\beta_1(0) + \\beta_2H+\\beta_3 0 \\times H + \\epsilon_i \\\\\n&= 0 + 0(0) + .5H + .1(0 \\times H) + \\epsilon_i \\\\\n&= 0.5H + \\epsilon_i\n\\end{split}\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n\\begin{split}\nW_{S=1(male)}\n&= \\beta_0 + \\beta_1(1) + \\beta_2H+\\beta_3 1 \\times H + \\epsilon_i \\\\\n&= 0 + 0(1) + .5H + .1(1 \\times H) + \\epsilon_i \\\\\n&= 0.5H + 0.1H + \\epsilon_i \\\\\n&= 0.6H + \\epsilon_i\n\\end{split}\n\\end{equation}\n\\]\nThis makes it much more clear why the direct effect is zero, since the main effect of sex is zero in this model. We only see an effect from sex through the interaction with height, which is what is known as the indirect effect."
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#homework-1",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#homework-1",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Homework",
    "text": "Homework\n\nFrom the Howell1 dataset, consider only the people younger than 13 years old. Estimate the causal association between age and weight. Assume age influences weight through two paths. First, age influences height, and height influences weight. Second, age directly influences weight through age-related changes in muscle growth and body proportions. Draw the DAG that represents these causal relationships. And then write a generative simulation that takes age as an input and simulates height and weight, obeying the relationships in the DAG.\n\nFirst, we’ll import the dataset directly from the package’s Github repository. We will filter to those &lt; 13 years old, and convert height to inches, and weight to lbs:\n\n\nCode\nhowell1 &lt;-\n  read_delim(\n    file = \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\",\n    delim = \";\"\n  ) %&gt;%\n  \n  # Keep those younger than 13\n  filter(age &lt; 13) %&gt;%\n  \n  # Convert the units (more intuitive for me)\n  mutate(\n    height = height / 2.54,\n    weight = weight * 2.205\n  )\n\n\nRows: 544 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\ndbl (4): height, weight, age, male\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhowell1\n\n\n# A tibble: 146 × 4\n   height weight   age  male\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   48     43.3  12       1\n 2   41.5   30.8   8       0\n 3   34     23.1   6.5     0\n 4   43     35.3   7       0\n 5   45     39.4  11       1\n 6   48     45.0   8       1\n 7   50.8   51.5  12       0\n 8   38.5   29.3   5       0\n 9   43.5   34.0   9       0\n10   38.5   28.1   5       0\n# ℹ 136 more rows\n\n\nNext, we can write the causal diagram as described:\n\n\n\n\n\nflowchart LR\n  A(Age) --&gt; C(Weight)\n  A --&gt; B(Height)\n  B --&gt; C\n\n\n\n\n\n\nIf we let\n\\[A=Age \\hskip.1in H=Height \\hskip.1in W=Weight\\]\nThen, we assume that:\n\\[H = f_H(A) \\hskip.2in W = f_W(A,H)\\] Finally, we can write the generative simulation to produce synthetic data governed by the DAG:\n\n\nCode\n# Simulating synthetic children\nsim_children &lt;-\n  function(N) {\n    \n    # 1. Generate uniform ages\n    A &lt;- runif(N, 0, 13)\n    \n    # 2. Generate heights as a linear combination of age\n    H &lt;- 22 + 2*A + rnorm(N, mean = 0, sd = 1)\n    \n    # 3. Generate weights as a linear combination of age and height\n    W &lt;- .8*H + rnorm(N, mean = 0, sd = .5)\n    \n    # Make a data frame\n    tibble(A, H, W)\n  }\n\n\nWe first generate ages uniformly from 0 to 13, so\n\\[A \\sim Uniform(0,13)\\]\nThen, we generate heights from a normal distribution with means that are linearly related to the age.\n\\[H \\sim Normal(\\mu =22 + 2 \\times A, \\sigma = 3)\\] Notice the intercept term to ensure a positive height for children who are 0 years old. Note: A distribution like the Gamma may be better here to ensure we don’t get negative heights. For younger ages, I would assume that the distribution of heights has a little right-skew. In any case, we’ll move forward with the Normal distribution here for simplicity sake.\nThen we generate the weights as a linear function of the observed age and heights.\n\\[W \\sim Normal(\\mu=.8H, \\sigma = .5)\\] We make the assumption that there is a linear relationship between weight with age and height, and that for any age, the increase in mean weight per inch increase in height is the same. In fact, the effect for age is 0 since it is observed through height.\n\n\nCode\n# Simulate some children\nset.seed(123)\nsimmed_children &lt;- sim_children(200)\nplot(simmed_children)\n\n\n\n\n\n\n\n\n\n\nUse a linear regression to estimate the total causal effect of each year of growth on weight.\nNow suppose the causal association between age and weight might be different between boys and girls. Use a single linear regression, with a categorical variable for sex, to estimate the total causal effect of age on weight separately for boys and girls. How do boys and girls differ? Provide one or more posterior contrasts as a summary."
  },
  {
    "objectID": "post/statistical-rethinking-2023-class-notes/index.html#notes-3",
    "href": "post/statistical-rethinking-2023-class-notes/index.html#notes-3",
    "title": "Statistical Rethinking 2023 Class Notes",
    "section": "Notes",
    "text": "Notes\n\nThe linear regression can approximate anything, so we need to design it with the causal model in mind\nGenerative models + multiple estimands, we’ll have multiple estimands\nNeed post-processing of posterior distribution to gain inference of joint distributino\nWe require categories, splines, etc. to build causal estimators\nNeed to stratify by category to get at the estimands we want (separate lines)\n\nExample\n\nExtend example above to include patient sex, age\nNeed to determine how height, weight, sex are causally related (add to DAG), and statistically related\nTo determine which way the arrows go, think about the interventions you’re willing to consider\nDon’t have to draw them, but the implied unobserved causes of each variable are implied\n\nThese are ignorable unless shared across variables\nEx. temperature is a cause of sex and weight in some species\n\nWhat is the causal effect of S on W?\n\nAccounts for direct and indirect effect\nWe can also ask what is the direct causal effect of S on W?\nThese questions require different models\n\nGenerally want to assign the same prior for parameters for each category level (below)\n\nUsing indexing is advantageous because you have symmetry such that all parameters can get the same prior, they are all interpreted the same within their levels\nUsing indicators makes parameters relative to other levels, which causes you have to put priors on other parameters because it is an adjustment parameter (one is an average, one is an adjustment to an average)\n\n\n\\[W_i \\sim Normal(\\mu_i, \\sigma) \\hskip.1in \\mu_i=\\alpha_{S[i]}\\] \\[\\alpha = [\\alpha_1, \\alpha_2] \\hskip.1in \\alpha_j \\sim Normal(60,10)\\] Total Causal Effect\n\nSimulate one data set of all males, another of all females, look at the average difference in weight\n\nThis is the actual causal effect\nThen you can generate a random data set, run the modeling process, and then ensure that the model provides the expected estimate\n\nLook at the posterior distribution of the mean difference, and randomly draw samples from the individual posteriors and compute the differences to answer questions like “what is the probability that a randomly selected male will be heavier than a randomly selected female?”\nThis was basically just an intercept-only model for sex, and the effect due to height would be captured in that difference\n\nDirect Effect\n\nHow do we partial out the indirect effect of Height (block it)?\n\nStratify by height to block the association between S and W that is transmitted through H\n\nDifference in intercept, the indirect is slope differences\nHere, the model allows for separate slopes by sex, so we can tease out the impact of height\n\nCenter the height to make the interpretation of the intercept be the average\nMakes priors more intuitive, and computation easier\n\n\n\\[W_i \\sim Normal(\\mu_i, \\sigma) \\hskip.1in \\mu_i=\\alpha_{S[i]} + \\beta_{S[i]}(H_i-\\bar{H})\\] \\[\\alpha=[\\alpha_1,\\alpha_2] \\hskip.1in \\beta=[\\beta_1,\\beta_2]\\]\n\nIn this case, nearly all the total effect of sex on weight is explained through height (the direct effect (posterior of the difference between weights at each height) is nearly 0 at all heights)\n\nCurve Fitting\n\nWe use linear models to do this; i.e., it’s not mechanistic, but we use it wisely\nStrategies\n\nPolynomials: Don’t do it; no local smoothing, only global; learn to much from data in regions that lie far away\n\nIt’s not worth having a model that looks OK for most of the data that we know is completely erroneous (e.g., parabola at some point shows babies get heavier as their height decreases, which we know is wrong); even though this is a small portation of observations, it’s still knowingly wrong, so why use it?\n\nSplines & GAMs: Not as bad as polynomials; add total many locally trained terms\n\n\nSplines\n\nFlexible curve that will find trends\nB-splines are linear models containing additive terms with synthetic variables\n\nThink of it as a collection of individual curves (basis functions), but the weight of each basis function is non-zero at only particular areas of x, and spline is the sum of the curves at a particular point\n\n\n\\[\\mu_i = \\alpha + w_1B_{i,1} + w_2B_{i,2} + ...\\]\n\nIdeal model for age/height would be to account for what we know about human biology: infant, toddler, adolescent, adult. In the first 3, we expect only upward growth, so we should constrain.\n\nFull Luxury Bayes\n\nEquivalent approach is to use one model for entire causal sample\nThen run simulations from overall system to get answers to specific queries"
  },
  {
    "objectID": "post/managing-the-readmission-risk-pool/index.html",
    "href": "post/managing-the-readmission-risk-pool/index.html",
    "title": "A prediction system for managing the hospital readmission risk pool",
    "section": "",
    "text": "Each year hospitals across the United States get penalized by CMS, withholding up to 3% of all Medicare reimbursement for an entire fiscal year, for having excess readmissions. Additionally, there are implications from commercial payors, quality programs, etc. that make it a focal point of the general value-based care landscape. Not to mention the obvious patient burden (both financially and psychologically) of being hospitalized twice in a short period of time. Thus, it has become a key area of focus for hospitals in monitoring the overall health of their clinical and financial operations.\nIn turn, the day-to-day discussion becomes one of strategy: what interventions, processes, and workflows should be put in place to proactively prevent hospital readmissions from occurring? This is not totally straightforward. Do we focus on preventing the readmission, or the initial hospitalization altogether? You can’t have a readmission without an index stay. For which patients? Who needs which resources? There can be conflicting priorities. Hospitals depend on admissions (mostly from commercial payors) to keep the lights on. Also, programs like the Hospital Readmissions Reduction Program (HRRP) (CMS’ penalty program) only apply to Medicare beneficiaries, which is a subset of the overall hospital population (and sometimes the most unprofitable). Should we just put our resources towards preventing admissions for the unprofitable patients? Probably not. Obviously, we are constrained morally (and probably legally) from giving payor-based, preferential treatment, but this is simply the reality of the things stakeholders need to sift through. In my view, the best you can do to balance things, at least to start, is to be meticulously aware of what is happening–through data.\nIn this realm, one component of particular interest is using predictive analytics to anticipate and intervene on high-risk patients in order to prevent a subsequent hospitalization. Despite there being a large body of work by researchers developing creative and innovative approaches for preventing readmissions, the reality is that many hospitals do not leverage all the literature that is available because (a) most of it is just that–research, and it’s difficult to confidently translate and tailor that to an actionable program for any one hospital, and (b) it is simply too difficult to parse and organize because there is so much of it.\nSo, when hospitals do go down the route of implementing predictive tools for readmissions, it seems to be more readily available, yet sub-optimal modeling frameworks that are used. For example, Epic has their own readmission risk predictor native to its EHR platform for this very purpose. The common thing I notice about the tools used in practice (and in the way CMS does it) is the modeling setup: they predict the likelihood of readmission based on the state of the patient at the time of hospital discharge. Yet, it is well known, and frankly just common sense, that the actual drivers, the real, preventable reasons, for a readmission are circumstances of the patient after being discharged from the hospital. That’s not to say these scores aren’t correlated with the rates of readmission, or can’t provide a useful marker; but with the complexities of managing a diverse hospital population, it makes it difficult to figure out what to do with a rapidly stale risk score that only reflects how the patient was when they left the hospital, and ignores everything that happened after.\nThus, a reframing of the modeling problem is in order. And in fact, it’s more than that. In this effort, how the information is displayed, propagated and relayed between clinical teams and leadership is as (if not more) important than the shear goal of accurate risk estimation (according to statistical metrics). It requires cross-functional involvement (from the get-go), strategic design, attention to nuance and rigor, and a flexible scope in order to tie hospital-wide impact down to the individual patient.\nThis is my idea of how you might build a tool to effectively manage the readmission risk pool."
  },
  {
    "objectID": "post/managing-the-readmission-risk-pool/index.html#prototype",
    "href": "post/managing-the-readmission-risk-pool/index.html#prototype",
    "title": "A prediction system for managing the hospital readmission risk pool",
    "section": "A prototype",
    "text": "A prototype\nSo what might such a tool look like? Well here’s one possibility (the source code for this is here):\n\nNot too fancy, but it’s a start. The focal point being the interactive map widget. Suppose your service area was the state of Wisconsin. Each dot represents a patient in the risk pool at their home address (i.e., they were recently discharged from the hospital), and the relative size/color of the dot represents the amount of risk. For example, in the popup box, Patient 240 was discharged 13 days ago with a 47% readmission risk, and has 17 days remaining in the risk pool.\nIt gives a simple and intuitive way to view the current risk pool for the hospital (system). Care managers can use it to identify patients for intervention in real-time, while leadership can use it to quickly get a pulse on the total volume of patients across the system at risk for readmission and how that would affect aggregated metrics over a period of time. Of course, there can be many enhancements to this for better utility, such as functionality to filter patients by disease categories, service lines, discharge location, PCP location (if they have one), whether they have a visit scheduled, payor, who will contribute to the HRRP (and who will not)…the list goes on. The goal is to have a readily-available, cohesive tool that can provide day-to-day actionable information for all parties involved."
  },
  {
    "objectID": "post/managing-the-readmission-risk-pool/index.html#subsequent-impact",
    "href": "post/managing-the-readmission-risk-pool/index.html#subsequent-impact",
    "title": "A prediction system for managing the hospital readmission risk pool",
    "section": "Subsequent impact",
    "text": "Subsequent impact\nThe effect of this as a starting point is that the jump to predictive analytics becomes much more intentional. You get what you can from understanding what has already happened, which I think is a lot, and then proceed to a more advanced (i.e., predictive) solution that is much more well-defined once processes have been optimized with the current state and its capabilities have reached their limit. Then when you want to add that additional feature, the focus can be on that, making it much more clear for everyone who needs to be involved (e.g., IT, data analytics, clinicians, managers, etc.) what the specific goals are.\nFor example, if a care management team can already quickly and intuitively answer all the questions they have about an individual or group of patients with respect to things that already occurred in order to manage the risk pool on a day-to-day basis, then naturally the predictive piece only arises when the need is necessary (and predictably, it’s probably related to resource utilization). In a world with unlimited resources, they could just continually intervene on the entire risk pool everyday, until they know each patient is not going to be readmitted (and ideally after). But in our world, they’ll be faced with scenarios like:\n\nThere are 500 patients in the risk pool, but I only have time to intervene on 50 of them. Which ones should I choose?\nWhich patients are currently at the highest risk of being readmitted?\nPatient A was discharged 20 days ago, and patient B was discharged 10 days ago. Which one should I intervene on today?\n\nAt which point the necessary personnel can be convened, and an enhancement can be implemented for that specific purpose.\nAll-in-all, going back to the introduction, this is about being meticulously aware."
  },
  {
    "objectID": "post/managing-the-readmission-risk-pool/index.html#build-the-pipelines",
    "href": "post/managing-the-readmission-risk-pool/index.html#build-the-pipelines",
    "title": "A prediction system for managing the hospital readmission risk pool",
    "section": "Build the pipelines",
    "text": "Build the pipelines\nFrom a technological perspective, we can first consider putting in a random number as a placeholder for the risk. This allows us to build the data pipelines, infrastructure and workflows needed to support the models once we’re ready for the real thing. Some key things to think about:\n\nWill it be delivered through an API?\nWhat databases need to be accessed, and when, in order to evaluate the models and produce a prediction?\nHow will we monitor the model’s accuracy, and have the ability to iterate/update it?\nHow will administrators and clinical teams interact with it?\nWhere/how should the number(s) be displayed in the application?\n\nHashing these things out enables a plan to be put in place for the product as a whole, and ensures the buy in from all of the teams that will be needed to use and maintain it.\nAt this point, it’s still just a systems, logistical, and personnel problem. Once everyone understands how it’s going to work, then the focus (for the data science teams) can transition to the math."
  },
  {
    "objectID": "post/managing-the-readmission-risk-pool/index.html#model",
    "href": "post/managing-the-readmission-risk-pool/index.html#model",
    "title": "A prediction system for managing the hospital readmission risk pool",
    "section": "The model",
    "text": "The model\nWe’re going to get a little technical here.\nLet\n\\[D = \\text{Readmission duration of interest (e.g., 30 days)}\\] \\[T = \\text{Time since discharge to the current time point}\\] \\[R = \\text{Time that the patient is readmitted (if at all)}\\]\nThen what we want to estimate is:\n\\[P(R \\leq D | R &gt; T)\\] In layman’s terms, were simply asking this: if, as of now, a patient is still at risk for readmission (i.e., in the risk pool), what is the probability that they will be readmitted in the remaining window of interest, given they have not been readmitted up to this point?\nSo if a patient was discharged 20 days ago, and they still have not been readmitted, we want to know how likely it is that they will be readmitted in the next 10 days (assuming we care about 30 day readmissions), given what has happened since discharge."
  },
  {
    "objectID": "post/managing-the-readmission-risk-pool/index.html#how-do-we-estimate-it",
    "href": "post/managing-the-readmission-risk-pool/index.html#how-do-we-estimate-it",
    "title": "A prediction system for managing the hospital readmission risk pool",
    "section": "How do we estimate it?",
    "text": "How do we estimate it?\nNotice that the model is actually quite general, which leaves the door open to many possible ways to structure the data and choose statistical methodologies as seen fit. We’re basically needing some modeling process, however defined, that can produce a predicted probability at an arbitrary point in time, accounting for what has happened up to that point. Intuitively, I think some sort of Bayesian approach would be pretty cool, since you can conceptually think of updating yesterday’s risk with the new information you’ve gathered about the patient up to today (e.g., maybe they just completed an office visit). In that sense the information is more naturally accumulating. But since I haven’t thought that one through yet, I’ll propose a simpler way to start:\n\n1. Start with the baseline risk\nWe’re not trying to totally recreate the wheel here. Those discharge-based risk estimates we talked about in the introduction can still be valuable at that point in time. Since they already account for many patient/clinical characteristics before and during the hospitalization, we might as well use it. Additionally, if a hospital (system) is already using, say, an Epic readmission risk score, this can just be seen as a day-to-day updating of that. However, if you don’t have/want this, this framework will still work.\nWe just want an estimate of the patient’s risk of readmission when they leave the hospital, however that may be generated. If you want to pretend you know nothing at that time, then just think of starting everyone at the overall average risk.\n\n\n2. Define the denominators\nLet’s assume we’ll generate one prediction per day for each patient over the course of the 30-days after they are discharged. So, we’re going to train separate models for each of those days: 1, 2, …, 30. Thus, we need to define the set of patients who were in the risk pool at each of those points in time over whatever period our training data will cover. For example, for the Day 5 model, our denominator will consist of patients who were readmitted after 5 days from discharge or were not readmitted at all. For the Day 20 model, patients could not have been readmitted before 20 days post-discharge. And so on. So, these denominators get smaller and smaller for increasing time points.\n\n\n3. Define the numerators\nAnalogously, we need to indicate whether the patient was in fact readmitted or not. For the Day 5 denominator, we want to indicate if they were readmitted in the following 25 days. For the Day 20 denominator, we want to indicate if they were readmitted in the following 10 days.\n\nChoosing an outcome distribution\nAs defined, the numerator sets us up for a binary classification model, like logistic regression, or other more complex machine learning (ML) algorithms. However, we may alternatively think of keeping track of the actual number of days until the readmission, which conforms more to a time-to-event model, like Cox-proportional hazards (or an ML equivalent). In the latter case, we could choose to censor patients at the \\(D-T\\) time point (e.g., 30 days), or a later time point.\nThe thing I like about the time-to-event setup is that it allows us to more easily estimate risks for any future time points, like 60 days or 90 days, and doesn’t try to equate a patient who was readmitted at 31 days with a patient who was not readmitted at all, as a binary outcome would. We know practically that these patients are not the same.\n\n\n\n4. Append the predictors\nNow the fun part. At a given point in time, what do we think are the main indicators/events that occur during follow-up that will impact the risk that a patient will ultimately be readmitted? Here I’m mostly focused on interventions, assuming most of the demographics and clinical history is already accounted for in the baseline risk (and we can actually use this risk as a predictor in all of the models too). This might include things like:\n\nDid they complete a follow-up visit?\nDo they have a visit scheduled?\nDid they fill their prescription?\nDo they have transportation means?\n…the list goes on…\n\nWe need to define these at time point \\(T\\) for each model. Now this list might vary by time point (e.g., if the things impacting readmission risk at Day 1 are different than those at Day 25), by disease (e.g., readmission risk factors for a post-surgical patient are presumably different than a COPD patient), or a host of other things. We might have different models by these different subgroups, so all of these nuances should be discussed as a preparatory step to modeling.\n\n\n5. Calibrate to baseline risk\nWe can think of what we’re constructing as a risk trajectory over follow-up, so if we’re using an already-established baseline risk score at discharge, then each of our subsequent predictions can just be transformed to be relative to that, making a smooth transition from discharge through follow-up.\nNow, you can train the models. It would also be a good idea to concurrently setup a model monitoring mechanism, to be able to continually track and evaluate model performance over time, and perform ad-hoc analysis related to the effectiveness of the tool in its intent. Setting up this infrastructure in advance allows agility in the ability to iterate and update models as needed to ensure that they stay relevant."
  },
  {
    "objectID": "post/managing-the-readmission-risk-pool/index.html#an-updated-prototype",
    "href": "post/managing-the-readmission-risk-pool/index.html#an-updated-prototype",
    "title": "A prediction system for managing the hospital readmission risk pool",
    "section": "An updated prototype",
    "text": "An updated prototype\nNow suppose we have built our models and integrated them into the tool above, such that each patient gets attached to them their current risk of readmission as of now (the source code for this is here).\n\nFor example, in the popup box, Patient 254 was discharged from the hospital 12 days ago with a readmission risk of 14.1%, and has 18 days left in the risk pool. However, we now see that this patient’s current risk of readmission has been reduced to 2.4%, presumably due to the various interventions and events that have taken place since then. Additionally, we can see that, as a group, we expected 37 (7.4%) patients to be readmitted once discharged, but now we only expect 23 (4.6%) to be, indicating that as a hospital (system) our interventions seem to be effectively reducing risk. But we do still have the ability to identify those individuals still currently at high risk, by finding the dots on the map that are largest and closer to red in gradient.\nThe aggregated expected readmission rate could be a useful metric for hospital leadership to keep a “global” pulse with, being able to see when an influx of readmissions may be coming, but is driven by the individual risk estimates that the front line care teams are working with, so it’s all tied together. The longitudinal look at these metrics would also be useful in the same vein for answering questions like, “How many readmissions are we expecting in the next week?”, which would allow you to promptly allocate extra resources to put out that fire in real-time. Analogously, we could add risk trajectories for individual patients, such that you can view how a patient’s risk has changed each day since discharge, to get a better feel for which interventions helped reduce it (or didn’t have any effect).\nNow we have a tool in place to proactively identify high-risk patients in real-time that is optimally integrated into clinical/operational workflows. We also have a real-time pulse on the bigger picture, such as what the overall readmission rate is, how it is changing over time, the expected HRRP penalty amount, etc. Again, although there has been a significant improvement in the overall management of readmissions, these components may reach their limit of intended purpose, and people might start asking questions like: “We can accurately identify who is at highest risk, but what is the best intervention to use?” That’s where the last part comes in."
  },
  {
    "objectID": "post/low-cost-ways-to-build-and-deploy-apps/index.html",
    "href": "post/low-cost-ways-to-build-and-deploy-apps/index.html",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "",
    "text": "In the era of artificial intelligence (AI), organizations are often told they must buy in or get left behind. I’ve attended various presentations like this. The presenters discuss how things are changing, provide some high-level overview of AI, and then convey that the organization must develop a strategy for it. However, the feeling left in the room is usually that of confusion and ambiguity. They understand that they should be thinking about it, but it’s totally unclear what (if anything) they should actually do.\nFirst, it’s not really clear what is meant by “AI”, as it can mean many things (I often don’t know what they mean by it either). Second, the implication seems to be pointed toward AI in the context of tools that can be purchased from vendors, therefore adding a financial stressor to decision makers when figuring out how to act. It also conveys a one-sided message: that AI is products that you buy, never really discussing the technical foundation, and thus foregoing the idea of taking these concepts and building incrementally.\nSure, there are all sorts of obscure ways organizations can use tools like ChatGPT, for example, but I would venture to guess that in the bigger picture, much of the target audience is at a relatively early stage of their data science journey, and unaware of all of the opportunity that is available using low-cost, or free, programming languages and technologies that can get them well on their way to a mature advanced analytics function, without making a direct jump to cost-intensive solutions in hopes that they “work”. There’s a better way.\nData science is all about asking the right questions and figuring out ways to provide answers to those questions to the people who need them at the right time so they can take action. This is somewhat arbitrary, but that also means it leaves the door open to an infinite number of ways to develop solutions for it, many of which can be solved (a) by going back to the fundamentals: data quality, data collection, data storage, infrastructure, reporting workflows, etc., and (b) with, at least to start, freely-available software, tools and programming languages. AI is simply hyperbole for data and analytical strategy. There’s lots of ways to start using data better, and that doesn’t necessarily mean opening your wallet."
  },
  {
    "objectID": "post/low-cost-ways-to-build-and-deploy-apps/index.html#locally",
    "href": "post/low-cost-ways-to-build-and-deploy-apps/index.html#locally",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "1. Locally",
    "text": "1. Locally\nWe already said that the primary goal of sharing an application is to get it off of your local machine, but this is still worth mentioning, because it is how you start. Once you install R, RStudio, and the shiny package, you’ll have everything you need to begin developing and running apps. This is of course a great way to develop, and often the way you would do it regardless, because you can write the code, run the app to see how it works, make changes, test, and repeat.\nBut once it’s finalized and you want to share it, without other infrastructure in place, you would need to send the code files to someone else and have them run it on their own machine, which means they need to have all the software installed as well. This can be a totally legitimate approach in certain cases. It is certainly a starting point, but it’s far from ideal.\n\nSide note\nAnother reason I mention local “deployment” is because I actually use this approach quite often. When I am analyzing a dataset, it is extremely useful to be able to spin up an application for my own usage to easily explore certain aspects of it, among many other uses. In this case, the audience is me, and I can make it work however I need it to serve my purposes. You can do this too, and don’t need it to run anywhere but your own computer, yet it is still very useful."
  },
  {
    "objectID": "post/low-cost-ways-to-build-and-deploy-apps/index.html#shinyapps",
    "href": "post/low-cost-ways-to-build-and-deploy-apps/index.html#shinyapps",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "2. shinyapps.io",
    "text": "2. shinyapps.io\nThe most obvious location to deploy a Shiny app is the place specifically purposed for it: shinyapps.io. It is a cloud-based server built and maintained by Posit (who we’ll see come up a lot in this article) that is made for hosting apps built in this framework. It has been around for quite a while, and, as we’ll see, there have been other platforms developed for hosting these apps that may be a better fit. Nevertheless, this is a totally viable option and great way to start deploying things, and they have a free tier that allows you to deploy up to five (5) applications with 25 hours of active usage per month.\n\nThe Basics\nThe concept is simple: develop your app on your local machine (e.g., in RStudio) and then run the rsconnect::deployApp function to deploy it to shinyapps.io. You can create an account with your email address or sign up through GitHub (like I did), among other options. Here is an outline of the steps I took to deploy this app:\n\n1. Make an application\nFirst you just need to code up an app. This could be very simple like the one found here to get started.\nI’m using an existing demo app we have in our GitHub page. From Terminal we can execute (and you can too):\ngit clone https://github.com/centralstatz/ExampleApps.git\nThe app we want is located at ExampleApps/ReadmissionRiskPool.\n\n\n2. Configure connection to shinyapps.io\nWhen you sign up, you’ll get a basic list of instructions for getting started. Assuming you are doing it with R, you need to install the rsconnect R package and register your account information, which includes tokens/secrets that are provided with your shinyapps.io account.\n# Install the configuration library\ninstall.packages(\"rsconnect\")\n\n# Setup account information\nmy_user_name &lt;- \"&lt;GET_FROM_ACCOUNT&gt;\"\nmy_token &lt;- \"&lt;GET_FROM_ACCOUNT&gt;\"\nmy_secret &lt;- \"&lt;GET_FROM_ACCOUNT&gt;\"\nrsconnect::setAccountInfo(\n  name = my_user_name,         \n  token = my_token,     \n  secret = my_secret\n)\n\n\n3. Deploy the application\nOnce your account is configured, you can call rsconnect::deployApp() with the application specified and it will be uploaded to the host server.\n# Assume we're working in the repo just cloned\nrsconnect::deployApp(\"ReadmissionRiskPool\")\nAnd just like that we have a custom, interactive web application available to the world.\nOne thing you might notice is the web URL: https://tgzz86-alex0zajichek.shinyapps.io/readmissionriskpool/\nIt is obviously not very nice looking. One way to clean this up would be to have a better username (the first part of it). Then, once you start getting into the shinyapps.io paid plans, they allow for custom domains.\nAnother part of this is security. The app we just deployed is publicly available, which is definitely not always what is desired. Again, with the paid tiers you can begin introducing authentication into the applications for restricted access."
  },
  {
    "objectID": "post/low-cost-ways-to-build-and-deploy-apps/index.html#posit-cloud",
    "href": "post/low-cost-ways-to-build-and-deploy-apps/index.html#posit-cloud",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "3. Posit Cloud",
    "text": "3. Posit Cloud\nThis platform takes it up a notch. Posit Cloud, developed and maintained by the same company, is not only a place to share applications, but a cloud-based service where you can develop your code as well, so everything is accessed in your web browser. As the website describes it is a great platform for collaboration, teaching, etc. in a more all-encompassing environment.\nYou can organize content into “spaces”, which are disjointed areas for separate work streams. Within each space, and at the content item level, you can control settings for who has access, to what, and how much. Each user creates a login to the platform, and is able to access the spaces that they have created or have been shared with them. The free tier enables you a single shared space (which you can invite other users to) with up to 25 projects/outputs and limited computation usage. There is also infrastructure in place to establish connections to databases, among other things, that make it a feasible home for full-fledged data solutions. The other very important thing to note is that it is not only Shiny applications that can be deployed here, but all sorts of other analytical content such as R Markdown and Quarto documents, API’s, etc. Here is an introductory video for more information straight from the source:\n\n\nHow we use Posit Cloud\nWith the paid plans of Posit Cloud, you are allowed an unlimited number of spaces, as well as beefed-up compute. This is what enables us to take advantage of this platform as an offering for client engagements. Each client gets their own private space which all of their analytical content we are collaborating on lives, and only we (us and the client) have access. Here’s one possible example of how a client engagement could work:\n\nA client reaches out because they want to build a web application that enables them to interactively explore data related to their customer base on a reactive map, among other functionality.\nWe initialize a new space in Posit Cloud and invite the client via email. They receive the invitation link that takes them to the space upon login, creating an account if they have not done so already.\nWe initialize a new private (or public, if applicable) GitHub repository on our organization’s page to hold all of the application’s source code and its change-tracking history. Optionally, the client can be added as a member with viewing privileges of these files as well for full transparency.\nWe initialize a new RStudio project within the workspace sourced from the created GitHub repository. This serves as the development environment for the application.\nThe client has a dataset in a large Excel file that we would like to use as the source for the application. It gets uploaded into the application’s project by us after the client sends it in an email (or alternatively, the client goes in and uploads it themselves).\nThe app development work begins. We work iteratively with the client to construct it most optimally to fit their needs (occassionally pushing code to GitHub to track changes). Because they have direct access to the space, they can see it anytime. We can quickly show progress updates, bounce ideas back and forth, test functionality, and answer questions in a timely manner, until it is satisfactory. Then, we deploy the final application, and the client is enabled to go into it on-demand and use it as seen fit.\nOver time, the data becomes stale so the client would like it to be updated on a recurring monthly basis. One option would be to send a new Excel file each month and we will manually update and re-deploy the application. Instead, we decide to use Posit Cloud’s built-in data integration capabilities. So we establish a connection to a SQL database in which the Excel file was sourced from, and build the queries directly into the application’s source code so that the application itself is sourced directly from the database. No middleman required.\n\nOverall, Posit Cloud is a highly recommended tool to use for individuals and/or teams of people to get started with application development and deployment. Or even if you’re a seasoned Shiny developer, the infrastructure you get out of the box is amazing. And again, it is free."
  },
  {
    "objectID": "post/low-cost-ways-to-build-and-deploy-apps/index.html#connect-cloud",
    "href": "post/low-cost-ways-to-build-and-deploy-apps/index.html#connect-cloud",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "4. Connect Cloud",
    "text": "4. Connect Cloud\nPosit just recently launched the Alpha release of their newest platform, called Connect Cloud. It is in its early phase, and it’s all about easy deployment. As the home page states, there are three (3) steps to get an application up and running with a shareable link that can be accessed from anywhere:\n\nCreate a new account by authenticating with GitHub (so you must have a GitHub account)\nLink to a public repository (from GitHub) containing the code for a Shiny application (or whatever other type of content you’re deploying)\nDeploy the application and share the link\n\nWe did this with the application deployed in #2, and yes, it’s that easy (and, once again, free). The link for the app on this platform is here.\nThe main thing you have to remember to do before making your final commit to GitHub, and subsequently configuring the app connection to the repository, is to create the manifest.json file in your application’s root directory to capture the environment parameters that need to be created (automatically by Connect Cloud) for it to run. This can be done with a simple command:\nrsconnect::writeManifest()\nYou can watch this video to get a more thorough introduction:\n\nWe don’t quite yet know where this platform is going to go given that it is in such early stages. Although the infrastructure it already provides, and the ease at which it is done, is magic-like, there are of course a lot of things it doesn’t have (yet): login without GitHub, source apps from private repositories, private content, authentication, security, etc. These were the things I wondered about when I attended the live webinar above, which in turn had me thinking how it exactly compares with the purpose of Posit Cloud (in the video, I explicitly asked this at 29:26). It sounds like these “commercial” considerations are all part of the roadmap, so we are very excited to see where it goes.\nNevertheless, even in its current state, Connect Cloud is a highly recommended platform to start using for anyone wanting to build a data science portfolio and deploy public content."
  },
  {
    "objectID": "post/low-cost-ways-to-build-and-deploy-apps/index.html#shiny-server",
    "href": "post/low-cost-ways-to-build-and-deploy-apps/index.html#shiny-server",
    "title": "Low cost ways to build and deploy analytical web apps",
    "section": "5. Shiny Server",
    "text": "5. Shiny Server\nWant total control over the infrastructure? If yes, Shiny Server might be for you.\nThis is an open-source server configuration that can be installed on-premises (or wherever you’d like). The huge advantage is that it is always free–you just need to install the software. The disadvantage is that you need a server bulky enough to handle the desired compute and you need people who know how to manage it. Compared to the others, this option is like the wild west. It provides you the basic skeleton to get stuff working, but from there you have ultimate freedom to do with it what you wish, which can easily get out of hand as the developer/user bases grow and you’re trying to maintain environments as new software package versions are constantly being released.\nI always consider this a fantastic option for relatively large organizations who are early in their data science journeys. They likely have mature information technology (IT) teams supporting existing systems, but have not yet invested heavily in advanced analytics infrastructure. This provides an opportunity to leverage those mature systems admin teams to setup up and manage the backend of this infrastructure that then enables data scientists in the organization to deliver great analytic content (and prove its value) without spending tons of money on acquiring the software. Low risk with huge potential.\n\nAmazon Web Services (AWS)\nDespite it being open-source, most individuals (or small companies for that matter) probably don’t have adequate server space to implement Shiny Server in the way they’d like. However, you can do it if you use external compute resources. Amazon EC2 is one way to make it happen. And still for free.\nThe way it works is that you spin up an EC2 instance of your choice (essentially a computer, ranging from very low to very high compute power, the t2.micro being the one that you can use for free), install Shiny Server on the instance, and then have all the freedom in the world to deploy applications to it and access them on the web. This is an excellent resource that I followed when learning how to do this, which you can see a detailed account of my steps here as well.\nThe real cool thing about this setup is that you can take it wherever you want to go. You have unlimited ability to customize the design of your server pages, integrate it with other software/tools, assign custom domains, etc. The possibilities are endless. When I learned how to do this, I was able to quickly get it to a state where my server is live and accessible at a subdomain of one of my websites: http://apps.zajichekstats.com/. Clicking that link takes you to the home page of my Shiny Server on an AWS EC2 t2.micro instance, where subsequent applications are found at subpages of that (e.g., http://apps.zajichekstats.com/shinydemo-aws/). This entire setup, including the subdomain assignment, was done for free, and it is barely scratching the surface of what can be done with it."
  },
  {
    "objectID": "post/the-overlap-weight/index.html",
    "href": "post/the-overlap-weight/index.html",
    "title": "The overlap weight in survival analysis",
    "section": "",
    "text": "I was recently introduced to overlap weighting, which is part of a general family of methods for balancing covariates when estimating treatment effects with observational data. Specifically, it focuses on the clinical equipoise; that is, the patients in which the treatment decision is most uncertain. I find it more elegant than matching (which I’ve used in the past), and figured a useful way to better understand the approach is to deconstruct, interpret, and translate a SAS simulation in the context of survival analysis implemented by the original authors. All code is written in (and translated to) R. We’ll start by loading some packages.\nCode\nlibrary(tidyverse)\nlibrary(survival)\nlibrary(reactable)"
  },
  {
    "objectID": "post/the-overlap-weight/index.html#simulatepatients",
    "href": "post/the-overlap-weight/index.html#simulatepatients",
    "title": "The overlap weight in survival analysis",
    "section": "Simulate some patients",
    "text": "Simulate some patients\nThe first thing we need to do is generate some (fake) patients to facilitate the simulation. Here we’ll focus on age, sex, and income as patient-identifying characteristics.\n\n\nCode\n# Set the sample size\nn &lt;- 10000\n\n# Set the seed\nset.seed(123)\n\n# Build the population\npopulation &lt;-\n  tibble(\n    age = rnorm(n, mean = 50, sd = 10),\n    zage = (age - 50) / 10,\n    male = rbinom(n, size = 1, prob = 0.6),\n    income = rnorm(n, mean = 50, sd = 10),\n    zincome = (income - 50) / 10\n  )\npopulation\n\n\n# A tibble: 10,000 × 5\n     age    zage  male income zincome\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  44.4 -0.560      0   36.5  -1.35 \n 2  47.7 -0.230      1   44.2  -0.579\n 3  65.6  1.56       1   41.4  -0.861\n 4  50.7  0.0705     1   59.7   0.973\n 5  51.3  0.129      0   56.2   0.619\n 6  67.2  1.72       1   63.9   1.39 \n 7  54.6  0.461      1   35.1  -1.49 \n 8  37.3 -1.27       1   56.4   0.639\n 9  43.1 -0.687      1   53.7   0.375\n10  45.5 -0.446      1   53.7   0.370\n# ℹ 9,990 more rows\n\n\nWe’ve generated a sample of 10000 patients. We’ll assume age is measured in years, income in thousands of dollars ($), and male is 1 for Male and 0 for Female. The columns zage and zincome are just standardized versions of the originals to avoid scaling nuances (we’ll refer to these as \\(age_z\\) and \\(income_z\\))."
  },
  {
    "objectID": "post/the-overlap-weight/index.html#treatmentpropensity",
    "href": "post/the-overlap-weight/index.html#treatmentpropensity",
    "title": "The overlap weight in survival analysis",
    "section": "Define the treatment propensity",
    "text": "Define the treatment propensity\nHere’s where the important theory starts to creep in (already). We assume that there is a true propensity score, say \\(p_i^A\\), that is the true probability that patient i receives treatment A given their specific characteristics (and let’s assume there are possible treatments A & B). Further, we assume (some transformation of) this probability is a linear combination of all the characteristics that confound the crude treatment effect on the outcome. In this simulation, we’ll assume the logit model:\n\\[log(\\frac{p_i^A}{1 - p_i^A}) = 0.41 \\times age_z - 0.22 \\times male - 0.69 \\times income_z - 0.40\\]\nSo let’s add this true propensity score to the population data set:\n\n\nCode\npopulation &lt;-\n  population |&gt;\n  \n  # Add the true propensity score (unknown quantity)\n  mutate(\n    log_odds_pA = -0.4 + log(1.5)*zage + log(0.8)*male + log(.50)*zincome,\n    pA = 1 / (1 + exp(-log_odds_pA))\n  )\npopulation\n\n\n# A tibble: 10,000 × 7\n     age    zage  male income zincome log_odds_pA    pA\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1  44.4 -0.560      0   36.5  -1.35        0.311 0.577\n 2  47.7 -0.230      1   44.2  -0.579      -0.315 0.422\n 3  65.6  1.56       1   41.4  -0.861       0.606 0.647\n 4  50.7  0.0705     1   59.7   0.973      -1.27  0.219\n 5  51.3  0.129      0   56.2   0.619      -0.777 0.315\n 6  67.2  1.72       1   63.9   1.39       -0.888 0.292\n 7  54.6  0.461      1   35.1  -1.49        0.595 0.644\n 8  37.3 -1.27       1   56.4   0.639      -1.58  0.171\n 9  43.1 -0.687      1   53.7   0.375      -1.16  0.238\n10  45.5 -0.446      1   53.7   0.370      -1.06  0.257\n# ℹ 9,990 more rows\n\n\nObviously in practice we don’t know what \\(p_i^A\\) is, so it must be estimated from the treatment assignments we observe in the data. Additionally, we aren’t strictly required to assume the logit model, but when we use it to estimate the propensity score for overlap weighting, it has the advantageous property of perfect balance. That is, the weighted-mean differences for all covariates included in the logistic regression model will be zero across the treatment groups."
  },
  {
    "objectID": "post/the-overlap-weight/index.html#treatmentassignment",
    "href": "post/the-overlap-weight/index.html#treatmentassignment",
    "title": "The overlap weight in survival analysis",
    "section": "Generate the treatment assignment",
    "text": "Generate the treatment assignment\nThe treatment assignment is one of the known quantities we would observe in a real life sample, and that is what would be used as the dependent variable for estimating propensity scores. However, since we are running a simulation, we need to generate the treatment assignments from the governing distribution. We assume that the observed treatment for patient i is the result of an (unfair) coin flip that has a probability equal to the true propensity score. In statistical notation,\n\\[A_i \\sim Bernoulli(p_i^A)\\]\nwhere \\(A_i\\) is the indicator of treatment A. Let’s add a realized treatment assignment to the population:\n\n\nCode\npopulation &lt;-\n  population |&gt;\n  \n  # Add the realized treatment assignment (known quantity)\n  mutate(\n    A = rbinom(n, size = 1, prob = pA),\n  )\npopulation\n\n\n# A tibble: 10,000 × 8\n     age    zage  male income zincome log_odds_pA    pA     A\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1  44.4 -0.560      0   36.5  -1.35        0.311 0.577     1\n 2  47.7 -0.230      1   44.2  -0.579      -0.315 0.422     0\n 3  65.6  1.56       1   41.4  -0.861       0.606 0.647     1\n 4  50.7  0.0705     1   59.7   0.973      -1.27  0.219     0\n 5  51.3  0.129      0   56.2   0.619      -0.777 0.315     1\n 6  67.2  1.72       1   63.9   1.39       -0.888 0.292     1\n 7  54.6  0.461      1   35.1  -1.49        0.595 0.644     0\n 8  37.3 -1.27       1   56.4   0.639      -1.58  0.171     0\n 9  43.1 -0.687      1   53.7   0.375      -1.16  0.238     0\n10  45.5 -0.446      1   53.7   0.370      -1.06  0.257     0\n# ℹ 9,990 more rows\n\n\nIn this case, A=1 represents treatment A and A=0 represents treatment B. Again, this is the actual treatment we would observe the patient receiving in a sample."
  },
  {
    "objectID": "post/the-overlap-weight/index.html#trueoverlapweight",
    "href": "post/the-overlap-weight/index.html#trueoverlapweight",
    "title": "The overlap weight in survival analysis",
    "section": "Compute the (true) overlap weight",
    "text": "Compute the (true) overlap weight\nWe need to define what the overlap weight is, and it’s actually quite simple: just assign the patient the probability they did not receive their observed treatment.\n\\[\n\\begin{equation}\nOW_i=\n    \\begin{cases}\n        1-p_i^A & \\text{if } A_i=1\\\\\n        p_i^A & \\text{if } A_i=0\\\\\n    \\end{cases}\n\\end{equation}\n\\]\nNotice the notation. Since it depends on the true propensity score, the overlap weight is another unknown quantity that is estimated from the data. It also depends on the realized treatment assignment, so the collection of weights across patients differ depending on the observed treatment distribution. We can add these weights to the population data set:\n\n\nCode\npopulation &lt;-\n  population |&gt;\n  \n  # Add the true overlap weight for the realized treatment (unknown quantity)\n  mutate(\n    OW = A * (1-pA) + (1-A) * pA\n  )\npopulation\n\n\n# A tibble: 10,000 × 9\n     age    zage  male income zincome log_odds_pA    pA     A    OW\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1  44.4 -0.560      0   36.5  -1.35        0.311 0.577     1 0.423\n 2  47.7 -0.230      1   44.2  -0.579      -0.315 0.422     0 0.422\n 3  65.6  1.56       1   41.4  -0.861       0.606 0.647     1 0.353\n 4  50.7  0.0705     1   59.7   0.973      -1.27  0.219     0 0.219\n 5  51.3  0.129      0   56.2   0.619      -0.777 0.315     1 0.685\n 6  67.2  1.72       1   63.9   1.39       -0.888 0.292     1 0.708\n 7  54.6  0.461      1   35.1  -1.49        0.595 0.644     0 0.644\n 8  37.3 -1.27       1   56.4   0.639      -1.58  0.171     0 0.171\n 9  43.1 -0.687      1   53.7   0.375      -1.16  0.238     0 0.238\n10  45.5 -0.446      1   53.7   0.370      -1.06  0.257     0 0.257\n# ℹ 9,990 more rows\n\n\nIn a real analysis, these weights are what we are ultimately after in order to estimate the average causal treatment effect on the outcome of interest while balancing differences in patient characteristics across the groups (i.e., removing the confounding factors).\n\nTarget population\nNow we do end up normalizing the weights so they sum to one within each treatment group. But the intuition about what’s happening is that the focus of the treatment effect estimation is shifted to patients that are most likely in either treatment group. This is known as the clinical equipoise, and the resulting treatment effect is interpreted as the average treatment effect in the overlap population.\nThis makes a lot of sense. When we’re comparing treatments, we should focus most heavily on patients that could be candidates for either, and less so on patients that were bound for one. Thus, we up-weight patients who have the most overlap in characteristics with the opposing treatment group. The beautiful thing here is that we do this without throwing away any information; smoothly and proportionately weighting each subject just the amount that we should."
  },
  {
    "objectID": "post/the-overlap-weight/index.html#treatmenteffect",
    "href": "post/the-overlap-weight/index.html#treatmenteffect",
    "title": "The overlap weight in survival analysis",
    "section": "Set the treatment effect",
    "text": "Set the treatment effect\nWe’ve generated the treatments and established how they are related to patient characteristics, but haven’t talked about the outcome in which we’re ultimately interested in estimating the treatment effect for. Since we’re focusing on time-to-event outcomes, we’ll stay in that context, but the general idea is that we assume there exists a realization of what a patient’s outcome would have been under each treatment scenario. Then if we compare the difference of those outcomes across all patients, the average difference must be caused by the treatment.\n\nDefining the event times\nIn this simulation, we’ll generate event times from a Weibull distribution. Starting at treatment initiation, this might be the time until cancer recurrence, hospitalization, or something else; we’re just looking to see how long it takes for some event to occur. The PDF for this distribution looks like this:\n\\[f(t) = \\frac{\\alpha}{\\sigma}\\left(\\frac{t}{\\sigma}\\right)^{\\alpha-1}e^{-\\left(\\frac{t}{\\sigma}\\right)^\\alpha}\\]\nwhere \\(t\\) is the event time, \\(\\alpha\\) is the shape parameter, and \\(\\sigma\\) is the scale parameter. In our example, we will say that:\n\\[T_i^A \\sim Weibull(\\alpha, \\sigma_i^A)\\] \\[T_i^B \\sim Weibull(\\alpha, \\sigma_i^B)\\] where \\(T_i^A\\) and \\(T_i^B\\) are the event times under treatments A and B, respectively. That is, the event times for each patient under each treatment follow a Weibull distribution with a common shape parameter (in this case, across both treatments), but a scale parameter that depends on the patient’s specific characteristics (in log-linear form) and differs by treatment, which captures the treatment effect. Specifically,\n\\[\\alpha = 1\\] \\[\\sigma_i^A = \\lambda \\times e^{-(\\phi_i - 0.36)}\\] \\[\\sigma_i^B = \\lambda \\times e^{-\\phi_i}\\] \\[\\phi_i = 1.10 \\times age_z - 0.22 \\times male - 0.36 \\times income_z\\] \\[\\lambda = 4055.56\\]\nBasically, the \\(\\phi_i\\) term does the baseline adjustment on the outcome risk for the patient’s specific characteristics, and the treatment effect is simply a fixed, additive deviation from that for all patients. The \\(\\lambda\\) term is the baseline hazard function, providing the scale parameter when all covariate values are zero (for treatment B), which in this case is constant over time.\n\nInterpreting the treatment effect\nWhen setting \\(\\alpha=1\\), the mean of a Weibull distribution is equal to its scale parameter. That is,\n\\[E[T_i^A] = \\sigma_i^A\\] \\[E[T_i^B] = \\sigma_i^B\\] This gives us a very nice interpretation of the treatment effect. If we compare them relatively, we get:\n\\[\n\\begin{equation}\n\\begin{split}\n\\frac{E[T_i^A]}{E[T_i^B]} & = \\frac{\\sigma_i^A}{\\sigma_i^B} \\\\\n& = \\frac{\\lambda \\times e^{-(\\phi_i - 0.36)}}{\\lambda \\times e^{-\\phi_i}} \\\\\n& = \\frac{e^{-(\\phi_i - 0.36)}}{e^{-\\phi_i}} \\\\\n& = \\frac{e^{0.36}e^{-\\phi_i}}{e^{-\\phi_i}} \\\\\n& = e^{0.36} \\\\\n& \\approx 1.43\n\\end{split}\n\\end{equation}\n\\]\nSo we can say that the time to event for patients on treatment A is 1.43 times, or 43%, longer on average than those on treatment B, given a fixed age, sex, and income. That is, treatment A is “better” than treatment B.\n\n\n\nSampling the event times\nNext, we need to actually sample the times for our population (note that it was confirmed that SAS and R have the same parameterizations):\n\n\nCode\npopulation &lt;-\n  population |&gt;\n  \n  # Sample outcome event times under each treatment (counterfactual)\n  mutate(\n    \n    # Baseline hazard (constant over time)\n    lambda = 1*365/0.09,\n    \n    # Define TRUE Weibull linear predictor\n    phi = log(3) * zage + log(0.8) * male + log(.70) * zincome,\n    sigma_A = lambda * exp(-(phi + log(0.70))),\n    sigma_B = lambda * exp(-phi),\n    \n    # Generate REALIZED survival times under each treatment scenario (same parameterizations in SAS)\n    t_A = rweibull(n, shape = 1, scale = sigma_A),\n    t_B = rweibull(n, shape = 1, scale = sigma_B), \n  )\npopulation |&gt; select(lambda, phi, sigma_A, sigma_B, t_A, t_B)\n\n\n# A tibble: 10,000 × 6\n   lambda     phi sigma_A sigma_B    t_A   t_B\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1  4056. -0.133    6617.   4632.  5689. 7760.\n 2  4056. -0.269    7585.   5309.  1831. 9482.\n 3  4056.  1.80      961.    673.   488.  300.\n 4  4056. -0.493    9482.   6637. 23722.  255.\n 5  4056. -0.0788   6269.   4388.  7649. 1800.\n 6  4056.  1.17     1804.   1263.  1953. 1493.\n 7  4056.  0.814    2568.   1797.  5462. 2652.\n 8  4056. -1.84    36514.  25560. 38262. 2385.\n 9  4056. -1.11    17604.  12323.  3577. 6894.\n10  4056. -0.845   13485.   9439. 38541. 3743.\n# ℹ 9,990 more rows\n\n\nNow we can make some density plots comparing the event time distributions across treatments (we’ll use log scaling for visual appeal):\n\n\nCode\npopulation |&gt;\n  \n  # Send down the rows\n  pivot_longer(\n    cols = starts_with(\"t_\"),\n    names_to = \"Treatment\",\n    values_to = \"EventTime\",\n    names_prefix = \"t_\"\n  ) |&gt; \n  \n  # Make a plot\n  ggplot() + \n  geom_density(\n    aes(\n      x = log(EventTime),\n      fill = Treatment\n    ),\n    alpha = .75\n  ) + \n  scale_x_continuous(labels = function(x) round(exp(x))) +\n  theme(\n    panel.background = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    legend.position = \"top\"\n  ) +\n  xlab(\"Event Time\")\n\n\n\n\n\n\n\n\n\nNotice the shift to the right in the distribution for treatment A. This is the treatment effect. In the hypothetical world where all patients received treatment A, the event times happened later than in the world where all patients received treatment B (and all event times were observed), indicating, on average, a “benefit” to treatment A, which was expected from our prior calculation."
  },
  {
    "objectID": "post/the-overlap-weight/index.html#observedoutcome",
    "href": "post/the-overlap-weight/index.html#observedoutcome",
    "title": "The overlap weight in survival analysis",
    "section": "Assign the observed outcome",
    "text": "Assign the observed outcome\nThe final step for simulation setup is to assign the event time as we might observe in a real data set. The event times generated in the previous section capture both treatment scenarios for all patients, but in reality we would only (potentially) observe the event time for the treatment the patient received. Additionally, we likely (or definitely) won’t be following all patients long enough to observe all events occur, meaning some patients will be censored. Let’s add the observed outcomes to the population:\n\n\nCode\npopulation &lt;-\n  population |&gt; \n  \n  # Add the observed outcome\n  mutate(\n    \n    # The ACTUAL event time outcome depends on the treatment ACTUALLY observed for the patient\n    actual_event_time = t_B * (1 - A) + t_A * A,\n    \n    # Generate a REALIZED censoring time (completely random)\n    censor_time = 500 + 500 * runif(n),\n    \n    # Calculate the OBSERVED time in the data set (known quantity)\n    time = pmin(actual_event_time, censor_time), # They either had the event, or were censored first\n    \n    # Calculate the event status (TRUE if event observed, FALSE if censored)\n    status = as.numeric(actual_event_time &lt; censor_time)\n  )\npopulation |&gt; select(actual_event_time, censor_time, time, status)\n\n\n# A tibble: 10,000 × 4\n   actual_event_time censor_time  time status\n               &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1             5689.        843.  843.      0\n 2             9482.        814.  814.      0\n 3              488.        882.  488.      1\n 4              255.        936.  255.      1\n 5             7649.        653.  653.      0\n 6             1953.        890.  890.      0\n 7             2652.        737.  737.      0\n 8             2385.        703.  703.      0\n 9             6894.        952.  952.      0\n10             3743.        526.  526.      0\n# ℹ 9,990 more rows\n\n\nOur final simulated data set has the following observed outcome summaries:\n\n\nCode\npopulation |&gt;\n  \n  # Compute summary metrics\n  summarize(\n    Patients = n(),\n    Time = mean(time),\n    .by = c(A, status)\n  ) |&gt; \n  \n  # Add shares within treatment\n  mutate(\n    Percent = Patients / sum(Patients),\n    .by = A\n  ) |&gt;\n  \n  # Clean/rearrange\n  transmute(\n    Treatment = \n      case_when(\n        A == 1 ~ \"A\",\n        TRUE ~ \"B\"\n      ),\n    Status = \n      case_when(\n        status == 1 ~ \"Event\",\n        TRUE ~ \"Censored\"\n      ),\n    Patients,\n    Percent,\n    Time\n  ) |&gt;\n  arrange(Treatment, desc(Status)) |&gt;\n  \n  # Make a table\n  reactable(\n    groupBy = \"Treatment\",\n    columns = \n      list(\n        Patients = colDef(aggregate = \"sum\", align = \"center\"),\n        Percent = colDef(name = \"Percent of patients in treatment group (%)\", aggregate = \"sum\", align = \"center\", format = colFormat(digits = 1, percent = TRUE)),\n        Time = colDef(name = \"Avg. time to outcome\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 1))\n      ),\n    striped = TRUE,\n    highlight = TRUE,\n    bordered = TRUE,\n    resizable = TRUE,\n    theme = reactablefmtr::sandstone()\n  ) |&gt;\n  reactablefmtr::add_source(\"Use arrows to expand table\", font_size = 12, font_style = \"italic\")\n\n\n\nUse arrows to expand table\n\n\n\n\nWe can also look at the survival curves.\n\n\nCode\npopulation |&gt;\n  \n  # Nest by treatment\n  group_by(A) |&gt;\n  nest() |&gt;\n  \n  # Esimtate survival curves for each treatment\n  mutate(\n    Surv = \n      data |&gt;\n      map(\n        function(.trt) {\n          \n          # Fit the model\n          mod &lt;- survfit(Surv(time, status) ~ 1, data = .trt)\n          \n          # Extract the elements\n          tibble(\n            Time = mod$time,\n            Survival = mod$surv,\n            Lower = mod$lower,\n            Upper = mod$upper\n          )\n          \n        }\n      )\n  ) |&gt; \n  \n  # Unnest the curves\n  select(-data) |&gt;\n  unnest(cols = Surv) |&gt;\n  ungroup() |&gt;\n\n  # Clean labels\n  mutate(\n    Treatment = \n      case_when(\n        A == 1 ~ \"A\",\n        TRUE ~ \"B\"\n      )\n  ) |&gt;\n  \n  # Make a plot\n  ggplot(\n    aes(\n      x = Time,\n      y = Survival\n    )\n  ) + \n  geom_line(aes(color = Treatment)) +\n  geom_ribbon(\n    aes(\n      ymin = Lower,\n      ymax = Upper,\n      fill = Treatment\n    ),\n    alpha = .25\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\"),\n    legend.position = \"top\"\n  ) +\n  xlab(\"Time since treatment\") +\n  ylab(\"Survival Probability\")\n\n\n\n\n\n\n\n\n\nIf we weren’t doing a simulation, we wouldn’t know how much of these differences are explained by the treatment. And actually, these crude survival curves suggest longer survival from treatment B, even though we know the opposite is true. The goal is to use this known information (along with patient characteristics) to estimate the causal treatment effect that is baked into all of the unknown quantities we have here that would be unavailable to us in a real-life analysis."
  },
  {
    "objectID": "post/the-overlap-weight/index.html#modelpropensityscores",
    "href": "post/the-overlap-weight/index.html#modelpropensityscores",
    "title": "The overlap weight in survival analysis",
    "section": "Model the propensity scores",
    "text": "Model the propensity scores\nWe’ve established that the overlap weights are quantities that need to be estimated from the data. In order to calculate those weights, we first need to estimate the propensity scores. We know what the true model is, but let’s assume we are only working with observable data:\n\n\nCode\npopulation |&gt; select(zage, male, zincome, A, time, status)\n\n\n# A tibble: 10,000 × 6\n      zage  male zincome     A  time status\n     &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.560      0  -1.35      1  843.      0\n 2 -0.230      1  -0.579     0  814.      0\n 3  1.56       1  -0.861     1  488.      1\n 4  0.0705     1   0.973     0  255.      1\n 5  0.129      0   0.619     1  653.      0\n 6  1.72       1   1.39      1  890.      0\n 7  0.461      1  -1.49      0  737.      0\n 8 -1.27       1   0.639     0  703.      0\n 9 -0.687      1   0.375     0  952.      0\n10 -0.446      1   0.370     0  526.      0\n# ℹ 9,990 more rows\n\n\nOur goal is to obtain patient-specific probabilities of receiving treatment A. We’ll estimate this with a logistic regression model, but we’ll allow for some flexibility in the shape of the relationships between the continuous variables (age and income) and the outcome with the use of restricted cubic splines (see my other post for another explanation):\n\n\nCode\n# Fit the propensity score model\nps_mod &lt;-\n  glm(\n    formula = A ~ rms::rcs(zage, 3) + rms::rcs(zincome, 3) + factor(male),\n    data = population,\n    family = \"binomial\"\n  )\nsummary(ps_mod)\n\n\n\nCall:\nglm(formula = A ~ rms::rcs(zage, 3) + rms::rcs(zincome, 3) + \n    factor(male), family = \"binomial\", data = population)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  -0.30027    0.06527  -4.600 4.22e-06 ***\nrms::rcs(zage, 3)zage         0.40206    0.05099   7.885 3.16e-15 ***\nrms::rcs(zage, 3)zage'       -0.07337    0.05813  -1.262    0.207    \nrms::rcs(zincome, 3)zincome  -0.63792    0.04950 -12.888  &lt; 2e-16 ***\nrms::rcs(zincome, 3)zincome' -0.04020    0.06434  -0.625    0.532    \nfactor(male)1                -0.25432    0.04415  -5.760 8.41e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 13359  on 9999  degrees of freedom\nResidual deviance: 12193  on 9994  degrees of freedom\nAIC: 12205\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe model seems to indicate that the non-linear terms for age and income don’t particularly matter (which is expected), but we’ll leave it as-is. Next, let’s attach the estimated propensity scores to the population:\n\n\nCode\npopulation &lt;-\n  population |&gt;\n  \n  # Add the estimated propensity score\n  mutate(\n    pA_hat = predict(ps_mod, type = \"response\") # P(A = 1 | X)\n  )\n\n\nWe can take a look at the estimated propensity score distribution across the treatment groups (we’ll also overlay the true distributions for comparison):\n\n\nCode\npopulation |&gt;\n  \n  # Send PS scores down the rows\n  pivot_longer(\n    cols = c(pA, pA_hat),\n    names_to = \"Type\",\n    values_to = \"Score\"\n  ) |&gt;\n\n  # Clean labels\n  mutate(\n    Treatment = \n      case_when(\n        A == 1 ~ \"A\",\n        TRUE ~ \"B\"\n      ),\n    Type = \n      case_when(\n        Type == \"pA\" ~ \"True\",\n        TRUE ~ \"Estimated\"\n      )\n  ) |&gt;\n  \n  # Make a plot\n  ggplot() + \n  geom_density(\n    aes(\n      x = Score,\n      fill = Treatment,\n      linetype = Type\n    ),\n    alpha = .40\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  theme(\n    panel.background = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    legend.position = \"top\"\n  ) +\n  xlab(\"P(A=1|X)\")\n\n\n\n\n\n\n\n\n\nThe model does a great job at estimating the true propensity scores. In a real analysis we probably wouldn’t be this close since we likely wouldn’t have only and all true confounders accounted for.\n\nVisualizing propensity score effects\nIn the same vein, we can explore the modeled relationships between each patient characteristic and the propensity scores.\n\n\nCode\nplots &lt;-\n  population |&gt;\n  \n  # Send PS scores down the rows\n  pivot_longer(\n    cols = c(pA, pA_hat),\n    names_to = \"Type\",\n    values_to = \"Score\"\n  ) |&gt;\n  \n  # Clean labels\n  mutate(\n    Treatment = \n      case_when(\n        A == 1 ~ \"A\",\n        TRUE ~ \"B\"\n      ),\n    Type = \n      case_when(\n        Type == \"pA\" ~ \"True\",\n        TRUE ~ \"Estimated\"\n      )\n  ) |&gt;\n  \n  # Send covariates down the rows\n  pivot_longer(\n    cols = c(zage, zincome, male),\n    names_prefix = \"^z\"\n  ) |&gt;\n  \n  # Make groups\n  mutate(\n    Group = \n      case_when(\n        name == \"male\" ~ \"categorical\",\n        TRUE ~ \"continuous\"\n      )\n  ) |&gt;\n  \n  # Make a nested frame\n  group_by(Group) |&gt;\n  nest() |&gt;\n  \n  # Make plot depending on type\n  mutate(\n    plot = \n      data |&gt;\n      map(\n        function(.group) {\n          \n          # Check condition\n          if(n_distinct(.group$value) == 2) {\n            \n            .group |&gt;\n              summarize(\n                Rate = mean(Score),\n                .by = c(Treatment, Type, name, value)\n              ) |&gt;\n              mutate(\n                Sex = \n                  case_when(\n                    value == 1 ~ \"Male\",\n                    TRUE ~ \"Female\"\n                  ),\n                name = \"Sex\"\n              ) |&gt;\n              ggplot() +\n              geom_point(\n                aes(\n                  x = Sex,\n                  y = Rate,\n                  color = Treatment,\n                  shape = Type,\n                  group = Type\n                )\n              ) +\n              geom_line(\n                aes(\n                  x = Sex,\n                  y = Rate,\n                  color = Treatment,\n                  linetype = Type,\n                  group = interaction(Type, Treatment)\n                )\n              ) +\n              facet_wrap(~name) +\n              coord_flip() +\n              scale_y_continuous(labels = scales::percent) +\n              theme(\n                panel.background = element_blank(),\n                panel.grid.major.x = element_line(color = \"gray\"),\n                legend.position = \"top\",\n                axis.title.y = element_blank()\n              ) +\n              ylab(\"P(A=1|X)\")\n            \n          } else {\n            \n            .group |&gt; \n              ggplot() + \n              geom_smooth(\n                aes(\n                  x = value,\n                  y = Score,\n                  color = Treatment,\n                  fill = Treatment,\n                  linetype = Type\n                ),\n                alpha = .25\n              ) +\n              facet_wrap(~name) +\n              scale_y_continuous(labels = scales::percent) +\n              theme(\n                panel.background = element_blank(),\n                panel.grid.major.y = element_line(color = \"gray\"),\n                legend.position = \"top\"\n              ) +\n              xlab(\"Z-Score\") +\n              ylab(\"P(A=1|X)\")\n          }\n          \n        } \n      )\n  ) \n\ngridExtra::grid.arrange(plots$plot[[1]], plots$plot[[2]])\n\n\n\n\n\n\n\n\n\nAs known from the true model, patients who received treatment A tend to be older, female patients with lower income. We can also see slight miscalibration in the estimates for patients who are older in that the model tends to underestimate the true propensity score in these patients. We see similar miscalibration for sex."
  },
  {
    "objectID": "post/the-overlap-weight/index.html#estimatedoverlapweight",
    "href": "post/the-overlap-weight/index.html#estimatedoverlapweight",
    "title": "The overlap weight in survival analysis",
    "section": "Calculate the (estimated) overlap weight",
    "text": "Calculate the (estimated) overlap weight\nWe’ve already defined how to calculate the overlap weights. The only difference here is that we’ll do it from the estimated propensity scores instead of the true ones. First, we’ll apply the formula to add the estimated weights to the population:\n\n\nCode\npopulation &lt;-\n  population |&gt;\n  \n  # Add the estimated overlap weight for the realized treatment (known quantity)\n  mutate(\n    OW_hat = A * (1-pA_hat) + (1-A) * pA_hat\n  )\n\n\nAs we’ve mentioned earlier, we will then normalize the weights within each treatment group so they have the same cumulative contribution for estimating the treatment effect in the outcome model.\n\n\nCode\npopulation &lt;- \n  population |&gt;\n  \n  # Add the normalized weights (adding true and estimated)\n  mutate(\n    OW_norm = OW / sum(OW),\n    OW_hat_norm = OW_hat / sum(OW_hat),\n    .by = A\n  )\npopulation |&gt; select(A, pA, pA_hat, OW, OW_norm, OW_hat, OW_hat_norm)\n\n\n# A tibble: 10,000 × 7\n       A    pA pA_hat    OW   OW_norm OW_hat OW_hat_norm\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1     1 0.577  0.583 0.423 0.000202   0.417   0.000198 \n 2     0 0.422  0.427 0.422 0.000201   0.427   0.000202 \n 3     1 0.647  0.610 0.353 0.000168   0.390   0.000185 \n 4     0 0.219  0.226 0.219 0.000105   0.226   0.000107 \n 5     1 0.315  0.329 0.685 0.000327   0.671   0.000318 \n 6     1 0.292  0.265 0.708 0.000338   0.735   0.000348 \n 7     0 0.644  0.628 0.644 0.000307   0.628   0.000297 \n 8     0 0.171  0.181 0.171 0.0000814  0.181   0.0000856\n 9     0 0.238  0.250 0.238 0.000114   0.250   0.000118 \n10     0 0.257  0.268 0.257 0.000123   0.268   0.000127 \n# ℹ 9,990 more rows\n\n\nTo get a sense of the impact of these weights, let’s first look at their distributions (again, adding the true weights for comparison):\n\n\nCode\npopulation |&gt;\n  \n  # Send overlap weight down the rows\n  pivot_longer(\n    cols = c(OW_norm, OW_hat_norm),\n    names_to = \"Type\",\n    values_to = \"Weight\"\n  ) |&gt;\n  \n  # Clean labels\n  mutate(\n    Treatment = \n      case_when(\n        A == 1 ~ \"A\",\n        TRUE ~ \"B\"\n      ),\n    Type = \n      case_when(\n        Type == \"OW_norm\" ~ \"True\",\n        TRUE ~ \"Estimated\"\n      )\n  ) |&gt;\n  \n  # Make a plot\n  ggplot() + \n  geom_density(\n    aes(\n      x = Weight,\n      fill = Treatment,\n      linetype = Type\n    ),\n    alpha = .40\n  ) +\n  theme(\n    panel.background = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    legend.position = \"top\"\n  ) +\n  xlab(\"Normalized Overlap Weight\")\n\n\n\n\n\n\n\n\n\nIn aggregate, the groups will have equal weight. The distribution shift has to do with the sampled treatment distribution. Only 38.8% of patients have treatment A so at an individual level each patient will contribute more on average.\n\nCumulative patient contribution\nWe can also look at the accumulation of patients (i.e., when each patient contributes equally) as a function of the cumulative overlap weight in each group. This allows us to understand how much (or little) the treatment effect estimation will be dominated by smaller concentrations of patients.\n\n\nCode\npopulation |&gt;\n  \n  # Send overlap weight down the rows\n  pivot_longer(\n    cols = c(OW_norm, OW_hat_norm),\n    names_to = \"Type\",\n    values_to = \"Weight\"\n  ) |&gt;\n  \n  # Clean labels\n  mutate(\n    Treatment = \n      case_when(\n        A == 1 ~ \"A\",\n        TRUE ~ \"B\"\n      ),\n    Type = \n      case_when(\n        Type == \"OW_norm\" ~ \"True\",\n        TRUE ~ \"Estimated\"\n      ),\n    NullWeight = 1\n  ) |&gt; \n  \n  # Rearrange\n  arrange(Treatment, Type, Weight) |&gt;\n  \n  # Compute the cumulative weight\n  mutate(\n    Weight = cumsum(Weight) / sum(Weight),\n    NullWeight = cumsum(NullWeight) / sum(NullWeight),\n    .by = c(Treatment, Type)\n  ) |&gt; \n  \n  # Make a plot\n  ggplot() + \n  geom_line(\n    aes(\n      x = Weight,\n      y = NullWeight,\n      color = Treatment,\n      linetype = Type\n    ),\n    linewidth = 1\n  ) +\n  scale_x_continuous(labels = scales::percent) +\n  scale_y_continuous(labels = scales::percent) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray\"),\n    panel.grid.major.y = element_line(color = \"gray\"),\n    legend.position = \"top\"\n  ) +\n  xlab(\"Cumulative percent of overlap weight (%)\") +\n  ylab(\"Cumulative percent of patients (%)\") \n\n\n\n\n\n\n\n\n\nWe can see that for treatment B, about 40% of the outcome model weight will be accounted for by only 25% of patients. The weights for treatment A are slightly more evenly spread across the patients.\n\n\nWeighted-mean differences\nIt was previously mentioned that the overlap weight methodology leads to perfect balance among the covariates used in the propensity score model. We can verify by looking at the group means for each model factor before and after weighting.\n\n\nCode\npopulation |&gt; \n  \n  # Add uniform weights\n  add_column(NullWeight = 1) |&gt;\n  \n  # Send weights down the rows\n  pivot_longer(\n    cols = c(NullWeight, OW_hat_norm),\n    names_to = \"Type\",\n    values_to = \"Weight\"\n  ) |&gt; \n  \n  # Send factors down the rows\n  pivot_longer(\n    cols = c(age, income, male),\n    names_to = \"Factor\",\n    values_to = \"Value\"\n  ) |&gt; \n  \n  # For each \n  summarize(\n    Patients = n(),\n    Mean = sum(Weight * Value) / sum(Weight),\n    .by = c(Factor, Type, A)\n  ) |&gt;\n  \n  # Clean up\n  transmute(\n    Factor = \n      case_when(\n        Factor == \"age\" ~ \"Age (years)\",\n        Factor == \"income\" ~ \"Income ($)\",\n        Factor == \"male\" ~ \"Male (%)\"\n      ),\n    Treatment = \n      case_when(\n        A == 1 ~ \"A\",\n        TRUE ~ \"B\"\n      ),\n    Type,\n    Patients,\n    Mean = \n      case_when(\n        Factor == \"Male (%)\" ~ 100 * Mean,\n        TRUE ~ Mean\n      )\n  ) |&gt;\n  \n  # Send over the columns\n  pivot_wider(\n    names_from = Type,\n    values_from = c(Patients, Mean)\n  ) |&gt; \n  \n  # Rearrange\n  arrange(Factor, Treatment) |&gt;\n  select(Factor, Treatment, ends_with(\"NullWeight\"), Mean_OW_hat_norm) |&gt; \n  \n  # Make a table\n  reactable(\n    groupBy = \"Factor\",\n    columns = \n      list(\n        Patients_NullWeight = colDef(name = \"Patients\", aggregate = \"sum\", align = \"center\"),\n        Mean_NullWeight = colDef(name = \"Before Weighting\", aggregate = zildge::rectbl_agg_wtd(\"Patients_NullWeight\"), align = \"center\", format = colFormat(digits = 1)),\n        Mean_OW_hat_norm = colDef(name = \"After Weighting\", aggregate = zildge::rectbl_agg_wtd(\"Patients_NullWeight\"), align = \"center\", format = colFormat(digits = 1))\n      ),\n    columnGroups = \n      list(\n        colGroup(\n          name = \"Mean Value\",\n          columns = c(\"Mean_NullWeight\", \"Mean_OW_hat_norm\")\n        )\n      ),\n    striped = TRUE,\n    highlight = TRUE,\n    bordered = TRUE,\n    resizable = TRUE,\n    theme = reactablefmtr::sandstone()\n  ) |&gt;\n  reactablefmtr::add_source(\"Use arrows to expand table\", font_size = 12, font_style = \"italic\")\n\n\n\nUse arrows to expand table\n\n\n\nWe can see that, overall, the weighted sample is slightly older and female with lower income.\n\n\nConfounder weight shifts\nSimilar to looking at weight attributions as a whole, we can explore weight shifts within subgroups of patient characteristics. This helps build intuition about which patients the subsequent treatment effect estimates will focus on most (and least).\n\n\nCode\npopulation |&gt; \n  \n  # Add uniform weights\n  add_column(NullWeight = 1) |&gt;\n  \n  # Convert to quntiles\n  mutate(\n    across(\n      c(age, income),\n      \\(x) Hmisc::cut2(x, g = 10)\n    ),\n    Age = age,\n    Income = income,\n    Sex = \n      case_when(\n        male == 1 ~ \"Male\",\n        TRUE ~ \"Female\"\n      ),\n    Treatment = \n      case_when(\n        A == 1 ~ \"A\",\n        TRUE ~ \"B\"\n      )\n  ) |&gt; \n  \n  # Send weights down the rows\n  pivot_longer(\n    cols = c(NullWeight, OW_hat_norm),\n    names_to = \"Type\",\n    values_to = \"Weight\"\n  ) |&gt;\n  \n  # Send factors down the rows\n  pivot_longer(\n    cols = c(Age, Income, Sex),\n    names_to = \"Factor\",\n    values_to = \"Level\"\n  ) |&gt; \n  \n  # Compute total weights\n  summarize(\n    Weight = sum(Weight),\n    .by = c(Factor, Level, Type, Treatment)\n  ) |&gt;\n  \n  # Find percent of weight\n  mutate(\n    Weight = Weight / sum(Weight),\n    .by = c(Factor, Type, Treatment)\n  ) |&gt;\n  \n  # Send over columns\n  pivot_wider(\n    names_from = Type,\n    values_from = Weight\n  ) |&gt;\n  \n  # Clean up\n  mutate(\n    Level = factor(Level) |&gt; fct_rev(),\n    Factor = \n      case_when(\n        Factor == \"Age\" ~ \"Age (years)\",\n        Factor == \"Income\" ~ \"Income ($)\",\n        TRUE ~ Factor\n      )\n  ) |&gt;\n  \n  # Make a plot\n  ggplot() +\n  geom_col(\n    aes(\n      x = Level,\n      y = NullWeight,\n      fill = Treatment\n    ),\n    color = \"black\",\n    linewidth = .75,\n    alpha = .40,\n    position = \"dodge\"\n  ) + \n  geom_point(\n    aes(\n      x = Level,\n      y = OW_hat_norm,\n      color = Treatment,\n      shape = Treatment\n    ),\n    position = position_dodge(width = 1),\n    size = 3\n  ) +\n  geom_line(\n    aes(\n      x = Level,\n      y = OW_hat_norm,\n      color = Treatment,\n      group = Treatment\n    ),\n    position = position_dodge(width = 1),\n    linewidth = .5\n  ) +\n  facet_wrap(~Factor, scales = \"free\") +\n  coord_flip() +\n  scale_y_continuous(labels = scales::percent) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray\"),\n    legend.position = \"top\"\n  ) +\n  xlab(\"Level\") +\n  ylab(\"Share of weight within group (%)\") +\n  labs(\n    fill = \"Before Weighting\",\n    shape = \"After Weighting\",\n    color = \"After Weighting\"\n  )\n\n\n\n\n\n\n\n\n\nWe can see the balancing take place. After overlap weighting, the contribution to treatment effect estimation is reduced in older patients with lower income for treatment A, and vice-versa for treatment B, with some levels in the middle having increased weight for both treatments. This is where the most overlap is between the groups.\nIt is also useful to look at these plots for factors that were not included in the propensity score model. We may find drastic weight shifts which might indicate significant co-variation among factors already accounted for."
  },
  {
    "objectID": "post/the-overlap-weight/index.html#truehazardratio",
    "href": "post/the-overlap-weight/index.html#truehazardratio",
    "title": "The overlap weight in survival analysis",
    "section": "A look at the true hazard ratio",
    "text": "A look at the true hazard ratio\nThe treatment effect will be quantified by a hazard ratio, which is an estimate of the ratio of instantaneous event rates between the treatment groups (this is what is done in the original simulation). This measure is different than the effect we interpreted from the true data-generating process, so, although we won’t expect them to provide the same numerical result, we should still get a similar conclusion, in that treatment A is superior to treatment B.\nTo start, we can compute the actual hazard ratio had we been able to observe each potential outcome (i.e., the counterfactual). To do this, we need to transform our data such that we have two rows of data per patient–one for each treatment. Then, we fit the Cox model, using the true overlap weights as case weights.\n\n\nCode\n# Fit the model\ntrue_hr_mod &lt;-\n  coxph(\n    formula = Surv(time, status) ~ factor(A),\n    data = \n      population |&gt;\n      \n      # Keep columns needed\n      select(\n        OW_norm, # True (normalized) overlap weight (could use un-normalized version here)\n        t_A, # True event time under treatment A\n        t_B, # True event time under treatment B\n        censor_time # When the patient was censored\n      ) |&gt;\n      \n      # Send true event times down the rows\n      pivot_longer(\n        cols = c(t_A, t_B),\n        names_to = \"A\",\n        values_to = \"time\"\n      ) |&gt;\n      \n      # Check if censored first\n      mutate(\n        A = case_when(A == \"t_A\" ~ 1, TRUE ~ 0), # Replace with our notation\n        time = pmin(time, censor_time),\n        status = as.numeric(time &lt; censor_time)\n      ),\n    weights = OW_norm\n  )\n\n# Extract the result\ntrue_hr &lt;- exp(true_hr_mod$coefficients)[[1]]\ntrue_hr\n\n\n[1] 0.7284168\n\n\nIf we could compare the worlds in which we observe all patients under each treatment, the hazard ratio is 0.73, suggesting that the instantaneous event rate is 27% lower with treatment A than with treatment B. This aligns with what we’ve established as the true effect. Our hope is that the estimate from the observable data (in the next section) will contain this value within some margin of sampling error."
  },
  {
    "objectID": "post/the-overlap-weight/index.html#estimatedhazardratio",
    "href": "post/the-overlap-weight/index.html#estimatedhazardratio",
    "title": "The overlap weight in survival analysis",
    "section": "Our estimate of the treatment effect",
    "text": "Our estimate of the treatment effect\nFinally, we can obtain our estimate of the treatment effect using only the observable quantities we have in our data set, which is what we would have in a real-life analysis. Similar to the previous section, we are just fitting a Cox model with the treatment as a covariate. Except this time we only observe one outcome per patient as a result of the treatment they actually received, and use the estimated overlap weight to balance the confounding factors (age, sex, and income). We will also use robust standard error estimates.\n\n\nCode\n# Fit the model\nestimated_hr_mod &lt;-\n  coxph(\n    formula = Surv(time, status) ~ factor(A),\n    data = population,\n    weights = OW_hat_norm,\n    robust = TRUE\n  )\n\n# Extract the result\nestimated_hr &lt;- exp(estimated_hr_mod$coefficients)[[1]]\nestimated_hr\n\n\n[1] 0.7423806\n\n\nCode\n# Extract the confidence interval\nestimated_hr_ci &lt;- exp(confint(estimated_hr_mod))\nestimated_hr_ci\n\n\n               2.5 %    97.5 %\nfactor(A)1 0.6756889 0.8156548\n\n\nOur estimate for the hazard ratio is 0.74, with a 95% confidence interval ranging from 0.68 to 0.82, suggesting that the instantaneous event rate for treatment A is between 18% and 32% lower compared to treatment B (note that this captures the true hazard ratio). Under the assumption that we’ve correctly specified all confounding factors (which we did), we can interpret this as the causal treatment effect, and, assuming the magnitude of improvement is of practical importance (when considering things like cost, resources, clinical outcomes, etc.), we can reliably conclude that there is a benefit to treatment A over treatment B, on average."
  },
  {
    "objectID": "post/com-poisson-regression/index.html",
    "href": "post/com-poisson-regression/index.html",
    "title": "Exploring COM Poisson regression",
    "section": "",
    "text": "A few years ago I encountered an interesting count distribution during a modeling project. The goal was to model the number of suture anchors used in rotator-cuff tendon tears, and how that was influenced by tear characteristics and surgeon preference. My instinct was to fit a Poisson model, but the target took on relatively small values and was also non-zero, so I had to do some digging for an alternative approach. I came across a method called Conway-Maxwell (COM) Poisson regression, which not only allowed for overdispersion (i.e., the population variance is greater than its mean), but also underdispersion. It hit me that I had never come across methodology for the latter and it seemed to align with my problem, so I was intrigued (Full Disclosure: I heard of COM-Poisson prior to that, albeit knew nothing about it, when one of my super smart graduate school buddies was doing research on it, so that’s also why it stuck).\nLong story short, we didn’t end up using the COM-Poisson model for the anchor project 😁, and instead went in favor of Zero-Truncated Poisson regression. Nevertheless I thought it was an awesome method that warranted further exploration and later did a talk on it. That was a few years ago–so this article is intended to be a reworking of that talk to relearn it for myself, but also to get the word out about this awesome method! All code is written in R."
  },
  {
    "objectID": "post/com-poisson-regression/index.html#poissondistribution",
    "href": "post/com-poisson-regression/index.html#poissondistribution",
    "title": "Exploring COM Poisson regression",
    "section": "The Poisson Distribution",
    "text": "The Poisson Distribution\nIf the probability mass function (PMF) for a non-negative, integer-valued \\(Y\\) is:\n\\[P(Y = y|\\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!}\\] then \\(Y\\) is distributed as a Poisson random variable, and has the following property:\n\\[E[Y] = \\lambda\\] It turns out that the maximum likelihood estimator (MLE) for \\(\\lambda\\) is the sample average. Remembering this, we provide the estimate for the first question to be 30 patients. (rounding up)\n\n\nCode\nlambda_hat &lt;- mean(with(ed_volumes, tapply(Volume, Date, sum)))\nlambda_hat\n\n\n[1] 29.53699\n\n\nWe also know that once we have an estimate for \\(\\lambda\\) that we can compute probabilities by plugging it into the PMF:\n\\[P(Y&gt;40) = 1 - P(Y\\leq40) = 1 - \\sum_{y=0}^{40} \\frac{e^{-\\lambda}\\lambda^y}{y!}\\]\n\n\nCode\ny &lt;- 0:40\np &lt;- exp(-lambda_hat) * lambda_hat^y / factorial(y)\nquestion2 &lt;- 1 - sum(p)\nquestion2\n\n\n[1] 0.02633629\n\n\nCode\n# Alternative approach\n1- ppois(40, lambda_hat)\n\n\n[1] 0.02633629\n\n\nWe estimate that there is a 2.6% chance that we will see more than 40 patients in the ED in a single day–done with the first two questions!\n😨\nUnfortunately, we missed a crucial assumption about the Poisson distribution that we didn’t account for:\n\\[E[Y] = Var[Y]\\] The estimates we provided assumed that the mean and variance of our underlying distribution were equal. The sample variance was 53.4 which is considerably larger than the mean of 29.5. Here is what an actual Poisson distribution looks like with \\(\\lambda\\) = 29.5 (our sample average) in comparison with the observed data:\n\n\nCode\ned_volumes %&gt;%\n  \n  # Compute the total for each date\n  group_by(Date) %&gt;%\n  summarise(\n    Volume = sum(Volume),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  # Count the occurrences of each daily volume\n  group_by(Volume) %&gt;%\n  summarise(\n    N = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  # Compute the observed and theoretical relative frequencies\n  mutate(\n    Observed = N / sum(N),\n    Theoretical = dpois(Volume, lambda = lambda_hat)\n  ) %&gt;%\n  \n  # Remove raw count\n  select(-N) %&gt;% \n  \n  # Make a plot\n  ggplot(\n    aes(\n      x = Volume\n    )\n  ) +\n  geom_col(\n    aes(\n      y = Observed,\n      fill = \"Observed Data\"\n    ),\n    color = \"black\"\n  ) +\n  geom_area(\n    aes(\n      y = Theoretical,\n      fill = \"Actual Poisson\"\n    ),\n    alpha = .25,\n    color = \"black\"\n  ) +\n  geom_vline(\n    aes(\n      xintercept = lambda_hat\n    ),\n    color = \"#b84f48\"\n  ) +\n  xlab(\"Daily Patient Volume\") +\n  ylab(\"Relative Frequency (%)\") +\n  theme(\n    legend.title = element_blank(),\n    legend.text = element_text(size = 12),\n    legend.position = \"top\",\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    strip.text = element_text(size = 12),\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\")\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"blue\", \"gray\"))\n\n\n\n\n\n\n\n\n\nThe tails of the observed sample are heavier than a Poisson would be with the same mean. Therefore, the estimates we provided for questions 1 and 2 are probably not very accurate. Let’s take a different approach: answer question 3 first, and then work our way back to the other two.\nAs a reminder, we want to assess whether there is more variation in patient volume during the week versus weekend, or in the AM versus PM hours. To get this comparison, we need a way to simultaneously estimate the effect of each of these factors on the patient volume: enter Poisson regression."
  },
  {
    "objectID": "post/com-poisson-regression/index.html#modelformulation",
    "href": "post/com-poisson-regression/index.html#modelformulation",
    "title": "Exploring COM Poisson regression",
    "section": "Model Formulation",
    "text": "Model Formulation\nJust like other generalized linear models (GLM), a Poisson regression model estimates the population average (the same average as described above!), but conditioned on a set of covariates.\nMore formally, for a given set of covariates \\(x_1, x_2, .., x_p\\), we assume the following functional form:\n\\[log(\\lambda_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2}+...+\\beta_px_{ip} = X_i\\beta\\] This log-linear model allows us to estimate the mean number of events (i.e., the Poisson parameter \\(\\lambda\\)) for a given arrangement of covariate values. We also get informative interpretation of the individual effects of each covariate. For our example, the model looks like this:\n\\[log(\\lambda_i) = \\beta_0 + \\beta_{weekend}x_{i_{weekend}} + \\beta_{PM}x_{i_{PM}}\\] where \\(\\lambda_i\\) is the expected (half-day) patient volume, \\(x_{i_{weekend}} = 1\\) if it is Saturday or Sunday, and \\(x_{i_{PM}} = 1\\) if it is PM hours (and they are 0 otherwise). We can take the following steps to estimate the \\(\\beta\\) parameters:\n\nPlug the regression formula into the Poisson PMF\n\n\\[\n\\begin{equation}\n\\begin{split}\nP(Y_i = y_i|\\lambda_i) & = \\frac{e^{-\\lambda_i}\\lambda_i^{y_i}}{y_i!} \\\\\n& = \\frac{e^{-e^{X_i\\beta}}{(e^{X_i\\beta})}^{y_i}}{y_i!} \\\\\n\\end{split}\n\\end{equation}\n\\]\n\nDerive the likelihood function\n\n\\[L(\\lambda_i) = \\prod_{i=1}^N \\frac{e^{-e^{X_i\\beta}}{(e^{X_i\\beta})}^{y_i}}{y_i!} \\\\\\] where \\(N\\) is the total number of (half) days.\n\nCompute the MLE’s\n\n\\[\\hat{\\beta} = max_{\\beta} L(\\lambda_i)\\\\\\] \\(\\hat{\\beta}\\) is the set of estimated parameter values computed from maximizing the likelihood function. Once we have these, we can plug them into the regression formula, and away we go! Luckily, our software will compute these for us–all we need to do is supply the data.\n\n\nCode\n# Fit a generalized linear model\nmodel &lt;-\n  glm(\n    formula = Volume ~ Weekend + PM, # Specify the model form\n    data = # Supply the data\n      ed_volumes %&gt;% \n      mutate(\n        Weekend = weekdays(Date) %in% c(\"Saturday\", \"Sunday\"),\n        PM = TimeOfDay == \"PM\"\n      ),\n    family = \"poisson\" # Indicate the likelihood\n  )\n\n# Show the model summary\nsummary(model)\n\n\n\nCall:\nglm(formula = Volume ~ Weekend + PM, family = \"poisson\", data = ed_volumes %&gt;% \n    mutate(Weekend = weekdays(Date) %in% c(\"Saturday\", \"Sunday\"), \n        PM = TimeOfDay == \"PM\"))\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.78530    0.01455 191.483   &lt;2e-16 ***\nWeekendTRUE -0.41433    0.02371 -17.474   &lt;2e-16 ***\nPMTRUE       0.01762    0.01926   0.915     0.36    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1054.15  on 729  degrees of freedom\nResidual deviance:  723.55  on 727  degrees of freedom\nAIC: 4007.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe estimated model turns out to be:\n\\[log(\\hat{\\lambda_i}) = 2.7853 - 0.4143x_{i_{weekend}} + 0.0176x_{i_{PM}}\\] where \\(\\hat{\\lambda_i}\\) is the estimated average number of patients showing up to the ED on a given day of the week and time of day. Let’s take a look at the four possible estimated means:\n\n\nCode\n# Make grid of possible values for each input\nlist(\n  Weekend = c(TRUE, FALSE),\n  PM = c(TRUE, FALSE)\n) %&gt;%\n  \n  # Find cross-product\n  cross_df() %&gt;%\n  \n  # Add some columns\n  mutate(\n    \n    # Add predictions\n    EstimatedVolume =\n      predict(\n        model, # Supply the model\n        newdata = data.frame(Weekend, PM),\n        type = \"response\" # Applies the inverse link to the linear predictor\n      ),\n    EstimatedVolume = round(EstimatedVolume, 2),\n    \n    # Clean up names\n    Weekend = \n      case_when(\n        Weekend ~ \"Sat, Sun\",\n        TRUE ~ \"Mon - Fri\"\n      ) %&gt;%\n      factor() %&gt;%\n      fct_relevel(\"Mon - Fri\"),\n    PM = \n      case_when(\n        PM ~ \"PM\",\n        TRUE ~ \"AM\"\n      ) %&gt;%\n      factor() %&gt;%\n      fct_relevel(\n        \"AM\"\n      )\n  ) %&gt;%\n  \n  # Rearrange\n  arrange(\n    Weekend,\n    PM\n  ) %&gt;%\n  \n  # Send over the columns\n  pivot_wider(\n    names_from = PM,\n    values_from = EstimatedVolume\n  ) %&gt;%\n  \n  # Rename column\n  rename(\n    `Day of week` = Weekend\n  ) %&gt;%\n  \n  # Make a kable\n  knitr::kable(\n    format = \"html\",\n    caption = \"Expected ED patient volume\"\n  ) %&gt;%\n  kableExtra::kable_styling(\n    full_width = FALSE,\n    bootstrap_options = c(\"striped\", \"responsive\")\n  ) %&gt;%\n  kableExtra::add_header_above(c(\"\", \"Time of day\" = 2))\n\n\nWarning: `cross_df()` was deprecated in purrr 1.0.0.\nℹ Please use `tidyr::expand_grid()` instead.\nℹ See &lt;https://github.com/tidyverse/purrr/issues/768&gt;.\n\n\n\n\nExpected ED patient volume\n\n\n\n\n\n\n\n\n\nTime of day\n\n\n\nDay of week\nAM\nPM\n\n\n\n\nMon - Fri\n16.20\n16.49\n\n\nSat, Sun\n10.71\n10.90\n\n\n\n\n\n\n\n\nOur model suggests that we see \\(1 - e^{\\beta_{weekend}} \\approx\\) 33.9% less patient volume on the weekends compared to during the week, regardless if it is AM or PM hours–this is reflected in our table estimates. We can also see that there is little difference in patient volume by the time of day, regardless of the day of the week.\nAnswer to Question 3\nThe evidence tells us that not only is the day of the week the more important factor, but the time of day does not appear to matter at all. This is clear when we stratify our observed sample distribution:\n\n\nCode\ned_volumes %&gt;% \n  \n  # Make the indicators\n  mutate(\n    Weekend = \n      case_when(\n        weekdays(Date) %in% c(\"Saturday\", \"Sunday\") ~ \"Sat, Sun\",\n        TRUE ~ \"Mon - Fri\"\n      ) %&gt;%\n      factor() %&gt;%\n      fct_relevel(\"Mon - Fri\"),\n    TimeOfDay = \n      TimeOfDay %&gt;%\n      factor() %&gt;%\n      fct_relevel(\n        \"AM\"\n      )\n  ) %&gt;%\n  \n  # Make a plot\n  ggplot() +\n  geom_histogram(\n    aes(\n      x = Volume,\n      fill = Weekend\n    ),\n    color = \"black\",\n    bins = 20,\n    position = \"identity\",\n    alpha = .5\n  ) + \n  facet_wrap(\n    ~TimeOfDay,\n    nrow = 2\n  ) +\n  xlab(\"Half-Day Patient Volume\") +\n  ylab(\"Frequency\") +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    strip.text = element_text(size = 12),\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\")\n  )\n\n\n\n\n\n\n\n\n\nAnswers to Questions 1 & 2\nSo back to the first two questions. How can we use what we found for question 3 to inform us of these answers? Remember, we need to provide estimates regarding the total daily patient volume even though our model target was half-day patient volume. We have a couple options:\n\nSince we have established that the time of day does not impact patient volume, we could fit a new model by first aggregating the target to reflect the total daily patient volume and then only add the weekday versus weekend factor as a covariate.\n\n\n\nCode\n# Fit a generalized linear model\nmodel2 &lt;-\n  glm(\n    formula = Volume ~ Weekend, # Specify the model form\n    data = \n      ed_volumes %&gt;% \n      \n      # For each date\n      group_by(Date) %&gt;%\n      \n      # Compute the total daily patient volume\n      summarise(\n        Volume = sum(Volume),\n        .groups = \"drop\"\n      ) %&gt;%\n      \n      # Make the indicator\n      mutate(\n        Weekend = weekdays(Date) %in% c(\"Saturday\", \"Sunday\")\n      ),\n    family = \"poisson\" # Indicate the likelihood\n  )\n\n# Show the model summary\nsummary(model2)\n\n\n\nCall:\nglm(formula = Volume ~ Weekend, family = \"poisson\", data = ed_volumes %&gt;% \n    group_by(Date) %&gt;% summarise(Volume = sum(Volume), .groups = \"drop\") %&gt;% \n    mutate(Weekend = weekdays(Date) %in% c(\"Saturday\", \"Sunday\")))\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.48729    0.01082  322.15   &lt;2e-16 ***\nWeekendTRUE -0.41433    0.02371  -17.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 681.97  on 364  degrees of freedom\nResidual deviance: 352.21  on 363  degrees of freedom\nAIC: 2252.7\n\nNumber of Fisher Scoring iterations: 4\n\n\nNotice the relative effect of the day of week factor remained consistent from the original model–the intercept just changed to reflect the shifted distribution for the total patient volume. This approach works, but lets use the model we originally developed…\n\nThis option requires us to know a property of Poisson random variables. Specifically, if X and Y are two independent Poisson random variables and \\(Z = X + Y\\), then\n\n\\[Z \\sim Poisson(\\lambda_X + \\lambda_Y)\\] where \\(\\lambda_X\\) and \\(\\lambda_Y\\) are the expected values (means) for X and Y, respectively. This is relevant to us because we need to aggregate our model estimates over the time of day in order to get estimates for the total daily volume. Specifically, if \\(V\\) is the patient volume, then \\(V_{Daily} = V_{AM} + V_{PM}\\) for a particular day of the week. We assume from our model specification above that \\(V_{AM}\\) and \\(V_{PM}\\) follow independent Poisson distributions. Thus, we get the following:\n\\[\n\\begin{equation}\n\\begin{split}\n\\lambda_{Daily} & = E[V_{Daily}] \\\\\n& = E[V_{AM} + V_{PM}] \\\\\n& = E[V_{AM}] + E[V_{PM}] \\\\\n& = \\lambda_{AM} + \\lambda_{PM} \\\\\n& = e^{\\beta_0 + \\beta_{weekend}x_{i_{weekend}}} + e^{\\beta_0 + \\beta_{weekend}x_{i_{weekend}} + \\beta_{PM}} \\\\\n& = e^{\\beta_0 + \\beta_{weekend}x_{i_{weekend}}} (1 + e^{\\beta_{PM}})\\\\\n& =\n\\begin{cases}\ne^{\\beta_0} (1 + e^{\\beta_{PM}}) & \\text{if M-F} \\\\\ne^{\\beta_0 + \\beta_{weekend}} (1 + e^{\\beta_{PM}}) & \\text{if Sat, Sun} \\\\\n\\end{cases} \\\\\n& \\approx\n\\begin{cases}\n32.70 & \\text{if M-F} \\\\\n21.61         & \\text{if Sat, Sun} \\\\\n\\end{cases}\n\\end{split}\n\\end{equation}\n\\] Simply put, to get estimates around the total daily volume, we summed the regression equation over the time of day variable. Note that this is exactly what we would have gotten by just taking the sample mean of total daily volume stratified by each day of week grouping:\n\n\nCode\nstratified_means &lt;-\n  ed_volumes %&gt;% \n  \n  # For each date\n  group_by(Date) %&gt;%\n  \n  # Compute the total daily patient volume\n  summarise(\n    Volume = sum(Volume),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  # Make the indicator\n  mutate(\n    Weekend = \n      case_when(\n        weekdays(Date) %in% c(\"Saturday\", \"Sunday\") ~ \"Sat, Sun\",\n        TRUE ~ \"Mon - Fri\"\n      ) %&gt;%\n      factor() %&gt;%\n      fct_relevel(\"Mon - Fri\")\n  ) %&gt;%\n  \n  # For each group\n  group_by(Weekend) %&gt;%\n  \n  # Compute the mean\n  summarise(\n    Mean = mean(Volume),\n    Variance = var(Volume),\n    .groups = \"drop\"\n  )\n\nstratified_means %&gt;%\n  \n  # Make a kable\n  knitr::kable(\n    format = \"html\",\n    caption = \"Stratified Daily Sample Means and Variances\"\n  ) %&gt;%\n  kableExtra::kable_styling(\n    full_width = FALSE,\n    bootstrap_options = c(\"striped\", \"responsive\")\n  ) \n\n\n\n\nStratified Daily Sample Means and Variances\n\n\nWeekend\nMean\nVariance\n\n\n\n\nMon - Fri\n32.69732\n31.93495\n\n\nSat, Sun\n21.60577\n19.40618\n\n\n\n\n\n\n\n\nNotice also that the stratified sample variances align much more with our assumptions in using a Poisson distribution–this was one of our problems earlier, but it appears to be resolved! Now that we have weekday and weekend-specific Poisson distributions, we can confidently compute our final estimates for questions 1 and 2 the same way we did above.\nTable 2 already shows the answer to question 1: we can expect to see 33 patients per day Monday-Friday, and 22 patients on Saturdays or Sundays. To get the updated estimates for question 2, we just need to plug in the new means to the PDF and compute the cumulative probability:\n\n\nCode\nquestion2_updated &lt;-\n  stratified_means %&gt;%\n  \n  # For each set, compute the cumulative probability\n  cheese::stratiply(\n    by = Weekend,\n    f = ~1 - sum(exp(-.x$Mean) * .x$Mean^c(0:40) / factorial(c(0:40)))\n  )\nquestion2_updated\n\n\n$`Mon - Fri`\n[1] 0.08959949\n\n$`Sat, Sun`\n[1] 0.0001298706\n\n\nWe estimate that there is a 8.96% chance that we will see more than 40 patients on any given Monday-Friday, and a 0.01% chance that we will see that many patients on the weekend.\nOkay we got a little carried away with that but hopefully you have an idea of how Poisson regression works."
  },
  {
    "objectID": "post/com-poisson-regression/index.html#compoissondistribution",
    "href": "post/com-poisson-regression/index.html#compoissondistribution",
    "title": "Exploring COM Poisson regression",
    "section": "The COM-Poisson Distribution",
    "text": "The COM-Poisson Distribution\nIf the PMF for a non-negative, integer-valued \\(Y\\) is:\n\\[P(Y = y|\\lambda, \\nu) = \\frac{\\lambda^y}{y!^{\\nu}Z(\\lambda, \\nu)}\\] where \\(\\lambda, \\nu &gt; 0\\), and\n\\[Z(\\lambda, \\nu) = \\sum_{k=0}^{\\infty}\\frac{\\lambda^k}{k!^{\\nu}}\\] then \\(Y\\) is distributed as a COM-Poisson random variable.\nThat’s kind of a mouthful, but we can see some similarities in its form to the Poisson PDF. In fact, if you’re really math-savvy (which I’m not, I had to look this up), you’ll notice that the function \\(Z\\) is a power series. When we set \\(\\nu = 1\\), then\n\\[Z(\\lambda, \\nu = 1) = \\sum_{k=0}^{\\infty}\\frac{\\lambda^k}{k!} = e^\\lambda\\] Plugging that result back into the full PMF, it turns out to be exactly the same as a Poisson distribution. This tells us that the Poisson distribution is a special case of the COM-Poisson. In the other words, the COM-Poisson is a generalization of the Poisson distribution, so we’ll expect it do everything that the latter can–plus more.\n\nDispersion Parameter\nAn important feature of the COM-Poisson PDF is the parameter \\(\\nu\\), which determines its dispersion (i.e., how variable it is relative to its mean). In the standard Poisson distribution, the mean and variance are always the same, by definition. The \\(\\nu\\) parameter allows us to relax that restriction in cases where it does not apply (or we don’t want to force it to). The following table describes the properties when it is above, below, and equal to 1:\n\n\n\nParameter\nImplies\nDescribed As\n\n\n\n\n\\(\\nu=1\\)\n\\(E[Y] = Var[Y]\\)\nEquidispersed\n\n\n\\(\\nu&lt;1\\)\n\\(E[Y] &lt; Var[Y]\\)\nOverdispersed\n\n\n\\(\\nu&gt;1\\)\n\\(E[Y] &gt; Var[Y]\\)\nUnderdispersed\n\n\n\n\n\nMean and Variance\nUnfortunately, there is no closed form solution for the mean and variance of a COM-Poisson random variable. However, there are a couple related properties that are worth relaying:\n\nThe \\(\\lambda\\) parameter is the mean of the power-transformed counts\n\n\\[E[Y^\\nu] = \\lambda\\]\n\nThe mean and variance can be approximated as:\n\n\\[E[Y] \\approx \\lambda^{1/\\nu} - \\frac{\\nu-1}{2\\nu}\\] \\[Var[Y] \\approx \\frac{\\lambda^{1/\\nu}}{\\nu}\\] when \\(\\nu \\leq 1\\) (overdispersed) or \\(\\lambda &gt; 10^\\nu\\) (larger counts).\n\n\nCentrality Simulation\nThe properties described above give us some useful information, but it is hard to make sense of how these parameters directly govern the data we might observe from a COM-Poisson distribution. To give us a bit more insight, lets randomly generate a bunch of data for different parameter combinations and see what we find. To do this, we’ll use the rcmp function.\n\n\nCode\n# Make a parameter grid\nparams &lt;-\n  list(\n    lambda = c(1, 3, 5, 10, 25, 50),\n    nu = c(.5, 1, 1.25, 1.5)\n  ) %&gt;%\n  cross_df()\n\n# Number of samples\nn &lt;- 500\n\n# Number of simulations\ns &lt;- 100\n\n# Set a seed\nset.seed(123)\n\ncompoisson_sim1 &lt;-\n  params %&gt;%\n  \n  # Split into a list\n  split(1:nrow(.)) %&gt;%\n  \n  # For each element\n  map_df(\n    function(.x) {\n      \n      1:s %&gt;%\n        \n        # For each iteration\n        map_df(\n          function(.sim) {\n            \n            # Time the sampling\n            temp_time &lt;- system.time(temp_value &lt;- rcmp(n = n, lambda = .x$lambda, nu = .x$nu))\n            \n            tibble(\n              s = .sim,\n              lambda = .x$lambda,\n              nu = .x$nu,\n              value = temp_value,\n              time = temp_time[\"elapsed\"][[1]]\n            )\n            \n          }\n        )\n    }\n  )\n\n\nWe took 100 random samples of size 500 from 24 parameter combinations. This process took about 94.1 seconds to run. Lets first take a quick look at the distributions from the first simulation for each parameter set.\n\n\nCode\ncompoisson_sim1 %&gt;%\n  \n  # Filter to the first simulated value\n  filter(\n    s == 1\n  ) %&gt;%\n  \n  # Convert to factors\n  mutate(\n    across(\n      c(\n        lambda,\n        nu\n      ),\n      factor\n    ),\n    lambda = \n      lambda %&gt;%\n      fct_relabel(\n        function(x) paste0(\"lambda == \", x)\n      )\n  ) %&gt;%\n  \n  # Make a plot\n  ggplot() +\n  geom_histogram(\n    aes(\n      x = value,\n      fill = nu\n    ),\n    position = \"identity\",\n    alpha = 0.5\n  ) +\n  facet_wrap(\n    ~lambda,\n    scales = \"free\",\n    label = \"label_parsed\"\n  ) +\n  xlab(\"Observed Value\") +\n  ylab(\"Frequency\") +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    strip.text = element_text(size = 12),\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\")\n  ) +\n  labs(\n    fill = expression(nu)\n  )\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIt looks like the magnitude of counts increases with \\(\\lambda\\), and decreases with \\(\\nu\\). To get a more concrete view of this, lets compute the sample mean and variance for each iteration, and then average those over the simulations for each parameter combination. By doing this, we’ll also get a sense of the sampling variability of these statistics when \\(n=\\) 500.\n\n\nCode\ncompoisson_sim1 %&gt;%\n  \n  # For each group\n  group_by(lambda, nu, s) %&gt;%\n  \n  # Compute the statistics\n  summarise(\n    Mean = mean(value),\n    Variance = var(value),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  # Send the statistics down the rows\n  pivot_longer(\n    cols = c(Mean, Variance)\n  ) %&gt;%\n  \n  # For each group\n  group_by(lambda, nu, name) %&gt;%\n  \n  # Compute the mean/standard error\n  summarise(\n    Mean = mean(value),\n    SE = sd(value),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  # Join to get the average time for each iteration\n  inner_join(\n    y = \n      compoisson_sim1 %&gt;%\n      \n      # Remove the value, get unique rows\n      select(-value) %&gt;%\n      distinct() %&gt;%\n      \n      # For each group\n      group_by(lambda, nu) %&gt;%\n      \n      # Compute the average time per iteration\n      summarise(\n        time = mean(time),\n        .groups = \"drop\"\n      ),\n    by = \n      c(\n        \"lambda\",\n        \"nu\"\n      )\n  ) %&gt;%\n  \n  # Convert to factors\n  mutate(\n    nu = \n      nu %&gt;%\n      factor() %&gt;%\n      fct_relabel(\n        function(x) paste0(\"nu == \", x)\n      )\n  ) %&gt;%\n  \n  # Make a plot\n  ggplot(\n    aes(\n      x = lambda,\n      y = Mean\n    )\n  ) +\n  geom_line(\n    aes(\n      color = name\n    ),\n    size = 1.25\n  ) +\n  geom_point(\n    aes(\n      color = name\n    ),\n    size = 3\n  ) +\n  geom_ribbon(\n    aes(\n      ymin = Mean - 2*SE,\n      ymax = Mean + 2*SE,\n      fill = name\n    ),\n    alpha = .25\n  ) +\n  facet_wrap(\n    ~nu,\n    scales = \"free_y\",\n    label = \"label_parsed\"\n  ) +\n  ylab(\"Value\") +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    strip.text = element_text(size = 12),\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\")\n  ) +\n  scale_x_continuous(\n    breaks = unique(params$lambda)\n  ) +\n  labs(\n    x = expression(lambda)\n  )\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nWhen \\(\\nu=0.5\\) (overdispersed), the counts become large quickly with an increasing \\(\\lambda\\). As the data becomes more underdispersed (\\(\\nu &gt; 1\\)), the variance increases at a slower rate as \\(\\lambda\\) increases, relative to the mean.\nSo how does this translate to a regression model?"
  },
  {
    "objectID": "post/com-poisson-regression/index.html#compoissonregressionmodel",
    "href": "post/com-poisson-regression/index.html#compoissonregressionmodel",
    "title": "Exploring COM Poisson regression",
    "section": "The Regression Model",
    "text": "The Regression Model\nIt turns out that the model structure for COM-Poisson regression is very similar to the standard Poisson model. In fact, we can just copy and paste it from above:\n\\[log(\\lambda_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2}+...+\\beta_px_{ip} = X_i\\beta\\] The basic structure models the \\(\\lambda\\) parameter (on the log scale) as a linear combination of covariates. In turn, the steps for estimating the parameters are also the same:\n\nPlug the regression formula into the COM-Poisson PDF\nDerive the likelihood function\nCompute the maximum likelihood estimates (MLEs)\n\nThe only difference is that the \\(\\lambda\\) parameter in this model no longer represents the mean. So when we generate fitted values or interpret parameter estimates, there has to be a little more care taken as to what those represent.\nLet’s take a quick look at how to fit a model with the glm.cmp function and orient ourselves with its output. We’ll use the ed_volumes dataset from the previous section.\n\n\nCode\ncompoisson_model1 &lt;-\n  \n  # Function to fit a COM-Poisson model\n  glm.cmp(\n    formula.lambda = Volume ~ Weekend + PM, # lamda parameter linear predictor\n    data = \n      ed_volumes %&gt;% \n      mutate(\n        Weekend = weekdays(Date) %in% c(\"Saturday\", \"Sunday\"),\n        PM = TimeOfDay == \"PM\"\n      )\n  )\n\ncompoisson_model1\n\n\nCMP coefficients\n              Estimate     SE  z-value   p-value\nX:(Intercept)   2.8470 0.1531  18.5957 3.481e-77\nX:WeekendTRUE  -0.4230 0.0323 -13.1174 2.619e-39\nX:PMTRUE        0.0180 0.0195   0.9251    0.3549\nS:(Intercept)   0.0217 0.0529   0.4096    0.6821\n--\nTransformed intercept-only parameters\n   Estimate     SE\nnu   1.0219 0.0541\n--\nChi-squared test for equidispersion\nX^2 = 0.1614, df = 1, p-value = 6.8788e-01\n--\nElapsed: 0.39 sec   Sample size: 730   formula interface\nLogLik: -2000.6310   AIC: 4009.2620   BIC: 4027.6341   \nOptimization Method: L-BFGS-B   Converged status: 0\nMessage: CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH\n\n\nWe see some familiar results. Namely, the parameter estimates and p-values are pretty similar to the standard Poisson model above. However, there is additional output–this is where the dispersion comes in. When we fit the model, the function also estimated the value of the \\(\\nu\\) parameter, which in this case was 1.022 (it was expected to be near 1, given that our data was generated from a standard Poisson distribution). The X components make up the regression equation for \\(\\lambda\\), and the S components make up the regression equation for \\(\\nu\\). We just need to evaluate those equations on an observation’s covariate values, and we’ll have its estimated conditional COM-Poisson distribution."
  },
  {
    "objectID": "post/com-poisson-regression/index.html#compoissonlrt",
    "href": "post/com-poisson-regression/index.html#compoissonlrt",
    "title": "Exploring COM Poisson regression",
    "section": "Testing for Violations of Equidispersion",
    "text": "Testing for Violations of Equidispersion\nWe could just look at the estimated \\(\\nu\\) parameter and its standard error in the model output to draw a conclusion about the dispersion. Nevertheless, the package also provides a function called equitest to more formally carry out a likelihood ratio test (LRT) for the following hypotheses:\n\\[H_0: \\nu = 1 \\hskip.25in H_A: \\nu \\neq 1\\]\nIt tests for evidence that the data is not equidispersed, but does not specify its direction. Therefore, if \\(H_0\\) is rejected, it is up to us to garner from the model output whether there is over or under dispersion. Here is the output when performing this test on our model:\n\n\nCode\nequitest(compoisson_model1)\n\n\n$teststat\n[1] 0.1613952\n\n$pvalue\n[1] 0.6878752\n\n$df\n[1] 1\n\n\nAs expected, the p-value of 0.6879 does not indicate that over or under dispersion was detected."
  },
  {
    "objectID": "post/com-poisson-regression/index.html#stratifieddispersion",
    "href": "post/com-poisson-regression/index.html#stratifieddispersion",
    "title": "Exploring COM Poisson regression",
    "section": "Stratified Dispersion",
    "text": "Stratified Dispersion\nOne of the coolest features of this modeling framework (in my opinion) is that not only can you fit a model for over or under dispersed data, but you can allow the dispersion parameter to vary by covariate values. You probably noticed the formula.lambda argument in the glm.cmp function call where we specified the model form for the \\(\\lambda\\) parameter. There is an additional argument called formula.nu where we can analogously specify a linear predictor for the \\(\\nu\\) parameter, which, like \\(\\lambda\\), is on the log scale. This means that, for example, if we suspect that one group has overdispersed data, and another has underdispersed data, we can account for that within a single model. Very cool! Just for kicks, let’s see what this looks like when we allow the \\(\\nu\\) parameter to vary between AM and PM hours from our ed_volumes dataset:\n\n\nCode\ncompoisson_model2 &lt;-\n  glm.cmp(\n    formula.lambda = Volume ~ Weekend, # Linear predictor for log-lambda\n    formula.nu = Volume ~ PM, # Linear predictor for log-nu\n    data = \n      ed_volumes %&gt;% \n      mutate(\n        Weekend = weekdays(Date) %in% c(\"Saturday\", \"Sunday\"),\n        PM = TimeOfDay == \"PM\"\n      )\n  )\ncompoisson_model2\n\n\nCMP coefficients\n              Estimate     SE  z-value   p-value\nX:(Intercept)   2.8562 0.1532  18.6442 1.407e-77\nX:WeekendTRUE  -0.4231 0.0322 -13.1194 2.549e-39\nS:(Intercept)   0.0250 0.0530   0.4708    0.6378\nS:PMTRUE       -0.0064 0.0069  -0.9317    0.3515\n--\nChi-squared test for equidispersion\nX^2 = 1.0119, df = 2, p-value = 6.0295e-01\n--\nElapsed: 0.48 sec   Sample size: 730   formula interface\nLogLik: -2000.6243   AIC: 4009.2487   BIC: 4027.6208   \nOptimization Method: L-BFGS-B   Converged status: 0\nMessage: CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH\n\n\nWe can see that there are now two parameters associated with the S component of the model. We can simply evaluate and exponentiate those terms to get the \\(\\nu\\) estimates for each group:\n\n\nCode\n# Make a simple design matrix\nnu_design &lt;- matrix(c(1,1,0,1), nrow = 2)\n\n# Multiply by the S component of the model\nnu_log_estimates &lt;- nu_design %*% as.matrix(compoisson_model2$gamma)\n\n# Exponentiate\nnu_estimates &lt;- exp(nu_log_estimates)\n\n# Add the names\nrownames(nu_estimates) &lt;- names(compoisson_model2$gamma)\nnu_estimates\n\n\n                  [,1]\nS:(Intercept) 1.025285\nS:PMTRUE      1.018696\n\n\nAgain, both groups’ data were knowingly generated from a standard Poisson distribution here, so we expected the conditional estimates to be near 1 as well. Nevertheless, it illustrates the power and flexibility of this framework."
  },
  {
    "objectID": "post/com-poisson-regression/index.html#comfittedvalues",
    "href": "post/com-poisson-regression/index.html#comfittedvalues",
    "title": "Exploring COM Poisson regression",
    "section": "Generating Fitted Values",
    "text": "Generating Fitted Values\nAs previously mentioned, there is no closed form solution for the mean of a COM-Poisson random variable. So when we generate fitted values from the model’s linear predictor, it is hard to conceptualize what they represent (it is NOT the mean). Since our model does produce estimates for both \\(\\lambda\\) and \\(\\nu\\), the recommendation is to plug these into the inverse cumulative density function (CDF) to obtain quantile-based fitted values. Specifically, we can use the median. The figure below displays the median and \\(25^{th}\\)/\\(75^{th}\\) percentiles from the parameter grid used in the simulation above.\n\n\nCode\nparams %&gt;%\n  \n  # Add the quantiles\n  inner_join(\n    y = \n      tibble(\n        Quantile = c(0.25, 0.50, 0.75)\n      ),\n    by = character()\n  ) %&gt;%\n  \n  # Get the value\n  mutate(\n    Value = qcmp(Quantile, lambda, nu),\n    Linegroup = factor(Quantile == 0.50) %&gt;% fct_rev(),\n    nu = \n      nu %&gt;%\n      factor() %&gt;%\n      fct_relabel(\n        function(x) paste0(\"nu == \", x)\n      )\n  ) %&gt;%\n  \n  # Make a plot\n  ggplot(\n    aes(\n      x = lambda,\n      y = Value,\n      group = Quantile\n    )\n  ) +\n  geom_line(\n    aes(\n      linetype = Linegroup,\n      color = Linegroup\n    ),\n    size = 1.25\n  ) +\n  geom_point(\n    size = 2\n  ) +\n  facet_wrap(\n    ~nu,\n    scales = \"free_y\",\n    label = \"label_parsed\"\n  ) +\n  ylab(\"Median & 25th/75th Percentiles\") +\n  theme(\n    legend.position = \"none\",\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    strip.text = element_text(size = 12),\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\")\n  ) +\n  scale_x_continuous(\n    breaks = unique(params$lambda)\n  ) +\n  labs(\n    x = expression(lambda)\n  ) +\n  scale_color_manual(\n    values = c(\"black\", \"red\")\n  )\n\n\nWarning: Using `by = character()` to perform a cross join was deprecated in dplyr 1.1.0.\nℹ Please use `cross_join()` instead.\n\n\n\n\n\n\n\n\n\nThe median values seem to have a relatively similar behavior to the simulated means.\nSo, to obtain predicted values with our model object (we’ll use the first model we made), we have to:\n\nProduce observation-level estimates of \\(\\lambda\\) and \\(\\nu\\) with the regression equation\n\n\n\nCode\n# Multiply the design matrices by the parameter estimates for each linear predictor, then exponentiate\nlambda_hat &lt;- exp(compoisson_model1$X %*% as.matrix(compoisson_model1$beta))\nnu_hat &lt;- exp(compoisson_model1$S %*% as.matrix(compoisson_model1$gamma))\nhead(data.frame(lambda_hat, nu_hat))\n\n\n  lambda_hat   nu_hat\n1   17.23626 1.021921\n2   17.54978 1.021921\n3   17.23626 1.021921\n4   17.54978 1.021921\n5   17.23626 1.021921\n6   17.54978 1.021921\n\n\n\nPlug the estimates into the inverse CDF at the 50th percentile\n\n\n\nCode\ny_hat &lt;- qcmp(rep(0.5, length(lambda_hat)), lambda = lambda_hat, nu = nu_hat)\ntable(y_hat)\n\n\ny_hat\n 11  16 \n208 522 \n\n\nThere does exist a predict.cmp function that uses the mean approximation by default, so this could be used for fitted values as well if those conditions are satisfied."
  },
  {
    "objectID": "post/com-poisson-regression/index.html#compowersim",
    "href": "post/com-poisson-regression/index.html#compowersim",
    "title": "Exploring COM Poisson regression",
    "section": "Simulation: Power of Equidispersion Test",
    "text": "Simulation: Power of Equidispersion Test\nSo how well does the likelihood ratio test described above detect over or under dispersion in the data-generating process? We can use simulation! To keep it simple, we will base it off of intercept-only regression models (i.e., no covariates), in effect assessing power at varying values of \\(\\lambda\\). Presumably, power may depend on the complexity of the model, so coverage might vary as terms are added. Here are the parameter values we will consider:\n\\[\n\\begin{equation}\n\\begin{split}\n\\lambda & = (1, 3, 5, 10, 25) \\\\\n\\nu & = (0.5, 0.75, 1, 1.25, 1.5, 2) \\\\\nN & = (25, 50, 100, 500) \\\\\n\\end{split}\n\\end{equation}\n\\]\nWith those, we will take the following steps to carry out the simulation for each combination of \\(\\lambda\\), \\(\\nu\\), and \\(N\\):\n\nGenerate \\(N\\) random COM-Poisson realizations\nFit an intercept-only COM-Poisson regression model\nConduct the likelihood ratio test for equidispersion\nCalculate the p-value from (3)\nRepeat steps 1-4 for 100 iterations\nCompute the proportion of iterations in (5) where the p-value was less than 0.05\n\nThe following code chunk carries out these steps:\n\n\nCode\n# NOTE: THIS CHUNK TAKES A WHILE TO RUN\n\n# Number of iterations\nsims &lt;- 100\n\npower_results &lt;-\n  \n  # Parameter set\n  list(\n    lambda = c(1, 3, 5, 10, 25),\n    nu = c(0.5, 0.75, 1, 1.25, 1.5, 2),\n    N  = c(25, 50, 100, 500)\n  ) %&gt;%\n  \n  # Get the cross product\n  cross_df() %&gt;%\n  \n  # Split into a list\n  split(1:nrow(.)) %&gt;%\n  \n  # For each parameter combination\n  map_df(\n    function(x) {\n      \n      # Generate all CMP realizations\n      tibble(\n        S = rep(1:sims, x$N),\n        Y = rcmp(n = x$N * sims, lambda = x$lambda, nu = x$nu)\n      ) %&gt;%\n        \n        # For each iteration\n        group_by(S) %&gt;%\n        \n        # Compute the p-value\n        summarise(\n          PValue = \n            tryCatch(\n              equitest(glm.cmp(Y ~ 1))$pvalue,\n              error = function(e) NA_real_\n            ),\n          .groups = \"drop\"\n        ) %&gt;%\n        \n        # Add the parameter values\n        mutate(\n          lambda = x$lambda,\n          nu = x$nu,\n          N = x$N\n        )\n      \n    }\n  )\n\n\nThis code took quite a while to run (~4 hours). I’ve noticed the fitting procedure takes a bit longer than we’d be used to with a simple glm call (which is probably expected given the complexity of the estimation), and varies depending on the combination of parameter values. It’s also worth noting that it failed to converge on 0.9% of simulations, which tended to only occur when there was overdispersion (\\(\\nu=0.5\\)) with large counts (\\(\\lambda &gt; 5\\)). Nevertheless, the figure below gives us the results of the simulation showing the estimated power for each parameter combination:\n\n\nCode\npower_results %&gt;%\n  \n  # For each group\n  group_by(lambda, nu, N) %&gt;%\n  \n  # Compute the power, and a measure of simulation error\n  summarise(\n    Power = mean(PValue &lt;= 0.05, na.rm = TRUE),\n    SE = sd(PValue &lt;= 0.05, na.rm = TRUE) / sqrt(n()),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  # Make a factor\n  mutate(\n    N = factor(N),\n    nu = \n      nu %&gt;%\n      factor() %&gt;%\n      fct_relabel(\n        function(x) paste0(\"nu == \", x)\n      )\n  ) %&gt;%\n  \n  # Make a plot\n  ggplot(\n    aes(\n      x = lambda,\n      y = Power\n    )\n  ) +\n  geom_line(\n    aes(\n      color = N\n    ),\n    size = 1\n  ) +\n  geom_point(\n    aes(\n      color = N\n    ),\n    size = 3\n  ) +\n  geom_ribbon(\n    aes(\n      ymin = Power - 2*SE,\n      ymax = Power + 2*SE,\n      fill = N\n    ),\n    alpha = .35\n  ) +\n  facet_wrap(\n    ~nu,\n    label = \"label_parsed\"\n  ) +\n  ylab(\"Power\") +\n  theme(\n    legend.position = \"top\",\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    strip.text = element_text(size = 12),\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\")\n  ) +\n  scale_x_continuous(\n    breaks = unique(power_results$lambda)\n  ) +\n  labs(\n    x = expression(lambda)\n  )\n\n\n\n\n\n\n\n\n\nFindings:\n\nThe power increases with sample size and deviation from \\(\\nu=1\\)\nThe test has a much harder time detecting over/under dispersion when the counts are really small (i.e., \\(\\lambda = 1\\))\nWhen there is overdispersion (\\(\\nu&lt;1\\)), the power looks higher at lower values of \\(\\lambda\\), but the opposite occurs as the data becomes more underdispersed"
  },
  {
    "objectID": "post/how-to-assess-the-proportional-odds-assumption/index.html",
    "href": "post/how-to-assess-the-proportional-odds-assumption/index.html",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "",
    "text": "This dataset comes from the 2011 Annual Resident Survey in Durham, NC.\nCode\n# Load packages\nlibrary(tidyverse)\n\n# Import the dataset\ndat &lt;- read_delim(\n  file = \"https://query.data.world/s/zr3uaxpaagbzddttoreosktj2zy7lm?dws=00000\", \n  delim = \";\",\n  na = c(\"\", \" \", \"NA\", \"N/A\")\n) |&gt;\n  \n  # Keep a few columns\n  transmute(\n    QOL = \n      q3f_quality_of_life_in_city |&gt;\n      factor() |&gt;\n      fct_relevel(\n        \"Very Dissatisfied\",\n        \"Dissatisfied\",\n        \"Neutral\",\n        \"Satisfied\",\n        \"Very Satisfied\"\n      ),\n    Age = str_remove(`18_34_years`, \"(?i)\\\\syears$\") |&gt; factor(),\n    Income = \n      q38_annual_household_income |&gt;\n      factor() |&gt;\n      fct_relevel(\n        \"Under $30,000\",\n        \"$30,000 to $59,999\",\n        \"$60,000 to $99,999\",\n        \"$100,000 or more\"\n      ),\n    Sex = q34_respondents_gender\n  ) |&gt;\n  \n  # Remove missing cases\n  na.omit()\nSuppose we are interested in understanding the relationship between resident age and their perceived quality of life in the city, after adjusting for gender and annual household income. We have the following observed distribution (note that we’ve removed missing data for simplicity):\nCode\ndat |&gt; \n  \n  # Compute group summaries\n  summarize(\n    N = n(),\n    .by = \n      c(\n        QOL,\n        Age\n      )\n  ) |&gt;  \n  \n  # Flip the order\n  mutate(\n    QOL = fct_rev(QOL)\n  ) |&gt;\n  \n  # Make a plot\n  ggplot() + \n  geom_col(\n    aes(\n      x = Age,\n      y = N,\n      fill = QOL\n    ),\n    color = \"black\",\n    alpha = .75\n  ) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\"),\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    plot.title = element_text(hjust = .5)\n  ) +\n  xlab(\"Respondent Age (years)\") +\n  ylab(\"Count\") +\n  labs(title = \"Quality of life in the city\")\nOverall, it looks like older respondents tend to report more pessimistic views of quality of life."
  },
  {
    "objectID": "post/how-to-assess-the-proportional-odds-assumption/index.html#what-does-that-mean",
    "href": "post/how-to-assess-the-proportional-odds-assumption/index.html#what-does-that-mean",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "What does that mean?",
    "text": "What does that mean?\nLet’s clarify this by using our model output directly. We’ll fit the model, adjusted for annual household income and gender, using the MASS::polr function. The age group 18-34 will serve as the reference category in which all other age groups will be compared against. Note: By default the package computes odds ratios in the opposite direction to what we what we want, so we invert them.\n\n\nCode\n# Fit the model\nmod &lt;-\n  MASS::polr(\n    formula = QOL ~ Age + Income + Sex,\n    data = dat,\n    Hess = TRUE\n  )\n\n# Make a table of odds-ratios\nmod$coefficients |&gt;\n  \n  # Convert to data frame\n  enframe(\n    name = \"Term\",\n    value = \"Estimate\"\n  ) |&gt;\n  \n  # Join to get the CI\n  inner_join(\n    y = \n      # Get the 95% confidence intervals\n      confint(mod) |&gt; \n      \n      # Convert to tibble, add the coefficient names\n      as_tibble() |&gt; \n      add_column(Term = names(mod$coefficients)),\n    by = \"Term\"\n  ) |&gt; \n  \n  # Filter to age factor only\n  filter(str_detect(Term, \"^Age\")) |&gt;\n  \n  # Clean up\n  mutate(\n    Term = str_remove(Term, \"^Age\"),\n    across(\n      where(is.numeric),\n      \\(x) sprintf(\"%.2f\", 1 / exp(x))\n    )\n  ) |&gt; \n  \n  # Rename\n  rename(\n    `Age (years)` = Term,\n    `Odds-ratio` = Estimate,\n    Lower = `97.5 %`,\n    Upper = `2.5 %`\n  ) |&gt;\n  \n  # Change location\n  relocate(Lower, .before = Upper) |&gt;\n  \n  # Add the reference row\n  add_row(\n    `Age (years)` = \"18-34\",\n    `Odds-ratio` = \"-\",\n    Lower = \"-\",\n    Upper = \"-\",\n    .before = 1\n  ) |&gt;\n  \n  # Make a table\n  knitr::kable(format = \"html\") |&gt;\n  kableExtra::kable_styling() |&gt;\n  kableExtra::add_header_above(c(\"\", \"\", \"95% CI\" = 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% CI\n\n\n\nAge (years)\nOdds-ratio\nLower\nUpper\n\n\n\n\n18-34\n-\n-\n-\n\n\n35-44\n1.24\n0.67\n2.30\n\n\n45-54\n1.13\n0.64\n1.98\n\n\n55-64\n1.74\n0.97\n3.12\n\n\n65-74\n3.02\n1.40\n6.54\n\n\n75+\n0.93\n0.30\n2.88\n\n\n\n\n\n\n\nGenerally, the estimates pan out roughly how we suspected. In particular, the estimated odds of worse perceived quality of life in the city for 65-74 year olds are 3 times that of 18-34 year olds, after adjusting for annual household income and gender (with a 95% confidence interval of 1.4 to 6.5).\nAgain, this interpretation is assumed to hold true if “worse” is defined as very dissatisfied versus everything else, or very dissatisfied through satisfied versus very satisfied, and everything in between."
  },
  {
    "objectID": "post/how-to-assess-the-proportional-odds-assumption/index.html#continue",
    "href": "post/how-to-assess-the-proportional-odds-assumption/index.html#continue",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "Continue…",
    "text": "Continue…\nWe said earlier that the model assumes the same odds ratios for any mutually exclusive comparison of the ordered response categories. Thus, we can free up this constraint by thinking of constructing a collection of binary logistic regression models: one for each of those ordinal comparisons. Specifically,\n\nVery Dissatisfied versus everything else\nVery Dissatisfied or Dissatisfied versus everything else\nVery Dissatisfied, Dissatisfied, or Neutral versus Satisfied or Very Satisfied\nVery Dissatisfied through Satisfied versus Very Satisfied\n\nThen, we simply just look to see if the resulting odds ratios are reasonably similar across all of those models. If so, then we can be somewhat confident that it can be reduced to a single model, and stick with our original proportional-odds estimates.\nMy preference is to do this in a plot."
  },
  {
    "objectID": "post/how-to-assess-the-proportional-odds-assumption/index.html#making-the-plot",
    "href": "post/how-to-assess-the-proportional-odds-assumption/index.html#making-the-plot",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "Making the plot",
    "text": "Making the plot\nWe’ll cycle through the response categories, iteratively define the binary outcomes as described above, and then fit a logistic regression model for each definition. Once we do this, we get the following plot:\n\n\nCode\n# Set the number of comparisons\nn_comp &lt;- n_distinct(dat$QOL) - 1\n\n# Make each data set\n1:n_comp |&gt;\n  \n  # For each set\n  map_df(\n    function(.index) {\n      \n      # Extract the current response set\n      temp_resp &lt;- levels(dat$QOL)[1:.index]\n      \n      # Create binary outcome in the data\n      temp_dat &lt;- \n        dat |&gt;\n        \n        # Create target\n        mutate(\n          Response = \n            case_when(\n              QOL %in% temp_resp ~ 1,\n              TRUE ~ 0\n            )\n        )\n      \n      # Fit the binary logistic regression model\n      temp_mod &lt;-\n        glm(\n          formula = Response ~ Age + Income + Sex,\n          data = temp_dat,\n          family = \"binomial\"\n        )\n      \n      # Make a table of odds-ratios\n      temp_mod$coefficients |&gt;\n        \n        # Convert to data frame\n        enframe(\n          name = \"Term\",\n          value = \"Estimate\"\n        ) |&gt;\n        \n        # Join to get the CI\n        inner_join(\n          y = \n            # Get the 95% confidence intervals\n            confint.default(temp_mod) |&gt; \n            \n            # Convert to tibble, add the coefficient names\n            as_tibble() |&gt; \n            add_column(Term = names(temp_mod$coefficients)),\n          by = \"Term\"\n        ) |&gt; \n        \n        # Filter to age factor only\n        filter(str_detect(Term, \"^Age\")) |&gt;\n        \n        # Clean up\n        mutate(\n          Term = str_remove(Term, \"^Age\"),\n          across(\n            where(is.numeric),\n            \\(x) exp(x)\n          )\n        ) |&gt;\n        \n        # Rename\n        rename(\n          Age = Term,\n          OR = Estimate,\n          Lower = `2.5 %`,\n          Upper = `97.5 %`\n        ) |&gt;\n        \n        # Add the reference row\n        add_row(\n          Age = \"18-34\",\n          OR = 1,\n          Lower = 1,\n          Upper = 1,\n          .before = 1\n        ) |&gt;\n        \n        # Attach outcome level\n        add_column(QOL = levels(dat$QOL)[.index])\n      \n    },\n    .id = \"Order\"\n  ) |&gt;\n  \n  # Make the factor\n  mutate(\n    Order = as.numeric(Order),\n    QOL = factor(QOL) |&gt; fct_reorder(Order)\n  ) |&gt;\n  \n  # Make a plot\n  ggplot(\n    aes(\n      x = QOL,\n      y = OR,\n      group = 1\n    )\n  ) +\n  geom_line(\n    aes(\n      color = Age\n    ),\n    linewidth = 1.5\n  ) +\n  geom_point(\n    aes(\n      color = Age\n    ),\n    size = 3\n  ) +\n  geom_ribbon(\n    aes(\n      ymin = Lower,\n      ymax = Upper,\n      fill = Age\n    ),\n    alpha = .25\n  ) +\n  geom_hline(yintercept = 1, color = \"gray\") +\n  facet_wrap(~paste0(Age, \" years\")) +\n  coord_cartesian(ylim = c(0, 4)) +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray\"),\n    legend.position = \"none\",\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, size = 10),\n    plot.title = element_text(hjust = .5),\n    strip.text = element_text(size = 14)\n  ) +\n  xlab(\"Response\") +\n  ylab(\"Odds-ratio (95% CI) for being at or below response level\")\n\n\n\n\n\n\n\n\n\nUnfortunately we’re quite plagued by variability here, especially in the lower-end (i.e., very dissatisfied versus everything else), due to scanty event volumes, but you get the picture. Actually, for 65-74 year olds, the proportional-odds assumption seems to be a reasonable one: it was estimated earlier at 3.02, and we see the point estimates across these binary models vary between 2.5-3.5.\nFor other age categories, it may not be so good of an assumption. It looks like 35-44 and 55-64 year olds tend to have a much higher odds of responding very dissatisfied relative to 18-34 year olds, but there is much less of a difference (in all age categories) for the odds of responding very satisfied, suggesting something like older residents may make a point to select the least favorable response but don’t see much difference between being satisfied or very satisfied."
  },
  {
    "objectID": "post/how-to-assess-the-proportional-odds-assumption/index.html#so-what-do-we-do-in-practice",
    "href": "post/how-to-assess-the-proportional-odds-assumption/index.html#so-what-do-we-do-in-practice",
    "title": "How do you assess the proportional-odds assumption? Directly.",
    "section": "So what do we do in practice?",
    "text": "So what do we do in practice?\nFirst, the same proportional-odds assumptions hold for all covariates in the model, so we would also want to assess this for annual household income and gender. Second, if the assumption is not met, then we need to accommodate that by introducing more flexibility into the model. That may be by being clever with interaction terms, defining sensible groups to create, or by using separate binary models for each possible comparison, as we’ve done here. It’s really a judgement call."
  },
  {
    "objectID": "post/reconciling-regression-equation-spline-terms/index.html",
    "href": "post/reconciling-regression-equation-spline-terms/index.html",
    "title": "How to reconcile the regression equation from spline terms",
    "section": "",
    "text": "Suppose we want to model blood pressure as a function of age (dataset details found here):\nCode\n# Load packages\nlibrary(tidyverse)\n\n# Extract the data set\ndat &lt;- cheese::heart_disease\n\n# Make a plot\ndat |&gt;\n  ggplot() +\n  geom_point(\n    aes(\n      x = Age,\n      y = BP\n    ),\n    shape = 21,\n    size = 3,\n    fill = \"brown\",\n    alpha = .5\n  ) +\n  xlab(\"Age (years)\") +\n  ylab(\"Systolic blood pressure\") +\n  theme(\n    panel.background = element_blank(),\n    axis.title = element_text(size = 16),\n    axis.text = element_text(size = 12)\n  )\nIt looks like a larger age is associated with a larger blood pressure, on average, so we fit a simple linear regression model:\nCode\n# Fit the model\nmod1 &lt;- lm(BP ~ Age, data = dat)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = BP ~ Age, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.659 -11.449  -0.904  10.218  67.444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 101.4851     5.9364  17.095  &lt; 2e-16 ***\nAge           0.5548     0.1076   5.157 4.55e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.9 on 301 degrees of freedom\nMultiple R-squared:  0.08119,   Adjusted R-squared:  0.07814 \nF-statistic:  26.6 on 1 and 301 DF,  p-value: 4.547e-07\nWe estimate that for every 10 year increase in age, systolic blood pressure increases by 5.5 mmHg. We can also write out the full regression equation for estimating the blood pressure given a new patient’s age:\n\\[BP = 101.49 + 0.56 \\times Age\\]\nAnd we can add this to our plot to get a visual.\nCode\n# Make a plot\ndat |&gt;\n  ggplot() +\n  geom_point(\n    aes(\n      x = Age,\n      y = BP\n    ),\n    shape = 21,\n    size = 3,\n    fill = \"brown\",\n    alpha = .5\n  ) +\n  geom_abline(\n    slope = mod1$coefficients[[2]],\n    intercept = mod1$coefficients[[1]],\n    linewidth = 2\n  ) +\n  xlab(\"Age (years)\") +\n  ylab(\"Systolic blood pressure\") +\n  theme(\n    panel.background = element_blank(),\n    axis.title = element_text(size = 16),\n    axis.text = element_text(size = 12)\n  )\nSimple enough. We can verify our formula works by comparing with the output of the predict function:\nCode\ntibble(\n  BP1 = predict(mod1, newdata = dat),\n  BP2 = mod1$coefficients[[1]] + mod1$coefficients[[2]] * dat$Age\n) |&gt;\n  with(data = _, paste0(round(100 * mean(near(BP1, BP2))), \"% of fitted values match.\"))\n\n\n[1] \"100% of fitted values match.\"\nAs we can see, all of the predictions match, thus we understand exactly how the model works."
  },
  {
    "objectID": "post/reconciling-regression-equation-spline-terms/index.html#why-is-this-useful",
    "href": "post/reconciling-regression-equation-spline-terms/index.html#why-is-this-useful",
    "title": "How to reconcile the regression equation from spline terms",
    "section": "Why is this useful?",
    "text": "Why is this useful?\nFirst, it’s obviously important to understand how modeling software is getting to the results it gives you, so that you can correctly interpret it, among other things. Second, from a pragmatic point of view, the ability to write out the full regression equation allows you embed your model into any application (even Excel if you wanted), instead of requiring the predict function to be run, which in turn would require R to be a part of the application’s server. In this case, it’s unnecessary for that to be a requirement, since we can simply reconcile our model into an easily understandable equation. Luckily, there are also some functions that can extract these formulas for you for this exact purpose so you don’t have to go through the cumbersome calculations above."
  },
  {
    "objectID": "post/some-takeaways-from-effective-data-storytelling/index.html",
    "href": "post/some-takeaways-from-effective-data-storytelling/index.html",
    "title": "Some Takeaways from Effective Data Storytelling",
    "section": "",
    "text": "I’ve been reading through the book Effective Data Storytelling: How to drive change with data, narrative, and visuals by Brent Dykes and just wanted to document some thoughts/takeaways.\n\n1. Importance of domain knowledge\nOne of the biggest (and obvious) takeaways is the importance of domain knowledge is being able to craft an effective data story. You can have all the technical skills in world (e.g., math, statistics, programming, etc.), but if you aren’t able to relay that back to how it can impact things people care about, in simple terms, you’ll have trouble getting buy in. Thus, it is essential to be able to (i) acquire business knowledge and intuition for yourself, (ii) asking the right questions, and (iii) have subject matter experts (SMEs) who can partner with you to problem solve together. Domain knowledge is also incredibly important in the technical aspects as well (e.g., construction of useful statistical models generally requires assumptions about the context of the problem you are solving).\n\n\n2. Organizational culture and support\nI think in order to put data professionals in a position to be able to craft effective data stories, there needs to be some level of forethought into the data culture and structure of the organization. Just from a logistical perspective, it would be very difficult for someone to be able to take the necessary time, delicacy, rigor and sophistication needed to truly create an effective data story (e.g., using the right data, in the right analysis, with the right methods, with the right communication of results) without the support of foundational data infrastructure, data governance, process/project management, defined roles/responsibilities, overall strategy, etc. These things should be in place (or at least working towards it) to be able to apply the appropriate amount of rigor and detail required in an efficient and timely manner.\n\n\n3. Tailor communication of results to your audience\nI often build analyses in the form of HTML documents (via R Markdown) where the intention is to lay out a story of the data in a particular order, such the the end-user would read through the document on their own to gain the messages of the analysis with interactive figures and tables (I believe the idea of “scrollytelling”). However, I would often get asked by stakeholders to instead present/explain the material for them instead of them (understandably) reading through the whole thing on their own. So I would attend a meeting, bring up the document, and begin walking them through the document. I quickly found that this didn’t work. The intention of the document was to be read, not presented, and so people would get bogged down/distracted by the narrative in the document while presenting. It just felt uncomfortable. This is exactly something that is talked about in the book of what not to do. So, before sharing with stakeholders or presenting, I started converting the information in the document into a slide deck, pulling out the most useful information that was tailored to the way I would present it. Also, recreating graphs, tables, and/or figures that point out just the most pertinent data, instead of using the exhaustive summaries built in the exploratory analysis. These strategies proved to be much more effective and satisfying both for myself and the stakeholders."
  },
  {
    "objectID": "post/quickly-removing-higher-order-aggregation-text-in-reactable/index.html",
    "href": "post/quickly-removing-higher-order-aggregation-text-in-reactable/index.html",
    "title": "Removing higher-order aggregation text in {reactable}",
    "section": "",
    "text": "Often when I’m building a reactable I have potentially many aggregation levels but only want to display data in a lower subset of those. Here is a quick way to eliminate text using cell styling.\nFirst let’s load some packages and look at an example data set.\n\n\nCode\nlibrary(tidyverse)\nlibrary(reactable)\ndat &lt;- cheese::heart_disease\ndat\n\n\n# A tibble: 303 × 9\n     Age Sex    ChestPain           BP Cholesterol BloodSugar MaximumHR\n   &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;          &lt;dbl&gt;\n 1    63 Male   Typical angina     145         233 TRUE             150\n 2    67 Male   Asymptomatic       160         286 FALSE            108\n 3    67 Male   Asymptomatic       120         229 FALSE            129\n 4    37 Male   Non-anginal pain   130         250 FALSE            187\n 5    41 Female Atypical angina    130         204 FALSE            172\n 6    56 Male   Atypical angina    120         236 FALSE            178\n 7    62 Female Asymptomatic       140         268 FALSE            160\n 8    57 Female Asymptomatic       120         354 FALSE            163\n 9    63 Male   Asymptomatic       130         254 FALSE            147\n10    53 Male   Asymptomatic       140         203 TRUE             155\n# ℹ 293 more rows\n# ℹ 2 more variables: ExerciseInducedAngina &lt;fct&gt;, HeartDisease &lt;fct&gt;\n\n\nSuppose we are interested in these outcomes:\n\nThe rate of heart disease\nThe rate of patients with a maximum heart rate &gt; 150\n\nAnd we want to summarize those, along with a few other patient characteristics, within a handful of important clinical factors. We might pivot our data into a long format to start:\n\n\nCode\npivot_dat &lt;-\n  dat |&gt; \n  \n  # Convert to character types\n  mutate(\n    across(\n      c(ChestPain, ExerciseInducedAngina, BloodSugar),\n      as.character\n    ),\n    MaximumHR =\n      case_when(\n        MaximumHR &gt; 150 ~ \"Yes\",\n        TRUE ~ \"No\"\n      )\n  ) |&gt;\n  \n  # Send \"important clinical factors\" down the rows\n  pivot_longer(\n    cols = c(ChestPain, ExerciseInducedAngina, BloodSugar),\n    names_to = \"ClinicalFactor\",\n    values_to = \"Level\"\n  ) |&gt; \n  \n  # Send outcomes down the rows\n  pivot_longer(\n    cols = c(HeartDisease, MaximumHR),\n    names_to = \"Outcome\",\n    values_to = \"HasOutcome\"\n  )\npivot_dat\n\n\n# A tibble: 1,818 × 8\n     Age Sex      BP Cholesterol ClinicalFactor        Level  Outcome HasOutcome\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     \n 1    63 Male    145         233 ChestPain             Typic… HeartD… No        \n 2    63 Male    145         233 ChestPain             Typic… Maximu… No        \n 3    63 Male    145         233 ExerciseInducedAngina No     HeartD… No        \n 4    63 Male    145         233 ExerciseInducedAngina No     Maximu… No        \n 5    63 Male    145         233 BloodSugar            TRUE   HeartD… No        \n 6    63 Male    145         233 BloodSugar            TRUE   Maximu… No        \n 7    67 Male    160         286 ChestPain             Asymp… HeartD… Yes       \n 8    67 Male    160         286 ChestPain             Asymp… Maximu… No        \n 9    67 Male    160         286 ExerciseInducedAngina Yes    HeartD… Yes       \n10    67 Male    160         286 ExerciseInducedAngina Yes    Maximu… No        \n# ℹ 1,808 more rows\n\n\nWe can then calculate some summary statistics. Our full patient population is replicated for each ClinicalFactor and Outcome, so we should account for that.\n\n\nCode\nsummary_dat &lt;-\n  pivot_dat |&gt; \n  \n  # Summarize characteristics\n  summarize(\n    Patients = n(),\n    OutcomeRate = mean(HasOutcome == \"Yes\"),\n    Age = mean(Age),\n    Female = mean(Sex == \"Female\"),\n    .by = \n      c(\n        ClinicalFactor,\n        Level,\n        Outcome,\n        HasOutcome\n      )\n  ) |&gt; \n  \n  # Add counts within groups\n  mutate(\n    PercentOfPatients = Patients / sum(Patients),\n    .by = c(ClinicalFactor, Outcome)\n  ) |&gt; \n  \n  # Make clean levels/labels\n  mutate(\n    ClinicalFactor = \n      case_when(\n        ClinicalFactor == \"BloodSugar\" ~ \"Blood sugar &gt; 120 mg/dl?\",\n        ClinicalFactor == \"ChestPain\" ~ \"Chest pain type\",\n        ClinicalFactor == \"ExerciseInducedAngina\" ~ \"Exercise-induced angina?\"\n      ),\n    Level = \n      case_when(\n        Level == \"TRUE\" ~ \"Yes\",\n        Level == \"FALSE\" ~ \"No\",\n        TRUE ~ Level\n      ),\n    Outcome = \n      case_when(\n        Outcome == \"HeartDisease\" ~ \"Heart disease\",\n        Outcome == \"MaximumHR\" ~ \"Maximum HR &gt; 150\"\n      )\n  ) |&gt;\n  \n  # Rearrange\n  relocate(PercentOfPatients, .after = Patients) |&gt;\n  relocate(Outcome, .before = everything()) |&gt;\n  arrange(\n    Outcome,\n    ClinicalFactor,\n    Level,\n    HasOutcome\n  )\nsummary_dat\n\n\n# A tibble: 32 × 9\n   Outcome       ClinicalFactor      Level HasOutcome Patients PercentOfPatients\n   &lt;chr&gt;         &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;         &lt;int&gt;             &lt;dbl&gt;\n 1 Heart disease Blood sugar &gt; 120 … No    No              141            0.465 \n 2 Heart disease Blood sugar &gt; 120 … No    Yes             117            0.386 \n 3 Heart disease Blood sugar &gt; 120 … Yes   No               23            0.0759\n 4 Heart disease Blood sugar &gt; 120 … Yes   Yes              22            0.0726\n 5 Heart disease Chest pain type     Asym… No               39            0.129 \n 6 Heart disease Chest pain type     Asym… Yes             105            0.347 \n 7 Heart disease Chest pain type     Atyp… No               41            0.135 \n 8 Heart disease Chest pain type     Atyp… Yes               9            0.0297\n 9 Heart disease Chest pain type     Non-… No               68            0.224 \n10 Heart disease Chest pain type     Non-… Yes              18            0.0594\n# ℹ 22 more rows\n# ℹ 3 more variables: OutcomeRate &lt;dbl&gt;, Age &lt;dbl&gt;, Female &lt;dbl&gt;\n\n\nNow we’re ready to build the table (note the use of the zildge::rectbl_agg_wtd() function to compute the weighted average, see my other post for more information about this).\n\n\nCode\nreactable(\n  data = summary_dat,\n  groupBy = c(\"Outcome\", \"ClinicalFactor\", \"Level\"),\n  columns = \n    list(\n      ClinicalFactor = colDef(name = \"Clinical factor\"),\n      HasOutcome = colDef(name = \"Has Outcome?\"),\n      Patients = colDef(name = \"Count\", aggregate = \"sum\", align = \"center\"),\n      PercentOfPatients = colDef(name = \"Percent\", aggregate = \"sum\", align = \"center\", format = colFormat(digits = 2, percent = TRUE)),\n      OutcomeRate = colDef(name = \"Outcome rate (%)\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 2, percent = TRUE)),\n      Age = colDef(name = \"Avg. age (years)\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 2)),\n      Female = colDef(name = \"Female (%)\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 2, percent = TRUE))\n    ),\n  columnGroups = \n    list(\n      colGroup(\n        name = \"Patients\",\n        columns = c(\"Patients\", \"PercentOfPatients\")\n      )\n    ),\n  striped = TRUE,\n  highlight = TRUE,\n  bordered = TRUE,\n  resizable = TRUE,\n  theme = reactablefmtr::sandstone()\n) |&gt;\n  reactablefmtr::add_source(\"Use arrows to expand table\", font_size = 12, font_style = \"italic\")\n\n\n\nUse arrows to expand table\n\n\n\n\nThere are a few problems with this table:\n\nThe redundancy of the top-level (and second-level) summary statistics is unappealing\nThe top-level aggregation is just flat-out wrong because we’ve duplicated counts over the data set\n\nMaybe 1 is not a huge deal, but I’ve probably already established somewhere that there are 303 patients in the data set and the overall rate of heart disease is 45.9%. You still may choose to keep it as-is in this case as it’s not totally harmful. However, 2 needs to be addressed as the data is misleading and not really interpretable. So how can we remove the results from (at least) the top-level while maintaining the lower-level aggregation?\nWe can define an R function that provides JavaScript custom styling to a cell.\n\n\nCode\nremove_text &lt;-\n  function(col, threshold) {\n    JS(\n      paste0(\n        \"function(rowInfo) {\n          if(rowInfo.row['\", col, \"'] &gt; \", threshold, \") {\n            return {fontSize:0}\n          }\n        }\"\n      )\n    )\n  }\n\n\nAll this function does is reduce the font size to zero for any cell where the value of col in the respective row is greater than threshold. Thus, since we are over-counting the number of patients in the top-level, we can just apply this function to rows in which the count exceeds 303, which is the patient count in our data set. We provide this function to the style argument of colDef().\n\n\nCode\nreactable(\n  data = summary_dat,\n  groupBy = c(\"Outcome\", \"ClinicalFactor\", \"Level\"),\n  columns = \n    list(\n      ClinicalFactor = colDef(name = \"Clinical factor\"),\n      HasOutcome = colDef(name = \"Has Outcome?\"),\n      Patients = colDef(style = remove_text(\"Patients\", nrow(dat)), name = \"Count\", aggregate = \"sum\", align = \"center\"),\n      PercentOfPatients = colDef(style = remove_text(\"Patients\", nrow(dat)), name = \"Percent\", aggregate = \"sum\", align = \"center\", format = colFormat(digits = 2, percent = TRUE)),\n      OutcomeRate = colDef(style = remove_text(\"Patients\", nrow(dat)), name = \"Outcome rate (%)\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 2, percent = TRUE)),\n      Age = colDef(style = remove_text(\"Patients\", nrow(dat)), name = \"Avg. age (years)\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 2)),\n      Female = colDef(style = remove_text(\"Patients\", nrow(dat)), name = \"Female (%)\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 2, percent = TRUE))\n    ),\n  columnGroups = \n    list(\n      colGroup(\n        name = \"Patients\",\n        columns = c(\"Patients\", \"PercentOfPatients\")\n      )\n    ),\n  striped = TRUE,\n  highlight = TRUE,\n  bordered = TRUE,\n  resizable = TRUE,\n  theme = reactablefmtr::sandstone()\n) |&gt;\n  reactablefmtr::add_source(\"Use arrows to expand table\", font_size = 12, font_style = \"italic\")\n\n\n\nUse arrows to expand table\n\n\n\n\nIf we want to remove text from the second level of aggregation, we can just adjust the threshold.\n\n\nCode\nreactable(\n  data = summary_dat,\n  groupBy = c(\"Outcome\", \"ClinicalFactor\", \"Level\"),\n  columns = \n    list(\n      ClinicalFactor = colDef(name = \"Clinical factor\"),\n      HasOutcome = colDef(name = \"Has Outcome?\"),\n      Patients = colDef(style = remove_text(\"Patients\", nrow(dat) - 1), name = \"Count\", aggregate = \"sum\", align = \"center\"),\n      PercentOfPatients = colDef(style = remove_text(\"Patients\", nrow(dat) - 1), name = \"Percent\", aggregate = \"sum\", align = \"center\", format = colFormat(digits = 2, percent = TRUE)),\n      OutcomeRate = colDef(style = remove_text(\"Patients\", nrow(dat) - 1), name = \"Outcome rate (%)\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 2, percent = TRUE)),\n      Age = colDef(style = remove_text(\"Patients\", nrow(dat) - 1), name = \"Avg. age (years)\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 2)),\n      Female = colDef(style = remove_text(\"Patients\", nrow(dat) - 1), name = \"Female (%)\", aggregate = zildge::rectbl_agg_wtd(\"Patients\"), align = \"center\", format = colFormat(digits = 2, percent = TRUE))\n    ),\n  columnGroups = \n    list(\n      colGroup(\n        name = \"Patients\",\n        columns = c(\"Patients\", \"PercentOfPatients\")\n      )\n    ),\n  striped = TRUE,\n  highlight = TRUE,\n  bordered = TRUE,\n  resizable = TRUE,\n  theme = reactablefmtr::sandstone()\n) |&gt;\n  reactablefmtr::add_source(\"Use arrows to expand table\", font_size = 12, font_style = \"italic\")\n\n\n\nUse arrows to expand table\n\n\n\n\nAnd that’s it.\nI’ll acknowledge that I didn’t exhaust all options for this task and that there are definitely better ways to do this with more directed intention by accessing various properties of the table. Obviously this only works if you have a column in the table that only increases during the aggregation and does so to predictable amounts. But it did the trick here."
  },
  {
    "objectID": "post/on-the-creation-of-classical-statistics/index.html",
    "href": "post/on-the-creation-of-classical-statistics/index.html",
    "title": "On the Creation of Classical Statistics",
    "section": "",
    "text": "I used to have a somewhat cynical view of R.A. Fisher, especially on the motivation for statistical significance (see my previous article). Even though he did explicitly advocate for the use of the 5% threshold:\n\n“If P is between .1 and .9 there is certainly no reason to suspect the hypothesis tested. If it is below .02 it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 and consider that higher values of [the statistic] indicate a real discrepancy.”1\n\nand\n\n“If one in twenty does not seem high enough, we may, if we prefer, draw the line at one in fifty (the 2 percent point), or one in a hundred (the 1 percent point). Personally, the writer prefers to set a low standard at the 5 percent point, and ignore entirely all results which fail this level.”2\n\nAfter reading Erich Lehmann’s book, Fisher, Neyman, and the Creation of Classical Statistics, I realize there is much more nuance to it, and he probably meant well in his statistical work (his other work, maybe a different story). I’m fairly convinced that he never imagined, nor would approve of, how statistical significance would be used and abused since then.\n\nAn experimentation context\nMuch of the methodology related to hypothesis testing that Fisher developed (along with other fundamental concepts (in 1922) like consistency, efficiency, and sufficiency) was motivated by the specific context he started out in as an agricultural statistician in 1919 at Rothamstead Experimental Station: that of small-sample, randomized experimentation. It is clear in his writings, though maybe implicit, that there were practical things he considered that played into the overall validity of inference, not only whether the p-value crossed a threshold:\n\n“…it is not known whether heterogeneity [of the soil] will be more pronounced in the one or the other direction in which the field is ordinarily cultivated…The effects are sufficiently widespread to make apparent the importance of eliminating the major effects of soil heterogeneity not only in one direction across the field, but at the same time in the direction at right angles to it.”3\n\nHe wasn’t proposing that his methods be mechanically applied, or that the method itself is what proves valid inference. Rather, inherent in that quote is the intuition Fisher had about the subject, the “soil heterogeneity”, that made the implications of the design useful for that situation. This, in combination with his obviously deep statistical knowledge, is what I believe ultimately made the usage of seemingly arbitrary significance thresholds valid in Fisher’s eyes. It’s not that he didn’t want statistical analysis to be “easier” for researchers (and he was somewhat back and forth on this):\n\n“However, his early recommendation and life-long practice prevailed. The desire for standardization trumped the advantages of considering each case on its own merit.”4\n\nI think he probably just put too much confidence in the implementers of his work to be as critical, meticulous, and simply as brilliant as he was. He never conceived of the erroneous ways his statistical and design principles would later be used.\n\n\nHe had a mentor\nOne of the most fascinating aspects of the history of classical statistics is the role of William Sealy Gosset (a.k.a. “Student”, as in Student’s t-test). For his entire career, he was a beer brewer at Arthur Guinness Son and Co. (one of my favorites), yet he is credited with putting forth, through his own curiosity, intelligence, and need of practical solutions for quality control efforts, the ideas of which Fisher would ultimately bring to fruition:\n\n“After a small-sample (”exact”) approach to testing was initiated by Gosset (“Student”) in 1908 with his t-test, Fisher in the 1920’s, under frequent prodding by Gosset, developed a battery of such tests, all based on the assumption of normality. These tests today still constitute the bread and butter of much of statistical practice.”4\n\nThat “frequent prodding” Lehmann is talking about, in addition to the timeline, is why I characterize Gosset more like a mentor. Fisher was 14 years younger, but incredibly gifted intellectually.\n\n“He [Gosset] then had the crucial insight that exact results [for a t-test] could be obtained by making an additional assumption…although he was not able to give a rigorous proof. The first proof was obtained (although not published) by Fisher in 1912…as a result of constant prodding and urging by Gosset, he found a number of additional small-sample distributions, and in 1925 presented the totality of these results in his book…getting Fisher to develop this methodology much further than he (Fisher) had originally intended.”4\n\nFisher was only 22 years old in 1912. It seems Gosset’s wisdom helped him pinpoint the arguments he would come to make, and ultimately gave him the encouragement and motivation to see it through. Without that, who knows if any of it would have been done.\n\n“This passage suggests that Fisher thought these problems to be difficult, and that he had no plans to work on them himself. However, in April 1922 he received two letters from Gosset that apparently changed his mind.”4\n\nNot to mention Gosset’s influence on Neyman’s (and Egon Pearson’s) foundational work regarding the “consideration of the alternatives (suggested by Gosset)”, Fisher did acknowledge his contributions and spoke highly of him.\n\n“…an exact solution of the distribution of regression coefficients…has been outstanding for many years; but the need for its solution was recently brought home to the writer by correspondence with ‘Student’, whose brilliant researches in 1908 form the basis of the exact solution”5\n\n\n\nHe was in fact, a genius\nDespite their “disdain” for one another:\n\n“Both Fisher and Neyman believed that they had made important contributions to the philosophy of science, but each felt that the other’s views were completely wrong-headed.”4\n\nMuch of their foundational work was complimentary. Fisher supplied the methodology, Neyman put the rubber stamp on it with mathematical proofs.\nThe thing that caught my attention that Lehmann mentions multiple times in the book is the way Fisher came up with those methods.\n\n“Fisher’s tests were solely based on his intuition. The right choice of test statistics was obvious to him. A theory that would justify his choices was developed by Neyman and Pearson in their papers in 1928 and 1933.”4\n\nAs you read about the progression of his work, it’s like all the fundamental statistical concepts pop-up one by one, and you realize the breadth and depth of Fisher’s accomplishments. The idea that this can be attributed to his “intuition” is just remarkable. It wasn’t just in testing, but also in design:\n\n“…the designs in DOE [The Design of Experiments, 1935] were presented without much justification, based entirely on his intuitive understanding of what the situation demanded. But again later writers found justifications by showing that Fisher’s procedures possessed certain optimality properties.”4\n\nEven when you read Fisher’s passages directly, you get the feeling that it just rolled off his tongue and he was writing down what flowed from his mind. Though what he was writing turned out to be fundamental to statistical practice:\n\n“…much caution should be used before claiming significance for special comparisons… Comparisons suggested by scrutiny of the results themselves are open to suspicion; for if the variants are numerous, a comparison of the highest with the lowest observed value will often appear to be significant, even from undifferentiated material.”3\n\nIn this case, the problem with multiple comparisons. This is the general tone of Fisher’s writings, just nonchalantly bringing up things like power, creating block designs, etc. as “obvious” considerations.\nUnfortunately, despite all Fisher did achieve, his stubbornness prevented him from achieving more.\n\n“…Fisher rarely gave an inch. Those holding different views from his own had ‘misread’ him and their statements were ‘incorrect’.”4\n\nAnd subsequently, even though he hinted at it with his idea of “sensitiveness”:\n\n“By not utilizing the idea of power, Fisher deprives himself of the ability to resolve one of the most important issues of experimental design, the determination of sample size.”4\n\nIt seems he grew bitter and resentful in older age. For one, Fisher, “the creator of modern statistics”, in his role at University College under Egon Pearson, was “not permitted to teach statistics”. Also, all the progress and innovation in statistics shifted to the United States after Neyman moved there in 1938. People appreciated his foundational work, but they were taking it in a different direction and he was too far away to continue having influence. Nevertheless, his legacy is set in stone.\n\n\nReferences\n\nFisher, R.A. (1925). Statistical methods for research workers. Oliver and Boyd: Edinburgh.\nFisher, R.A. (1926). The arrangement of field experiments. J. Min. Agric. G. Br. 33:503-513\nFisher, R.A. (1935). The Design of Experiments. Oliver and Boyd: Edinburgh.\nLehmann, Erich L (2011). Fisher, Neyman, and the Creation of Classical Statistics. Springer New York, NY. https://doi.org/10.1007/978-1-4419-9500-1\nFisher, R.A. (1922). The goodness of fit of regression formulae, and the distribution of regression coefficients. J. Roy. Statist. Soc., 85: 597-612"
  },
  {
    "objectID": "post/can-you-have-a-model-without-data/index.html",
    "href": "post/can-you-have-a-model-without-data/index.html",
    "title": "Can you have a model without data?",
    "section": "",
    "text": "In frequentist statistics, the paradigm in which much of statistical practice is done, has a specific requirement: we need data before we can attribute estimates to (i.e., “fit”) our model. Yes, we might pre-specify it’s form, and be quite confident in what that looks like, but ultimately before we get an answer, we need the data.\nFor example, suppose we are interested in the proportion of individuals with heart disease. We can specify an assumed model:\n\\[X \\sim Bernoulli(p)\\]\nwhere \\(X \\in \\{0,1\\}\\) and \\(p \\in [0,1]\\).\nThat is, we assume that whether an individual has heart disease is a coin flip with probability \\(p\\). Our goal is to estimate what \\(p\\) is.\nWe plan to use the typical approach for estimating a population proportion such that:\n\\[\\hat{p} = \\frac{\\sum_{i=1}^nx_i}{n} \\hskip.2in Var(\\hat{p}) = \\frac{\\hat{p}(1-\\hat{p})}{n}\\]\nwhere \\(x_i\\) is the indicator of whether or not individual \\(i\\) in the sample has heart disease, and \\(n\\) is the total number of individuals in the sample. That is, we take the average, or sample proportion. The variance provides a window of uncertainty in our estimate.\nOkay let’s do it.\nBut wait, in order for us to get a numerical quantity to work with, we need data to plug into these equations. That is the point. Our model in this paradigm becomes data focused, such that a sample is required. And a large enough one at that.\nOur model of the world is completely dependent on collecting and entering a sample into the estimators, despite what we may already know about heart disease rates. Thus, it is only informed by the data at hand.\nOkay, fine. So we find a dataset related to heart disease:\nCode\ndat &lt;- cheese::heart_disease\ndat\n\n\n# A tibble: 303 × 9\n     Age Sex    ChestPain           BP Cholesterol BloodSugar MaximumHR\n   &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;          &lt;dbl&gt;\n 1    63 Male   Typical angina     145         233 TRUE             150\n 2    67 Male   Asymptomatic       160         286 FALSE            108\n 3    67 Male   Asymptomatic       120         229 FALSE            129\n 4    37 Male   Non-anginal pain   130         250 FALSE            187\n 5    41 Female Atypical angina    130         204 FALSE            172\n 6    56 Male   Atypical angina    120         236 FALSE            178\n 7    62 Female Asymptomatic       140         268 FALSE            160\n 8    57 Female Asymptomatic       120         354 FALSE            163\n 9    63 Male   Asymptomatic       130         254 FALSE            147\n10    53 Male   Asymptomatic       140         203 TRUE             155\n# ℹ 293 more rows\n# ℹ 2 more variables: ExerciseInducedAngina &lt;fct&gt;, HeartDisease &lt;fct&gt;\nAnd we plug our estimates into the formulas and get our result:\nCode\n# Sample proportion\np_hat &lt;- mean(dat$HeartDisease == \"Yes\")\n\n# Sample size\nn &lt;- nrow(dat)\n\n# Standard error\nse &lt;- sqrt((p_hat * (1- p_hat)) / n)\n\n# Construct confidence interval\ndata.frame(\n  Estimate = paste0(round(p_hat * 100, 2), \"%\"),\n  Lower = paste0(round(100 * (p_hat - qnorm(.96) * se), 2), \"%\"),\n  Upper = paste0(round(100 * (p_hat + qnorm(.96) * se), 2), \"%\")\n) |&gt;\n  knitr::kable(format = \"html\") |&gt;\n  kableExtra::kable_styling(full_width = FALSE) |&gt;\n  kableExtra::add_header_above(c(\"\", \"92% Confidence Interval\" = 2))\n\n\n\n\n\n\n\n\n\n\n\n\n92% Confidence Interval\n\n\n\nEstimate\nLower\nUpper\n\n\n\n\n45.87%\n40.86%\n50.89%\nDespite what we may think of this result (which is certainly high in any general population context), there’s not much wiggle room with respect to the output. The data is what it is: we estimate that \\(p\\) is somewhere between 41% and 51% with 92% confidence. And that’s it.\nWhat if this sample wasn’t able to be gathered all at once? What if we already knew stuff about the rate of heart disease? What if we wanted our estimates to be informed by prior information we had?\nWhen we consider our data being sequentially collected, we run into problems early on.\nCode\n# Set grid\nn_obs &lt;- c(1, 5, 10, 25, 50, 100, 200, nrow(dat))\n\n# Set values\np_hat &lt;- c()\nn &lt;- c()\nse &lt;- c()\n\n# Iterate the grid\nfor(i in 1:length(n_obs)) {\n  \n  # Compute estimates\n  temp_n &lt;- n_obs[i]\n  temp_p_hat &lt;- mean(dat$HeartDisease[1:temp_n] == \"Yes\")\n  temp_se &lt;- sqrt((temp_p_hat * (1 - temp_p_hat)) / temp_n)\n  \n  # Add to lists\n  p_hat &lt;- c(p_hat, temp_p_hat)\n  n &lt;- c(n, temp_n)\n  se &lt;- c(se, temp_se)\n  \n}\n\n# Make a plot\nlibrary(ggplot2)\ndata.frame(\n  p_hat = c(NA, p_hat), \n  n = c(0, n),\n  se = c(NA, se)\n) |&gt;\n  ggplot() + \n  geom_point(\n    aes(\n      x = factor(n),\n      y = p_hat\n    ),\n    size = 3\n  ) +\n  geom_linerange(\n    aes(\n      x = factor(n),\n      ymin = p_hat - qnorm(.96) * se,\n      ymax = p_hat + qnorm(.96) * se\n    ),\n    linewidth = 1\n  ) +\n  coord_flip() +\n  theme(\n    panel.background = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray\"),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14)\n  ) +\n  scale_y_continuous(name = \"Estimated p (92% CI)\", labels = scales::percent) +\n  xlab(\"Sample size\")\nFirst, when we have no data, we can’t get any estimate (obviously). We have to pretend we know nothing about what the rate of heart disease is. After adding only 1 observation, our estimate for \\(p\\) is 0% with a 92% confidence interval ranging from 0% to 0%. This is not useful or informative, as that estimate was based on a single individual. Then, when we just add 4 more observations, our estimate of \\(p\\) becomes wildly uncertain (40% with a 92% confidence interval from 2% to 78%). This accumulated information is inconsistent and counter-intuitive (of course I’m using large sample methods, so we could use more appropriate small sample approaches, but that’s part of the point). Eventually as more data is added, the estimate gets more precise, but, again, completely driven by the data.\nThe bottom line being that in the frequentist paradigm, we are handcuffed. We can’t mathematically provide estimates until there is sufficient data collected, despite what our intuition or prior knowledge tells us about the parameter of interest beforehand.\nWhat if we have no data, or very little? What if we need to make decisions along the way before all of the data is collected, using our best estimate as of now? As we saw above, we need to wait to have sufficient data to get something reliable.\nIs there a way to provide a starting point about what we think the true rate of heart disease is, and then have our estimates be informed or augmented by evidence?\nYes, by being a Bayesian."
  },
  {
    "objectID": "post/can-you-have-a-model-without-data/index.html#prior",
    "href": "post/can-you-have-a-model-without-data/index.html#prior",
    "title": "Can you have a model without data?",
    "section": "Specify a prior distribution",
    "text": "Specify a prior distribution\nBefore any of the data is collected, we can use our subject matter knowledge about a phenomenon as to where we think a parameter value lies.\nIn the example above, suppose we thought it’s likely that the true parameter value \\(p\\) is somewhere around 35% in this population, of course allowing for some uncertainty.\nWe can assign a prior distribution to \\(p\\) using the Beta distribution (you could use anything you wanted that adheres to your prior knowledge, it just happens that this distribution works nicely in the case of proportions, an example of the conjugate prior):\n\\[p \\sim Beta(\\alpha = 4.5, \\beta = 7.5)\\]\nwhere the probability density function (PDF) is defined as:\n\\[f(x|\\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\]\nwhere \\(x \\in [0,1]\\), \\(\\alpha, \\beta &gt; 0\\), and \\(\\Gamma(n) = (n-1)!\\).\nAnd we can construct a plot to visualize our initial beliefs about the parameter \\(p\\):\n\n\nCode\n# Get the density values\nx &lt;- seq(0, 1, .01)\ny &lt;- dbeta(x, 4.5, 7.5)\n\n# Make the plot\ndata.frame(x, y) |&gt;\n  ggplot() +\n  geom_line(\n    aes(\n      x = x,\n      y = y\n    )\n  ) +\n  geom_vline(\n    xintercept = c(.2, .5),\n    color = \"red\"\n  ) +\n  theme(\n    panel.background = element_blank(),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  scale_x_continuous(\n    name = \"p\",\n    labels = scales::percent\n  )\n\n\n\n\n\n\n\n\n\nSo initially we think there is a 72% chance that the true value of \\(p\\) is between 20% and 50% (taken as the area under the curve between those two points), with more probability mass towards the center.\nIn this sense, we have our model estimate already in its complete form. If the current state of information isn’t sufficient, then we can collect data to help guide/inform our prior belief. So instead of requiring a (large enough) sample to realize any numerical estimate, we have one with zero data points. As we add data, our model will update proportionally/appropriately to the amount of new information it contains. Therefore, we can think of this prior distribution as equivalent to our current posterior distribution, whether we got here from prior data, intuition, or a plain guess, it doesn’t really matter. Our current knowledge of \\(p\\) captures all that we know about it, and will only change as new information is added. Thus, we have constructed a model with no data."
  },
  {
    "objectID": "post/can-you-have-a-model-without-data/index.html#updating-the-model-with-data",
    "href": "post/can-you-have-a-model-without-data/index.html#updating-the-model-with-data",
    "title": "Can you have a model without data?",
    "section": "Updating the model with data",
    "text": "Updating the model with data\nNow we want a way to update our prior (or current) knowledge of the parameter of interest as new information comes in. The result of this is called the posterior distribution, which tells us where the parameter value(s) are most likely to be, given our prior beliefs + new data. The derivation of this distribution is done through Bayes’ theorem.\nIn our example (and any analysis), the posterior distribution is written as:\n\\[P(p|data) = \\frac{P(p)P(data|p)}{P(data)}\\]\nThe \\(P(p)\\) is the prior distribution, which we saw above, \\(P(data|p)\\) is the likelihood of observing our data given a particular value of \\(p\\), and \\(P(data)\\) is the probability of observing our dataset across all values of \\(p\\) (i.e., the law of total probability). In general, the denominator is not dependent on the parameter, and since we’re conditioned on the \\(data\\), this just amounts to a normalizing constant to ensure the posterior distribution is a valid probability distribution. Thus, we only need to concern ourselves with the form of the numerator, and can write the posterior as proportional to the product of the prior and likelihood:\n\\[P(p|data) \\propto P(p)P(data|p)\\]\n\nDefine the likelihood\nSimilar to what we saw in the frequentist approach above, the likelihood of the data in our Bayesian model can be thought of as a Bernoulli random variable, where each patient has heart disease or they don’t, for a given probability \\(p\\). Because our observations are independent, the collection of these “coin flips” can be summarized using the Binomial distribution:\n\\[H|n, p \\sim Binomial(n,p)\\]\nwhere \\(n\\) is the sample size, \\(p\\) is the probability of heart disease (the parameter of concern), and \\(H\\) is the total number of patients with heart disease in a sample. The probability mass function (PMF) for this distribution looks like:\n\\[P(data|p) = P(H|n,p) = \\frac{n!}{H!(n-H)!}p^H(1-p)^{n-H}\\]\nSo for a given sample size and probability, we can compute the likelihood of observing any number of patients with heart disease.\n\n\nDerive the posterior\nAs mentioned, the posterior is derived by taking the prior distribution multiplied by the likelihood function.\n\\[\n\\begin{equation}\n\\begin{split}\nP(p|data) & \\propto P(p)P(data|p) \\\\\n& = P(p)P(H|n, p)\\\\\n& = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1} \\frac{n!}{H!(n-H)!}p^H(1-p)^{n-H} \\\\\n& \\propto p^{\\alpha - 1 + H}(1-p)^{\\beta - 1 + n - H}\n\\end{split}\n\\end{equation}\n\\]\nIt turns out this is just another Beta distribution with a different parameterization (note the * to differentiate from the prior parameter values):\n\\[p|N,Y \\sim Beta(\\alpha^* = \\alpha + H, \\beta^* = \\beta + n - H)\\]\nNotice that we only needed the kernel to classify this distribution, because that part was dependent on the parameter \\(p\\). The rest is just a constant that normalizes it to be a valid probability distribution (as mentioned earlier), meaning it sums (integrates) to 1. Thus, since we know it’s a Beta, we can write out the full posterior PDF:\n\\[\n\\begin{equation}\n\\begin{split}\nf(p|\\alpha^*, \\beta^*) & = \\frac{\\Gamma(\\alpha^* + \\beta^*)}{\\Gamma(\\alpha^*)\\Gamma(\\beta^*)}p^{\\alpha^*-1}(1-p)^{\\beta^*-1} \\\\\n& = \\frac{\\Gamma(\\alpha + \\beta + n)}{\\Gamma(\\alpha + H)\\Gamma(\\beta + n - H)}p^{\\alpha + H-1}(1-p)^{\\beta + n - H-1} \\\\\n\\end{split}\n\\end{equation}\n\\]\nWhat this tells us is that from our initial model, the posterior distribution is just moved/shifted as new data comes in. Also notice the effect of sample size as clearly indicated by the equation: as more data comes in (i.e., higher \\(H\\) and \\(N\\) values), the more the prior distribution will be drowned out. Meaning that we are only straying away from the initial/prior belief “proportional” to how much new information is coming in. This is what allows us to have perfectly valid models and estimates, even with a single observation or no data at all.\n\n\nPlug in the data\nThe hard part is done. Now all we need to do is plug in our data into the posterior distribution. In our sample of 303 patients (\\(n\\)), we observed 139 patients with heart disease (\\(H\\)). Plotting it across the range of possible values for \\(p\\) looks like this:\n\n\nCode\n# Set sample stats\nH &lt;- sum(dat$HeartDisease == \"Yes\")\nn &lt;- nrow(dat)\n\n# Get the density values\nx &lt;- seq(0, 1, .01)\ny &lt;- dbeta(x, 4.5 + H, 7.5 + n - H)\n\n# Make the plot\ndata.frame(x, y) |&gt;\n  ggplot() +\n  geom_line(\n    aes(\n      x = x,\n      y = y\n    )\n  ) +\n  geom_vline(\n    xintercept = c(.4, .5),\n    color = \"red\"\n  ) +\n  theme(\n    panel.background = element_blank(),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  scale_x_continuous(\n    name = \"p\",\n    labels = scales::percent\n  )\n\n\n\n\n\n\n\n\n\nAfter our update using the data set (appended to our prior belief), we now estimate there is a 94% chance that the true value of \\(p\\) is between 40% and 50% (taken as the area under the curve between those two points), again with more probability mass towards the center.\n\n\nIncremental updates\nTo drive the point home, we’ll now revisit how our estimates change when data is added sequentially, and contrast that with what we saw from the frequentist approach above. To do this, we can just evaluate the posterior distribution at incremental chunks of our dataset to see how it changes as more data is added (assuming some sort of chronological structure to the data).\n\n\nCode\n# Set values\np &lt;- seq(0, 1, .01)\nresults &lt;- data.frame(n = 0, p = p, y = dbeta(p, 4.5, 7.5))\n\n# Iterate the grid\nfor(i in 1:length(n_obs)) {\n  \n  # Compute estimates\n  temp_n &lt;- n_obs[i]\n  temp_H&lt;- sum(dat$HeartDisease[1:temp_n] == \"Yes\")\n  temp_y &lt;- dbeta(p, 4.5 + temp_H, 7.5 + temp_n - temp_H)\n  \n  # Store values\n  results &lt;-\n    results |&gt;\n    rbind(\n      data.frame(\n        n = temp_n,\n        p = p,\n        y = temp_y\n      )\n    )\n  \n}\n\n# Make the plot\nresults |&gt;\n  ggplot() +\n  geom_line(\n    aes(\n      x = p,\n      y = y\n    )\n  ) + \n  facet_wrap(~paste0(\"n = \", n) |&gt; forcats::fct_reorder(y, max)) +\n  geom_vline(\n    xintercept = c(.4, .5),\n    color = \"red\"\n  ) +\n  theme(\n    panel.background = element_blank(),\n    axis.text = element_text(size = 10),\n    axis.title = element_text(size = 14),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  scale_x_continuous(\n    name = \"p\",\n    labels = scales::percent\n  )\n\n\n\n\n\n\n\n\n\nAs done before, the red lines indicate the amount of the posterior distribution between 40% and 50%.\nFirst, we actually have an estimate before there is any data collected (when \\(n=0\\)). As we add a few observations, it only changes a little, but our estimates still retain our prior information about \\(p\\). Then as more data is collected, we see the posterior distribution become much more precise in where it estimates \\(p\\) to be.\nThis smooth integration of, and transition from, the prior knowledge we incorporate into the model to the information augmented by the data we collect is one reason why I think Bayesian thinking is better suited for scientific modeling. A more natural accumulation of knowledge, erasing the boundaries between what we already know (which should be considered a form of “data” itself) and hard data collected on a spreadsheet. It changes the way you approach the problem: instead of focusing right away on the data, which will be exhausted once it’s used, you focus on conceptualizing the living, breathing model of the world that generated it, and thus allow data to only contribute to that model as seen fit."
  },
  {
    "objectID": "post/you-should-have-a-data-science-blog/index.html",
    "href": "post/you-should-have-a-data-science-blog/index.html",
    "title": "You should have a data science blog",
    "section": "",
    "text": "One thing I would highly recommend to anyone with an interest in statistics/data science, whether you’re a student just starting out or a professional increasing your skills, is to create a blog. In this blog post, we’ll cover some of the benefits of doing so and touch on our preferred workflow to get one up and running (for free)."
  },
  {
    "objectID": "post/you-should-have-a-data-science-blog/index.html#software-developmentdeployment",
    "href": "post/you-should-have-a-data-science-blog/index.html#software-developmentdeployment",
    "title": "You should have a data science blog",
    "section": "1. Software development/deployment",
    "text": "1. Software development/deployment\nYou might think that the logistics/maintenance of a blog sounds cumbersome. You just want to stick to the content. However, learning how to create, develop, maintain, and deploy the blog, regardless of its content, provides in itself exposure to essential skills for data science: software development and deployment. Simply knowing the math or the R/Python commands to run a model is one thing, but if you want to deliver effective analytical solutions, those results probably need to be delivered to the end user in a useful way. Building a blog (website) forces you to learn about things such as version control, CI/CD, web hosting, DNS records, web development…the list goes on, which are all components of creating a data science product. Marrying these things to the content creation itself provides you an arsenal of tools that can be exploited in all sorts of contexts."
  },
  {
    "objectID": "post/you-should-have-a-data-science-blog/index.html#solidify-concepts",
    "href": "post/you-should-have-a-data-science-blog/index.html#solidify-concepts",
    "title": "You should have a data science blog",
    "section": "2. Solidify concepts",
    "text": "2. Solidify concepts\nThere are a lot of topics to keep track of when you’re learning statistics/data science, and one very effective way to solidify your understanding is to write it out coherently. For example, you may be learning about linear regression but there may be nuances surrounding it that are fuzzy, such as why a \\(\\beta\\) parameter is interpreted the way that it is. So, you may opt to write a blog post that goes through this derivation in an applied example with a coherent, structured narrative–by the end, you’ll generally grasp the thing you set out for. You just need to start with an outline of thing you want to understand and then learn it as you write. This is exactly the type of thing I do as well when I want to fully vet my understanding of something, such as in this post. Not only does this help with becoming a better writer, which is a great skill in and of itself, but it forces you to articulate a topic thoroughly, as if someone else is going to read it (which they hopefully will!). More importantly, it allows you to have permanent, easily accessible place to put your work."
  },
  {
    "objectID": "post/you-should-have-a-data-science-blog/index.html#reference-repository",
    "href": "post/you-should-have-a-data-science-blog/index.html#reference-repository",
    "title": "You should have a data science blog",
    "section": "3. Reference repository",
    "text": "3. Reference repository\nOn a related note of conceptual understanding: your blog posts are now just public web pages on the internet. That means you can use your collection of articles simply as an accessible repository for you to refer back to when you need them. If you took the time to write an article to help yourself solidify a concept (#2), then it was probably (a) tricky enough that over time you may lose your intuition on it occasionally, and (b) important enough that it was worth writing out. So, having a place where you’ve written out your thought process about a topic, in your own words, is an invaluable resource for you to refer back to. I can’t tell you the number of times I’ve referred back to articles I’ve written to remember little things. The beauty of it is that when my curiosity comes at a random part of the day, I know exactly where to look, and I can whip out my phone, remember the thing, and then stop thinking about it."
  },
  {
    "objectID": "post/you-should-have-a-data-science-blog/index.html#portfolio-to-showcase",
    "href": "post/you-should-have-a-data-science-blog/index.html#portfolio-to-showcase",
    "title": "You should have a data science blog",
    "section": "4. Portfolio to showcase",
    "text": "4. Portfolio to showcase\nThe fact that you even have a website is impressive. It shows that you can figure out how to do things, but more importantly, that you have the drive/initiative to create projects related to your craft. The skills in creating and maintaining the website are undoubtedly transferable to the work you’d do in data science. Add in interesting content you are producing in the blog posts themselves, such as code tutorials, analyses, method exploration, whatever it may be, then you have something that can prove your capabilities, and set you apart from other candidates. The convenience and security of having that all accessible through a simple web URL that you can quickly share with someone is a real advantage, as if someone asks you for work samples or something like that, you can confidently respond knowing the work was already done."
  },
  {
    "objectID": "post/you-should-have-a-data-science-blog/index.html#engaging-in-public-discourse",
    "href": "post/you-should-have-a-data-science-blog/index.html#engaging-in-public-discourse",
    "title": "You should have a data science blog",
    "section": "5. Engaging in public discourse",
    "text": "5. Engaging in public discourse\nHaving your own blog means you can write about whatever you’d like, however you want to say it, providing an avenue to contribute your own unique thoughts and perspectives. You may read a book or another article on some data science (or other) topic, and have strong thoughts, questions, or opinions about what was said. Or you just want to dig a little deeper into a certain aspect of it. Or reframe it in a way that is more understandable for yourself. Write it out and share it! Others may find your take interesting (or not, who cares). It only adds thought diversity, and makes you more connected to your field, as a peer."
  },
  {
    "objectID": "post/you-should-have-a-data-science-blog/index.html#install-software",
    "href": "post/you-should-have-a-data-science-blog/index.html#install-software",
    "title": "You should have a data science blog",
    "section": "1. Install software",
    "text": "1. Install software\nI prefer to use RStudio as my IDE for developing my websites, so you’ll also need to install R. Then, you can install Quarto and you’ll have what you need."
  },
  {
    "objectID": "post/you-should-have-a-data-science-blog/index.html#store-code-on-github",
    "href": "post/you-should-have-a-data-science-blog/index.html#store-code-on-github",
    "title": "You should have a data science blog",
    "section": "2. Store code on GitHub",
    "text": "2. Store code on GitHub\nThe source code for the blog should be version controlled and stored in a remote repository. I prefer GitHub. It’s not actually necessary to do this, but creates a much cleaner workflow and promotes better software development practices, as it retains all history to your code changes. As you develop your website locally, you’ll commit and push changes as you get to good stopping points, and GitHub will serve as your website’s source of truth."
  },
  {
    "objectID": "post/you-should-have-a-data-science-blog/index.html#host-your-site-on-netlify",
    "href": "post/you-should-have-a-data-science-blog/index.html#host-your-site-on-netlify",
    "title": "You should have a data science blog",
    "section": "3. Host your site on Netlify",
    "text": "3. Host your site on Netlify\nYour code is on GitHub, but you need a server that will actually host the live application. That is where Netlify comes in, where you can host your website for free. To make it very easy, you can configure Netlify to update your website everytime code changes are made to GitHub (#2), automatically. So when I develop my website (like this very blog post), I just commit and push my changes to GitHub (from RStudio) and that will automatically trigger Netlify to grab changes from the repository and update the live website. By default, your website URL will be something like website.netlify.app, but Netlify also has a great domain management system where you can point your website to a custom URL that you own (which you’ll likely have to pay for). Nevertheless, if you don’t mind the default URL, it is completely free."
  },
  {
    "objectID": "post/ways-to-ensure-success-of-statistical-project/index.html",
    "href": "post/ways-to-ensure-success-of-statistical-project/index.html",
    "title": "5 ways to help ensure success of a statistical project",
    "section": "",
    "text": "Sometimes stats projects don’t go as planned. There are delays, setbacks, surprises, ambiguity, scope creep…the list goes on. All of these things can lead to a seemingly longer list of questions than what was started with: Did we answer the research question? Are we confident in the result? What do we do now? It’s a sense of dissatisfaction.\nMany of these issues stem from the early phases of the project, and (maybe I should collect data on this) they can largely be alleviated when more care is taken at that stage. Actually, the book I’m currently reading summed it up perfectly, “Well-designed research is research capable of answering the question it’s trying to answer”. Sounds obtuse, but it is undoubtedly true, even for projects outside of what you may define as “research”. This is about proper planning. So here are 5 things that can help increase the likelihood of a successful statistical project:\n\n1. Assemble your team…early\nIt’s often a misconception that the statistician’s role is to simply analyze, or “run the tests on”, the data at the end once it is collected. This is far from optimal. Statistical analysis is not systematic or mechanical. Rather, it requires knowledge and intuition about the subject matter context. Add in a lack of transparency to the data collection itself, the chances of lost insight definitely increase. In fact, it might be the case that a few poor design choices end up adding huge complexities in answering the original question, or maybe even make it impossible altogether. So, if you have a project idea, consult your statistician! Early and often during the development phase.\nNow, data people are certainly not the only ones you need involved early. Far from it. Who better to help shape the final product than the end-users—the people who will actually be using the information and know what works? If you want to integrate models into your operational workflow, what sort of resource or technical constraints may there be? Well, we probably have to talk to systems and IT people who will also likely need to commit their own resources for upkeep. And it comes full circle, because all of these nuances may even affect the statistical choices made from a mathematical perspective (i.e., design, modeling framework, etc.). There are plenty more important roles that could be described here, but the bottom line is that cross-functional collaboration, from the beginning, is crucial.\n\n\n2. Make the goals clear, then plan accordingly\nThere is a common issue in data projects of ambiguous or non-specific objectives. Statistics is inherently “gray”, by definition, because there is no right answer. Uncertainty always exists. So unless you already know exactly what you’re looking for in the data, without a clearly defined goal, you can find yourself spinning in circles and never know when what you’ve done is sufficient to move on.\nMy recommendation is to have multiple levels: the statistical goals and the real-world goals that they are supporting. The statistical goals should be stated as clear, specific questions with quantitative answers that data (among other things) will be used to estimate. However, we have to think about how these statistical quantities will be used afterwards. For example, the approach taken to estimate the likelihood of a customer buying a product (a statistical goal) may differ if we’re trying to decrease costs versus increase retention, especially when thinking about the solution as a whole. The question comes down to what we are trying to accomplish with the new information. Once that is clear, we can envision the roadmap for how it will be used, which can be an anchor for developing the right methodology, making analytic decisions easier to manage.\n\n\n3. Think about taking action\nOnce you obtain the new information you set out to find, what are you going to do about it? Under what circumstances? Based on which results? Having some inkling as to what it is going to enable (or disable) someone to do–not just in general, but a specific example–adds clarity to the practical implications of investing the time and money into finding the answers. These things can unravel many of the nuances that were originally an oversight, and may end up causing changes to how the information gets disseminated, who gets involved and when, or even the math itself. All in all, it allows for more proper design at the beginning, a reduction of wasted time/resources, and a better chance of finding the right solution. The reason we perform statistical analysis is (or should be) to inform some action or decision. If it doesn’t, then you may need to think about why that is and adjust. My favorite way to frame this to someone is by asking a simple question: “If you knew X at time Y then you could do Z. What are X, Y and Z?”.\n\n\n4. Create a tangible product\nIt may sound trivial, but it’s worth thinking about (and even explicitly defining). By what means will the result or solution be delivered? To whom? When? How? Is it going to be a comprehensive report or just a number sent in an email? It might even be a model deployed in the organization’s systems and workflows, or an application hosted on the web. These tell you all sorts of things about who should get involved (see #1) and what it will take to get there. It is about ensuring that the right information gets to the right people at the right time in an expected and predictable manner. It can be the case that a data project fails not because of bad statistics or models, but because it wasn’t disseminated optimally. Having a tangible end-product that you can work towards keeps everyone’s eye on the ball, exposes where the problems are, helps you plan deliverables, create milestones, and makes it clear when you are veering off course.\n\n\n5. Answer the question, “did it work?”\nOftentimes when you look at a statistic like a p-value, it leaves an empty feeling like you haven’t been convinced. That’s because it does not yet directly translate to the real-world impact the new insight is supposed to address. Maybe it’s useful during data analysis, but we should be thinking beyond the inferences made from the sample at hand to what we need to see to convince us that the results really matter. If the information we’ve garnered is actually useful, then we should expect improvements to play out where the rubber meets the road once it is utilized. The most direct way to do this: test it."
  },
  {
    "objectID": "post/filterable-maps/index.html",
    "href": "post/filterable-maps/index.html",
    "title": "Building a filterable map with {leaflet} and {crosstalk} for hospital readmissions",
    "section": "",
    "text": "Want to make this map widget? Read On! 👇👇👇\nHospital Readmissions Reduction Program (HRRP) FY 2022 - WI Map\n\n\n\n\n\n\n\n\n\n\nPayment Reduction\n\n\n\n\n\n\n\n\n\n\n\n\nCohort-specific penalty:\n\n\n\n\n\n\n\n\n\nAMI\n\n\n\n\n\n\n\n\nCABG\n\n\n\n\n\n\n\n\nCOPD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHF\n\n\n\n\n\n\n\n\nPNEU\n\n\n\n\n\n\n\n\nTHA/TKA\nSo you run R locally but want to be able to deliver engaging, interactive analytic content to your stakeholders. Is there a way to do that? Lately, I’ve spent a lot of time learning and exploring tools to find the answer to that question. Luckily, the answer is yes, because there are many amazing packages that have been developed to help with this, including reactable, plotly, DT, leaflet, and crosstalk, among others. This article focuses on the latter two, and how we can build map widgets that are changeable by the user through filters, all in a self-contained HTML document."
  },
  {
    "objectID": "post/filterable-maps/index.html#usecasedataset",
    "href": "post/filterable-maps/index.html#usecasedataset",
    "title": "Building a filterable map with {leaflet} and {crosstalk} for hospital readmissions",
    "section": "1. Build the dataset",
    "text": "1. Build the dataset\nWe want to build a data set consisting of one row per hospital, with payment penalty information and coordinates for plotting the hospital on the map. The data needed to do this comes from a variety of sources. We can break these up into components and then put them all together into our final data set.\n\ni. Payment penalties\nThe penalty amounts for each hospital are documented in the CMS FY2022 Final Rule. The file we need is contained within a zip file; we can use this solution for motivation of our approach. First, we can download the file:\n\n\nCode\n# Set location of zip file (publicly available on CMS website)\nhrrp_zip &lt;- \"https://www.cms.gov/files/zip/fy2022-hrrp-final-rule-supplemental-file.zip\"\n\n# Create a temporary file (Credits: rpubs.com/otienodominic/398952)\ntemp_file &lt;- tempfile()\n\n# Download into the temporary file\ndownload.file(hrrp_zip, temp_file)\n\n\nNext, we can pluck out the data file we need, save it to our current working directory, and then import it into our session.\n\n\nCode\n# Name of file needed within zip\nhrrp_file &lt;- \"FY2022_Final_Rule_Supplemental_File.xlsx\"\n\n# Unzip, and place the file in the current working directory\nunzip(temp_file, hrrp_file, exdir = \".\")\n\n# Import the data file into a data frame\nhrrp_results &lt;-\n  readxl::read_xlsx(\n    path = hrrp_file,\n    sheet = \"FR FY 2022\",\n    skip = 3,\n    na = c(\"\", \" \", \".\", \"-\", \"NA\", \"N/A\")\n  )\n\n# Delete the downloaded file\nfile.remove(hrrp_file)\nunlink(temp_file)\n\n\n\n\nCode\nprint(hrrp_results, n = 5)\n\n\n# A tibble: 3,048 × 36\n  `Hospital CCN` Payment adjustment f…¹ Payment Reduction Pe…² `Dual proportion`\n  &lt;chr&gt;                           &lt;dbl&gt;                  &lt;dbl&gt;             &lt;dbl&gt;\n1 010001                          0.990                   0.97             0.163\n2 010005                          0.998                   0.15             0.141\n3 010006                          0.984                   1.6              0.107\n4 010007                          0.995                   0.51             0.235\n5 010008                          1                       0                0.227\n# ℹ 3,043 more rows\n# ℹ abbreviated names: ¹​`Payment adjustment factor`,\n#   ²​`Payment Reduction Percentage`\n# ℹ 32 more variables: `Peer group assignment` &lt;dbl&gt;,\n#   `Neutrality modifier` &lt;dbl&gt;, `Number of eligible discharges for AMI` &lt;dbl&gt;,\n#   `ERR for AMI` &lt;dbl&gt;, `Peer group median ERR for AMI` &lt;dbl&gt;,\n#   `Penalty indicator for AMI` &lt;chr&gt;, `DRG payment ratio for AMI` &lt;dbl&gt;, …\n\n\n\n\nii. Hospital information\nWe need some identifying information for the hospitals such as name and zip code (this is what we’ll use for plotting). This is found on data.cms.gov, which we can import from directly:\n\n\nCode\n# Set path to file (from CMS)\nhosp_file &lt;- \"https://data.cms.gov/provider-data/sites/default/files/resources/092256becd267d9eeccf73bf7d16c46b_1689206722/Hospital_General_Information.csv\"\n\n# Import the file\nhosp_info &lt;-\n  read_csv(\n    file = hosp_file\n  )\n\n\n\n\nCode\nprint(hosp_info, n = 5)\n\n\n# A tibble: 5,394 × 39\n  `Facility ID` `Facility Name`             Address `City/Town` State `ZIP Code`\n  &lt;chr&gt;         &lt;chr&gt;                       &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;     \n1 010001        SOUTHEAST HEALTH MEDICAL C… 1108 R… DOTHAN      AL    36301     \n2 010005        MARSHALL MEDICAL CENTERS    2505 U… BOAZ        AL    35957     \n3 010006        NORTH ALABAMA MEDICAL CENT… 1701 V… FLORENCE    AL    35630     \n4 010007        MIZELL MEMORIAL HOSPITAL    702 N … OPP         AL    36467     \n5 010008        CRENSHAW COMMUNITY HOSPITAL 101 HO… LUVERNE     AL    36049     \n# ℹ 5,389 more rows\n# ℹ 33 more variables: `County/Parish` &lt;chr&gt;, `Telephone Number` &lt;chr&gt;,\n#   `Hospital Type` &lt;chr&gt;, `Hospital Ownership` &lt;chr&gt;,\n#   `Emergency Services` &lt;chr&gt;,\n#   `Meets criteria for promoting interoperability of EHRs` &lt;chr&gt;,\n#   `Meets criteria for birthing friendly designation` &lt;chr&gt;,\n#   `Hospital overall rating` &lt;chr&gt;, …\n\n\n\n\niii. Zip code centroids\nFor simplicity, we are going to use the hospital’s zip code for plotting them on the map. The tigris package provides access to the geometries we need:\n\n\nCode\n# Get set of zip codes for Wisconsin\nzips_wi &lt;- tigris::zctas(year = 2010, state = \"WI\")\n\n\nA zip code is a region, so we’ll need to reduce those down to a single set of coordinates. To do this, we can use the centroid of the region. The sf package has readily-available functions for this.\n\n\nCode\n# Gather centroids of corrdinates for each zip\nzips_wi_centroids &lt;-\n  zips_wi %&gt;%\n  \n  # Get the centroid\n  sf::st_centroid() %&gt;%\n  \n  # Pluck the coordinates\n  sf::st_coordinates() %&gt;%\n  \n  # Make a tibble\n  as_tibble() %&gt;%\n  \n  # Add identifying column\n  add_column(\n    Zip = zips_wi$ZCTA5CE10\n  ) %&gt;%\n  \n  # Rename columns\n  rename(\n    lon = X,\n    lat = Y\n  )\n\n\n\n\nCode\nprint(zips_wi_centroids, n = 5)\n\n\n# A tibble: 774 × 3\n    lon   lat Zip  \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 -89.8  43.7 53965\n2 -89.6  44.1 54943\n3 -89.5  44.2 54966\n4 -90.7  46.3 54546\n5 -90.5  46.5 54559\n# ℹ 769 more rows\n\n\n\n\niv. Build map data set\nFinally, we can clean and combine the previously loaded datasets, extracting the fields we need. First, we’ll filter to Wisconsin hospitals:\n\n\nCode\nmap_data &lt;-\n  hosp_info %&gt;%\n  \n  # Filter to Wisconsin hospitals\n  filter(\n    State == \"WI\"\n  ) %&gt;%\n  \n  # Keep a few pieces of information\n  select(\n    FacilityID = `Facility ID`,\n    FacilityName = `Facility Name`,\n    Zip = `ZIP Code`\n  )\n\n\nNext, we can add the zip code centroid coordinates for the hospital. One additional step we’ll take is to add a small amount of random noise to the coordinates. This will prevent hospitals that share the same zip code from being plotted directly on top of one another.\n\n\nCode\nmap_data &lt;-\n  map_data %&gt;%\n  \n  # Join to get the centroid for the hospital's zip code\n  inner_join(\n    y = zips_wi_centroids,\n    by = \"Zip\"\n  ) %&gt;%\n  \n  # Add random jitter to coordinates\n  mutate(\n    across(\n      c(\n        lat,\n        lon\n      ),\n      jitter,\n      amount = 0.05\n    )\n  )\n\n\nThe penalty information can be added next. We’ll add the:\n\nPenalty amount: The percentage in which reimbursements are reduced by due to excess readmissions\nDual-eligibility proportion: The percent of discharges in which the patient was dually-eligible for both Medicare and Medicaid benefits\nPeer group: The group that the hospital was compared against to determine penalty status. This is based on the dual-eligibility proportion.\nCohort-specific penalty indicators: A flag of whether or not the hospital was penalized in each of the six (6) focus cohorts\n\nWe’ll do a little clean-up on the cohort indicators for nicer names and presentation.\n\n\nCode\nmap_data &lt;-\n  map_data %&gt;%\n  \n  # Join to get HRRP program results\n  inner_join(\n    y = \n      hrrp_results %&gt;%\n      \n      # Make cleaner levels for penalty indicators\n      mutate(\n        across(\n          starts_with(\"Penalty indicator\"),\n          ~\n            case_when(\n              .x == \"Y\" ~ \"Yes\",\n              .x == \"N\" ~ \"No\",\n              TRUE ~ NA_character_\n            )\n        )\n      ) %&gt;%\n      \n      # Keep a few columns\n      select(\n        FacilityID = `Hospital CCN`,\n        PeerGroup = `Peer group assignment`,\n        DualProportion = `Dual proportion`,\n        Penalty = `Payment Reduction Percentage`,\n        starts_with(\"Penalty indicator\")\n      ) %&gt;%\n      \n      # Remove the prefix from the cohort columns\n      rename_with(\n        ~str_remove(.x, \"^Penalty indicator for \"),\n        starts_with(\"Penalty indicator\")\n      ),\n    by = \"FacilityID\"\n  )\n\n\nFinally, we’ll add a string that concatenates all of the cohorts that the hospital received penalty for. This will be used as a display for the tooltip in the map.\n\n\nCode\nmap_data &lt;-\n  map_data %&gt;%\n  \n  # Join to get list of penalized cohorts (for labeling)\n  left_join(\n    y = \n      hrrp_results %&gt;%\n      \n      # Send down the rows\n      pivot_longer(\n        cols = starts_with(\"Penalty indicator\"),\n        names_prefix = \"Penalty indicator for \"\n      ) %&gt;%\n      \n      # Filter to penalty cohorts\n      filter(\n        value == \"Y\"\n      ) %&gt;%\n      \n      # For each hospital\n      group_by(`Hospital CCN`) %&gt;%\n      \n      # Concatenate the list of penalty cohorts\n      summarise(\n        PenalizedCohortCount = n(),\n        PenalizedCohorts = paste(sort(unique(name)), collapse = \", \"),\n        .groups = \"drop\"\n      ) %&gt;%\n      \n      # Rename the column\n      rename(\n        FacilityID = `Hospital CCN`\n      ),\n    by = \"FacilityID\"\n  ) %&gt;%\n  \n  # Fill in non-penalized hospitals\n  mutate(\n    PenalizedCohortCount = coalesce(PenalizedCohortCount, 0),\n    PenalizedCohorts = coalesce(PenalizedCohorts, \"\")\n  ) \n\n\n\n\nCode\nprint(map_data, n = 5)\n\n\n# A tibble: 59 × 16\n  FacilityID FacilityName     Zip     lon   lat PeerGroup DualProportion Penalty\n  &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 520002     ASPIRUS STEVENS… 54481 -89.7  44.5         4          0.262    0.12\n2 520004     MAYO CLINIC HEA… 54601 -91.2  43.8         4          0.236    0.81\n3 520008     WAUKESHA MEMORI… 53188 -88.2  43.0         2          0.139    0.31\n4 520009     ASCENSION NE WI… 54915 -88.4  44.2         3          0.201    0.08\n5 520011     MARSHFIELD MEDI… 54868 -91.7  45.5         4          0.259    0.37\n# ℹ 54 more rows\n# ℹ 8 more variables: AMI &lt;chr&gt;, COPD &lt;chr&gt;, HF &lt;chr&gt;, pneumonia &lt;chr&gt;,\n#   CABG &lt;chr&gt;, `THA/TKA` &lt;chr&gt;, PenalizedCohortCount &lt;dbl&gt;,\n#   PenalizedCohorts &lt;chr&gt;"
  },
  {
    "objectID": "post/filterable-maps/index.html#shareddata",
    "href": "post/filterable-maps/index.html#shareddata",
    "title": "Building a filterable map with {leaflet} and {crosstalk} for hospital readmissions",
    "section": "2. Create a sharable dataset",
    "text": "2. Create a sharable dataset\nIn order to enable the map to be controlled and interacted through filters, we need to create a SharedData object using the crosstalk package. This allows the rendered HTML document to hold a connection between the data that feeds the map and the filters.\n\n\nCode\n# Make the shared data set\nsd &lt;- SharedData$new(data = map_data)"
  },
  {
    "objectID": "post/filterable-maps/index.html#baselayer",
    "href": "post/filterable-maps/index.html#baselayer",
    "title": "Building a filterable map with {leaflet} and {crosstalk} for hospital readmissions",
    "section": "3. Create the base layer",
    "text": "3. Create the base layer\nThe first layer of the map will consist of making a focal point on the state of Wisconsin. We will use the maps package to obtain the geometries for this region.\n\n\nCode\nstate_outline &lt;-\n  maps::map(\n    database = \"state\",\n    regions = \"wisconsin\",\n    fill = TRUE,\n    plot = FALSE\n  )\n\n\nNow, we can initiate the map with the leaflet toolkit and add the first layer with a simple gray shading.\n\n\nCode\nmy_map &lt;- \n  leaflet() %&gt;%\n  \n  # Add geographic tiles\n  addTiles() %&gt;%\n  \n  # Add WI state outline\n  addPolygons(\n    data = state_outline,\n    fillColor = \"gray\",\n    stroke = FALSE\n  ) \n\nmy_map"
  },
  {
    "objectID": "post/filterable-maps/index.html#countylines",
    "href": "post/filterable-maps/index.html#countylines",
    "title": "Building a filterable map with {leaflet} and {crosstalk} for hospital readmissions",
    "section": "4. Add county lines",
    "text": "4. Add county lines\nNext, we want to fill in the map with the Wisconsin county lines. The tigris package has a function called counties that enable us to extract these geometries.\n\n\nCode\ncounty_outlines &lt;- \n  tigris::counties(cb = TRUE) %&gt;%\n  filter(\n    STATE_NAME == \"Wisconsin\"\n  )\n\n\nWe can add the county lines to the current map by supplying another layer with addPolygons. The highlightOptions argument allows us to control how a county appears when hovered over, and the label argument builds strings for tooltip display with column references to the input dataset.\n\n\nCode\nmy_map &lt;-\n  my_map %&gt;%\n  \n  # Add county outlines\n  addPolygons(\n    data = county_outlines,\n    color = \"black\",\n    fillColor = \"#ff59f7\",\n    weight = 1,\n    opacity = .5,\n    fillOpacity = .35,\n    highlightOptions = \n      highlightOptions(\n        color = \"black\",\n        weight = 3,\n        bringToFront = FALSE\n      ),\n    label = ~NAME\n  )\n\nmy_map"
  },
  {
    "objectID": "post/filterable-maps/index.html#datapoints",
    "href": "post/filterable-maps/index.html#datapoints",
    "title": "Building a filterable map with {leaflet} and {crosstalk} for hospital readmissions",
    "section": "5. Show data points",
    "text": "5. Show data points\nThe data points representing the hospitals can be added as yet another layer to the map. We want the color of the data points to reflect the amount of payment penalty the hospital received, so we’ll first create a color palette ranging from 0% (no penalty) to 3% (maximum penalty).\n\n\nCode\n# Create a color pallete\npal &lt;- \n  colorNumeric(\n    palette = \"RdYlGn\",\n    domain = -1*seq(0,3,.1)\n  )\n\n\nThen we will use the shared data that was created in a prior section to add the map layer with addCircleMarkers.\n\n\nCode\n# Add shared data to the map\nmy_map %&gt;%\n  \n  addCircleMarkers(\n    data = sd, \n    lng = ~lon,\n    lat = ~lat,\n    label = ~paste0(FacilityName, \" (click for info)\"),\n    popup = \n      ~paste0(\n        \"Hospital: \", FacilityName, \n        \"&lt;br&gt;Zip Code: \", Zip,\n        \"&lt;br&gt;Peer Group: \", PeerGroup,\n        \"&lt;br&gt;Dual-Eligibility: \", round(DualProportion*100, 2), \"%\",\n        \"&lt;br&gt;Penalties: \", PenalizedCohorts,\n        \"&lt;br&gt;Payment Reduction: \", Penalty, \"%\"\n      ),\n    color = ~pal(-1*Penalty),\n    fillOpacity = .75\n  )"
  },
  {
    "objectID": "post/filterable-maps/index.html#filters",
    "href": "post/filterable-maps/index.html#filters",
    "title": "Building a filterable map with {leaflet} and {crosstalk} for hospital readmissions",
    "section": "6. Make filters",
    "text": "6. Make filters\nA row of filters (and/or other objects) can be built from a call to the bscols function in the crosstalk package. This enables us to align objects in a grid across columns within rows. The first row is the widget title/header, which is just styled HTML text:\n\n\nCode\n# Header\nbscols(\n  widths = c(1, 11),\n  \"\",\n  htmltools::h3(\"Hospital Readmissions Reduction Program (HRRP) FY 2022 - WI Map\", style = \"font-style:italic;\")\n)\n\n\n\n\n\n\nHospital Readmissions Reduction Program (HRRP) FY 2022 - WI Map\n\n\n\n\n\nThe next row is the slider selector for filtering the penalty amount. This is built with the filter_slider function. We feed it the SharedData object previously created so that changes to the filter trigger changes to the map, which has also been supplied with the same object.\n\n\nCode\n# Penalty filter\nbscols(\n  widths = c(1, 10, 1),\n  \"\",\n  filter_slider(\n    id = \"penalty\",\n    label = \"Payment Reduction\",\n    sharedData = sd,\n    column = ~Penalty,\n    step = 0.1,\n    min = 0,\n    max = 3,\n    post = \"%\"\n  ),\n  \"\"\n)\n\n\n\n\n\n\n\nPayment Reduction\n\n\n\n\n\n\n\n\n\nSimilarly, the next set of rows consist of a sub-header to title the cluster of filters, and two (2) rows of dropdown selectors for each of the six (6) cohort indicators, which are analogously built with calls to filter_select:\n\n\nCode\n# Sub-header for cohort filters\nbscols(\n  widths = c(1, 11),\n  \"\",\n  \"Cohort-specific penalty:\"\n)\n\n\n\n\n\nCohort-specific penalty:\n\n\n\n\nCode\n# Cohort filters: Row 1\nbscols(\n  widths = c(1, 3, 3, 3, 1),\n  \"\",\n  filter_select(\n    id = \"ami\",\n    label = \"AMI\",\n    sharedData = sd,\n    group = ~AMI\n  ),\n  filter_select(\n    id = \"cabg\",\n    label = \"CABG\",\n    sharedData = sd,\n    group = ~CABG\n  ),\n  filter_select(\n    id = \"copd\",\n    label = \"COPD\",\n    sharedData = sd,\n    group = ~COPD\n  ),\n  \"\"\n)\n\n\n\n\n\n\n\nAMI\n\n\n\n\n\n\n\n\nCABG\n\n\n\n\n\n\n\n\nCOPD\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Cohort filters: Row 2\nbscols(\n  widths = c(1, 3, 3, 3, 1),\n  \"\",\n  filter_select(\n    id = \"hf\",\n    label = \"HF\",\n    sharedData = sd,\n    group = ~HF\n  ),\n  filter_select(\n    id = \"pneu\",\n    label = \"PNEU\",\n    sharedData = sd,\n    group = ~pneumonia\n  ),\n  filter_select(\n    id = \"tja\",\n    label = \"THA/TKA\",\n    sharedData = sd,\n    group = ~`THA/TKA`\n  ),\n  \"\"\n)\n\n\n\n\n\n\n\nHF\n\n\n\n\n\n\n\n\nPNEU\n\n\n\n\n\n\n\n\nTHA/TKA"
  },
  {
    "objectID": "post/filterable-maps/index.html#formatorder",
    "href": "post/filterable-maps/index.html#formatorder",
    "title": "Building a filterable map with {leaflet} and {crosstalk} for hospital readmissions",
    "section": "7. Format order of appearance",
    "text": "7. Format order of appearance\nNotice that when you adjust the filters in the last section, they change the map from step 5. This is because they are all referencing the same SharedData object. So, our final step is that we simply need to rearrange the execution order of the code so that the objects appear in the relative positions we would like. The code chunk creating the following output is a copy-paste of what we executed in steps 2-6, just with some rearranging. We will set echo = FALSE in the code chunk options so that only the output renders together. Note that we created a different SharedData object here so that the filters in the prior steps don’t impact this output.\n\n\n\n\n\n\nHospital Readmissions Reduction Program (HRRP) FY 2022 - WI Map\n\n\n\n\n\n\n\n\n\n\nPayment Reduction\n\n\n\n\n\n\n\n\n\n\n\n\nCohort-specific penalty:\n\n\n\n\n\n\n\n\n\nAMI\n\n\n\n\n\n\n\n\nCABG\n\n\n\n\n\n\n\n\nCOPD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHF\n\n\n\n\n\n\n\n\nPNEU\n\n\n\n\n\n\n\n\nTHA/TKA"
  },
  {
    "objectID": "post/reactable-javascript/index.html",
    "href": "post/reactable-javascript/index.html",
    "title": "A couple useful JavaScript aggregation and formatting functions for {reactable}",
    "section": "",
    "text": "In my last post we demonstrated how to build a filterable map widget into an HTML output that was free of R runtime dependency. In continuation of that theme, this article focuses on the reactable package, which enables you to embed interactive data tables into your document. Specifically, we’ll look at how we can use its built-in JavaScript interface to define custom functions for commonly-desired aggregation and formatting."
  },
  {
    "objectID": "post/reactable-javascript/index.html#summarydata",
    "href": "post/reactable-javascript/index.html#summarydata",
    "title": "A couple useful JavaScript aggregation and formatting functions for {reactable}",
    "section": "Make a summary dataset",
    "text": "Make a summary dataset\nFirst we’ll need to create a data frame with group-level summary statistics that we want to display. For this example, we’ll look at the 30-day hospital readmission rate for heart failure patients on Medicare at the top five (5) most voluminous hospitals in a handful of Midwest states.\n\n\nCode\n# Import dataset\nreadmission_rates &lt;-\n  read_csv(\n    file = \"/Users/alexzajichek/Documents/GitHub/Unplanned_Hospital_Visits-Hospital.csv\",\n    na = c(\"\", \" \", \"NA\", \"N/A\", \"Not Available\")\n  ) %&gt;%\n  \n  # Filter to states with non-null values\n  filter(\n    State %in% c(\"WI\", \"MN\", \"MI\", \"IL\"),\n    `Measure ID` == \"READM_30_HF\",\n    !is.na(Denominator),\n    !is.na(Score)\n  ) %&gt;%\n  \n  # Convert to proportion\n  mutate(\n    Score = Score / 100\n  ) %&gt;%\n  \n  # Keep a few columns\n  select(\n    State,\n    Hospital = `Facility Name`,\n    Cases = Denominator,\n    `30-Day Readmission Rate` = Score\n  ) %&gt;%\n  \n  # For each state, keep the top 5 most voluminous hospitals\n  group_by(State) %&gt;%\n  slice_max(\n    n = 5,\n    order_by = Cases,\n    with_ties = FALSE\n  ) %&gt;%\n  ungroup()\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 66906 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (15): Facility ID, Facility Name, Address, City/Town, State, ZIP Code, C...\ndbl  (5): Denominator, Score, Lower Estimate, Higher Estimate, Footnote\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nreadmission_rates\n\n\n# A tibble: 20 × 4\n   State Hospital                                   Cases 30-Day Readmission R…¹\n   &lt;chr&gt; &lt;chr&gt;                                      &lt;dbl&gt;                  &lt;dbl&gt;\n 1 IL    NORTHSHORE UNIVERSITY HEALTHSYSTEM - EVAN…  2294                  0.212\n 2 IL    PALOS COMMUNITY HOSPITAL                    1436                  0.214\n 3 IL    NORTHWESTERN MEMORIAL HOSPITAL              1315                  0.187\n 4 IL    ADVOCATE CHRIST HOSPITAL & MEDICAL CENTER   1287                  0.202\n 5 IL    NORTHWESTERN MEDICINE MCHENRY               1286                  0.177\n 6 MI    BEAUMONT HOSPITAL, TROY                     1683                  0.21 \n 7 MI    BEAUMONT HOSPITAL ROYAL OAK                 1611                  0.2  \n 8 MI    ASCENSION PROVIDENCE HOSPITAL, SOUTHFIELD…  1191                  0.206\n 9 MI    TRINITY HEALTH ANN ARBOR HOSPITAL           1121                  0.191\n10 MI    SPECTRUM HEALTH                             1065                  0.172\n11 MN    MAYO CLINIC HOSPITAL ROCHESTER              1551                  0.173\n12 MN    ABBOTT NORTHWESTERN HOSPITAL                 778                  0.189\n13 MN    PARK NICOLLET METHODIST HOSPITAL             747                  0.213\n14 MN    MERCY HOSPITAL                               742                  0.176\n15 MN    ST CLOUD HOSPITAL                            623                  0.18 \n16 WI    AURORA ST LUKES MEDICAL CENTER              1362                  0.188\n17 WI    FROEDTERT MEMORIAL LUTHERAN HOSPITAL         621                  0.195\n18 WI    UNIVERSITY OF WI  HOSPITALS & CLINICS AUT…   609                  0.219\n19 WI    WAUKESHA MEMORIAL HOSPITAL                   605                  0.171\n20 WI    MILWAUKEE VA MEDICAL CENTER                  604                  0.222\n# ℹ abbreviated name: ¹​`30-Day Readmission Rate`"
  },
  {
    "objectID": "post/reactable-javascript/index.html#defaulttable",
    "href": "post/reactable-javascript/index.html#defaulttable",
    "title": "A couple useful JavaScript aggregation and formatting functions for {reactable}",
    "section": "The default table",
    "text": "The default table\nAs a starting point, let’s see what we get when we call the reactable function without any additional arguments.\n\n\nCode\nreadmission_rates %&gt;% reactable()\n\n\n\n\n\n\nWe get about what we’d expect: a basic, paginated table where each row from our dataset is represented verbatim. There are many things we could clean up here such as rounding, number representations, formatting, and, of course, aggregating the statistics to get state-specific readmission rates."
  },
  {
    "objectID": "post/reactable-javascript/index.html#builtinaggregation",
    "href": "post/reactable-javascript/index.html#builtinaggregation",
    "title": "A couple useful JavaScript aggregation and formatting functions for {reactable}",
    "section": "Built-in aggregation",
    "text": "Built-in aggregation\nThere are a number of built-in aggregation functions available to us by default. We just need to specify:\n\nThe groups we want the aggregation applied within using the groupBy argument\nThe columns we want to aggregate, and how, using colDef within the columns argument\n\nLet’s add functionality to the table above to aggregate the total case count and the average 30-day readmission rate within each state.\n\n\nCode\nreadmission_rates %&gt;%\n  reactable(\n    groupBy = \"State\",\n    columns = \n      list(\n        Cases = colDef(aggregate = \"sum\"),\n        `30-Day Readmission Rate` = colDef(aggregate = \"mean\")\n      )\n  )\n\n\n\n\n\n\nThe problem with this table is that the displayed state-level readmission rates represent the averaged rates across the individual hospitals. What we really want in the aggregation is for the hospital-specific rates to be weighted by their respective case volumes so that the state-level readmission rates are correct. This is where JavaScript comes in."
  },
  {
    "objectID": "post/reactable-javascript/index.html#function1",
    "href": "post/reactable-javascript/index.html#function1",
    "title": "A couple useful JavaScript aggregation and formatting functions for {reactable}",
    "section": "Function 1: Weighted mean",
    "text": "Function 1: Weighted mean\nReferring back to our example, we want to average the 30-day readmission rates over the hospitals within each state, but we need to weight them by their respective case volume. To do this, we can supply a custom JavaScript function to the aggregate argument. This function takes the entire vector of values within the group as its argument, as well as the group’s rows, and returns a scalar value. We can specify the column containing the case weights directly by name within the row.\n\n\nCode\nweighted_mean &lt;-\n  function(weight) {\n    JS(\n      paste0(\n        \"function(values, rows) {\n          var numerator = 0\n          var denominator = 0\n    \n          rows.forEach(function(row, index) {\n            numerator += row['\", weight, \"'] * values[index]\n            denominator += row['\", weight, \"']\n          })\n    \n          return numerator / denominator\n  \n        }\"\n      )\n    )\n  }\n\n\nIn our implementation, we encoded the JS function into an R function that calls for the weight column name so we can use it in any reactable in which we want this functionality applied. Let’s see what our table looks when we plug it in:\n\n\nCode\nreadmission_rates %&gt;%\n  reactable(\n    groupBy = \"State\",\n    columns = \n      list(\n        Cases = colDef(aggregate = \"sum\"),\n        `30-Day Readmission Rate` = colDef(aggregate = weighted_mean(weight = \"Cases\"))\n      )\n  )\n\n\n\n\n\n\nNow we have correctly calculated the state-level 30-day readmission rates."
  },
  {
    "objectID": "post/reactable-javascript/index.html#function2",
    "href": "post/reactable-javascript/index.html#function2",
    "title": "A couple useful JavaScript aggregation and formatting functions for {reactable}",
    "section": "Function 2: Conditional coloring",
    "text": "Function 2: Conditional coloring\nSuppose we would like to differentiate readmission rates in our table that are above or below the national average. To do this, we can define another JavaScript function and supply it to the style argument within colDef. First, lets pull in the comparison value.\n\n\nCode\nnational_rate &lt;-\n  read_csv(\n    file = \"/Users/alexzajichek/Documents/GitHub/Unplanned_Hospital_Visits-National.csv\",\n    na = c(\"Not Available\", \"Not Applicable\")\n  ) %&gt;%\n  \n  # Filter to the measure\n  filter(\n    `Measure ID` == \"READM_30_HF\"\n  ) %&gt;%\n  \n  # Pull the rate\n  pull(`National Rate`)\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 14 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Measure ID, Measure Name, Start Date, End Date\ndbl (9): National Rate, Number of Hospitals Worse, Number of Hospitals Same,...\nlgl (1): Footnote\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nnational_rate &lt;- national_rate / 100\nnational_rate\n\n\n[1] 0.198\n\n\nNext, lets define another JS function wrapped in an R function that takes the column to evaluate, the comparison value, and the colors to fill in the table cell when it is above or below (or the same as) that threshold. These arguments will feed into the JavaScript function that applies the specified HTML styling to each value in the column.\n\n\nCode\nconditional_coloring &lt;-\n  function(column, threshold, color_above, color_below, color_same = \"#fcfffd\") {\n    JS(\n      paste0(\n        \"function(rowInfo) {\n          var value = rowInfo.row['\", column, \"']\n    \n          if(value &gt; \", threshold, \") {\n            var color = '\", color_above, \"'\n          } else if(value &lt; \", threshold, \") {\n            var color = '\", color_below, \"'\n          } else {\n            var color = '\", color_same, \"'\n          }\n    \n          return {background:color}\n        }\"\n      )\n    )\n  }\n\n\nWe can now add it to the table:\n\n\nCode\nreadmission_rates %&gt;%\n  reactable(\n    groupBy = \"State\",\n    columns = \n      list(\n        Cases = colDef(aggregate = \"sum\"),\n        `30-Day Readmission Rate` = \n          colDef(\n            aggregate = weighted_mean(weight = \"Cases\"),\n            style = \n              conditional_coloring(\n                column = \"30-Day Readmission Rate\",\n                threshold = national_rate,\n                color_above = \"#eb7554\",\n                color_below = \"#54a637\"\n              )\n          )\n      )\n  )\n\n\n\n\n\n\nNote that the national 30-day readmission rate for heart failure patients on Medicare was 19.8%."
  },
  {
    "objectID": "post/reactable-javascript/index.html#finaltouches",
    "href": "post/reactable-javascript/index.html#finaltouches",
    "title": "A couple useful JavaScript aggregation and formatting functions for {reactable}",
    "section": "Final touches",
    "text": "Final touches\nFinally, let’s add a few finishing touches to really make the table pop.\n\n\nCode\nreadmission_rates %&gt;%\n  reactable(\n    groupBy = \"State\",\n    columns = \n      list(\n        Cases = colDef(aggregate = \"sum\"),\n        `30-Day Readmission Rate` = \n          colDef(\n            aggregate = weighted_mean(weight = \"Cases\"),\n            style = \n              conditional_coloring(\n                column = \"30-Day Readmission Rate\",\n                threshold = national_rate,\n                color_above = \"#eb7554\",\n                color_below = \"#54a637\"\n              ),\n            format = colFormat(digits = 2, percent = TRUE)\n          )\n      ),\n    striped = TRUE,\n    highlight = TRUE,\n    bordered = TRUE,\n    resizable = TRUE,\n    theme = reactablefmtr::sandstone()\n  )"
  },
  {
    "objectID": "post/quantum-entanglement-from-statistical-perspective/index.html",
    "href": "post/quantum-entanglement-from-statistical-perspective/index.html",
    "title": "Quantum entanglement from a statistician’s perspective",
    "section": "",
    "text": "Quantum entanglement is an intimidating phrase to encounter when you barely know what quantum means (and maybe it is even if you do). My daughter’s book, Quantum Entanglement for Babies, also does a good job of keeping the mystery alive:\n\nNow I’ve just barely scratched the surface in quantum computing (and I mean barely, like I’ve gotten so far as to understand how to build a circuit to add two bits together. Yes, 1 + 1 = 2). But as I was going through the section on quantum entanglement in this tutorial, I immediately noticed something familiar that it was getting at (albeit in an unfamiliar, roundabout way). And that was statistical independence.\n\nSome background\nWe can represent the state of qubits (like a bit, but in quantum), at a given point in time, as state vectors, which (loosely) correspond to the probability they will be measured in a particular state.\nFor example, suppose we have a qubit, \\(q_0\\), that has the following state vector:\n\\[q_0 = |0\\rangle = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ \\end{array}\\right]\\]\nThe positions of the vector represent the possible states the qubit can be in. Namely, since it’s basically just a bit, 0 (position 1) or 1 (position 2). The entries in the vector represent (again, loosely) the probability that the qubit will take on that state when measured. So in this example,\n\\[P(q_0 = 0) = 1 \\hskip.1in P(q_0=1)=0\\] It will always be measured in the 0 state.\nNow suppose we introduce another qubit, \\(q_1\\). And remember, computers just store information as sequences of bits. This qubit can also only be measured in states 0 or 1. Thus, the possible bit strings are:\n\n\n\n\\(q_1q_0\\)\nRepresents the number…\n\n\n\n\n00\n\\((0\\times2^1) + (0 \\times 2^0) = 0\\)\n\n\n01\n\\((0\\times2^1) + (1 \\times 2^0) = 1\\)\n\n\n10\n\\((1\\times2^1) + (0 \\times 2^0) = 2\\)\n\n\n11\n\\((1\\times2^1) + (1 \\times 2^0) = 3\\)\n\n\n\nSo one possible two-qubit state vector is:\n\\[|01\\rangle = \\left[\\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ \\end{array}\\right]\\]\nwhere, again, the positions represent the possible sequences of qubits (00, 01, 10, 11; there will always be \\(2^n\\) possible states, where \\(n\\) is the number of qubits), and the entries (for the third time, loosely) represent the probability of measuring that sequence. In this case,\n\\[P(q_0 = 1 \\cap q_1 = 0) = 1; \\hskip.1in P(\\text{other combos}) = 0\\] So now we can imagine the more interesting case where more than one entry is non-zero, that is, multiple different states have a positive probability of being measured. Given it still has to lead to a valid probability distribution, this means that the 100% must be distributed amongst the possibilities.\nThe final thing I’ll leave here is that the entries actually represent the square root of the probability, which is why I’ve been emphasizing probability “loosely”. So the “valid probability distribution” constraint applies to the square of the vector entries. In the first example above, a more complete way to write this would be:\n\\[P(q_0 = 0) = 1^2 = 1 \\hskip.1in P(q_0=1) = 0^2 = 0\\]\n\n\nWhat is entanglement?\nThe tutorial has us consider a couple of two-qubit state vectors:\n\\[|\\Phi^+\\rangle = \\frac{1}{\\sqrt{2}} \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ \\end{array}\\right] \\hskip.2in |+0\\rangle = \\frac{1}{\\sqrt{2}} \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\end{array}\\right]\\]\nIf we let \\(X = q_1q_0\\), that is, the bit string measured from the qubits, these imply the following:\n\\[P_{|\\Phi^+\\rangle}(X = 00) = P_{|\\Phi^+\\rangle}(X = 11) = \\frac{1}{2}\\]\n\\[P_{|+0\\rangle}(X = 00) = P_{|+0\\rangle}(X = 10) = \\frac{1}{2}\\]\nNotice how both bits change in \\(|\\Phi^+\\rangle\\), but only one changes in \\(|+0\\rangle\\). The former is entangled, the latter is not. This is because we cannot separate \\(|\\Phi^+\\rangle\\) into superpositions of two individual, one-qubit state vectors. But in \\(|+0\\rangle\\), we can:\n\\[q_0 = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ \\end{array}\\right] = |0\\rangle\\] \\[q_1 = \\frac{1}{\\sqrt{2}} \\left[\\begin{array}{c} 1 \\\\ 1 \\\\ \\end{array}\\right] = |+\\rangle\\]\nImplying that \\(q_0\\) will always be measured to 0, and all uncertainty (random variability) lies in measuring \\(q_1\\). This is known as a product state, because the probabilities in the two-qubit state vector can be determined by a cross-product of the individual ones.\n\n\nIt’s just independence\nStatistical independence occurs when the probability of observing an event does not change once we know something about another one. In our case, we can pretty clearly see this holds for \\(|+0\\rangle\\) but not \\(|\\Phi^+\\rangle\\). Let’s look at the latter case.\nFrom the the two-qubit state vector, we know the possible measurements are 00 or 11. Thus,\n\\[P_{|\\Phi^+\\rangle}(q_0 = 0) = P_{|\\Phi^+\\rangle}(q_0 = 1) = \\frac{1}{2}\\] \\[P_{|\\Phi^+\\rangle}(q_1 = 0) = P_{|\\Phi^+\\rangle}(q_1 = 1) = \\frac{1}{2}\\]\nMarginally, each qubit has an equal chance of being measured 0 or 1. But once we know something about the state of the other qubit, this changes:\n\\[P_{|\\Phi^+\\rangle}(q_1 = 0|q_0 = 0) = 1\\] \\[P_{|\\Phi^+\\rangle}(q_1 = 0|q_0 = 1) = 0\\] \\[P_{|\\Phi^+\\rangle}(q_1 = 1|q_0 = 0) = 0\\] \\[P_{|\\Phi^+\\rangle}(q_1 = 1|q_0 = 1) = 1\\]\nWe could flip those around and condition \\(q_0\\) on \\(q_1\\) and we’d end up with the same result. What this shows is that in the entangled state,\n\\[P(q_0|q_1) \\neq P(q_0)\\] implying\n\\[P(q_0 \\cap q_1) \\neq P(q_0)P(q_1)\\]\nand therefore are not independent. Once we know (measure) one qubit, we automatically know what the other one will be. If you go through the same math for \\(|+0\\rangle\\), you’ll see the marginal and conditional probabilities are in fact equal, and thus independent.\nNow I don’t know if/how this might change once you start introducing more qubits or allow for the full range of phase, but to keep things intuitive, my working definition of quantum entanglement is:\nDoes the probability of a qubit being measured to a particular state depend on the state of another qubit? If yes, they are entangled; otherwise, they are not."
  },
  {
    "objectID": "post/simple-example-why-statistical-significance-is-insufficient/index.html",
    "href": "post/simple-example-why-statistical-significance-is-insufficient/index.html",
    "title": "A simple example why statistical significance is insufficient for action",
    "section": "",
    "text": "When we see the phrase statistically significant, we’re often meant to believe it means that the result matters, but that is not the case. Here is a simple example why.\nWe decide to correlate the customer age with the sales amount. Suppose there are two scenarios from a sample of 500 customers:\nCode\n# Load packages\nlibrary(tidyverse)\n\n## Simulate some data\n\n# Set the seed for reproducibility\nset.seed(123456789)\n\n# Sample size\nn &lt;- 500\n\n# Create the data set\nsim_dat &lt;- \n  tibble(\n    Age = round(18 + rgamma(n = n, shape = 5, scale = 4)),\n    Sales_Large = 500 + 10 * (Age - mean(Age)) + rnorm(n = n, sd = 100),\n    Sales_Small = 500 + .10 * (Age - mean(Age)) + rnorm(n = n, sd = 1)\n  ) |&gt;\n  \n  # Send down the rows\n  pivot_longer(\n    cols = starts_with(\"Sales\"),\n    names_to = \"Effect\",\n    values_to = \"Sales\",\n    names_prefix = \"Sales_\"\n  ) \n\nsim_dat |&gt;\n  \n  # Make a paneled scatterplot\n  ggplot() +\n  geom_point(\n    aes(\n      x = Age,\n      y = Sales,\n      fill = Effect\n    ),\n    shape = 21,\n    color = \"black\",\n    alpha = .5,\n    size = 2\n  ) +\n  facet_wrap(~Effect, scales = \"free_y\") +\n  theme(\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    strip.text = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 16),\n    panel.spacing.x = unit(2, \"lines\")\n  ) +\n  xlab(\"Customer Age (years)\") +\n  ylab(\"Sales ($)\")\nAt a glance these graphs look very similar, such that age is positively correlated with the sales amount. We fit a linear regression model, or “best-fit line”, to summarize and describe the relationship.\nCode\n# Fit the models\nsim_models &lt;- \n  sim_dat |&gt;\n  \n  # Nest the data\n  nest(.by = Effect) |&gt;\n  \n  # Fit a linear model for each data set; get p-values\n  mutate(\n    \n    # Fit the model\n    model = \n      data |&gt;\n      map(\n        \\(.dat) \n        \n        lm(\n          formula = Sales ~ Age,\n          data = .dat\n        )\n        \n      ),\n    \n    # Compute the p-value\n    pvalue = \n      model |&gt;\n      map(\n        \\(.model) \n        2 * pt(\n          q = .model$coefficients / sqrt(diag(vcov(.model))), \n          df = .model$df.residual, \n          lower.tail = FALSE\n        )[[2]]\n      )\n  )\n\nsim_dat |&gt;\n  \n  # Make a paneled scatterplot\n  ggplot() +\n  geom_point(\n    aes(\n      x = Age,\n      y = Sales,\n      fill = Effect\n    ),\n    shape = 21,\n    color = \"black\",\n    alpha = .5,\n    size = 2\n  ) +\n  geom_smooth(\n    aes(\n      x = Age,\n      y = Sales\n    ),\n    formula = y~x,\n    method = \"lm\",\n    color = \"black\",\n    se = FALSE\n  ) +\n  geom_text(\n    data = \n      sim_models |&gt;\n      \n      # Extract the p-value\n      unnest(cols = pvalue) |&gt;\n      \n      # Clean up p-value\n      mutate(\n        pvalue = \n          case_when(\n            pvalue &lt; 0.001 ~ \"&lt;0.001\",\n            TRUE ~ as.character(round(pvalue, 3))\n          )\n      ),\n    aes(\n      x = 60,\n      y = c(160, 496.5),\n      label = paste0(\"P-value: \", pvalue)\n    )\n  ) +\n  facet_wrap(~Effect, scales = \"free_y\") +\n  theme(\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    strip.text = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 16),\n    panel.spacing.x = unit(2, \"lines\")\n  ) +\n  xlab(\"Customer Age (years)\") +\n  ylab(\"Sales ($)\")\nThe p-values for both of these models are extremely, and equally, small (&lt;0.1%), indicating statistical significance. In fact, the evidence is so strong that some might say it is very significant–much smaller than the standard (and infamous) rule of thumb threshold of 5%.\nIt is based on this information alone that often would elicit the conclusion/statement/finding that age is significantly associated with sales.\nYou hear this language all the time, especially in research. It brings with it certain implications of importance, as if it has now become a meaningful fact that should warrant attention and/or action.\nThe problem?\nWatch what happens when we add the actual scales for sales to these graphs (if you’ve noticed, they’ve been missing this whole time):\nCode\nsim_dat |&gt;\n    \n    # Make a paneled scatterplot\n    ggplot() +\n    geom_point(\n        aes(\n            x = Age,\n            y = Sales,\n            fill = Effect\n        ),\n        shape = 21,\n        color = \"black\",\n        alpha = .5,\n        size = 2\n    ) +\n    geom_smooth(\n        aes(\n            x = Age,\n            y = Sales\n        ),\n        formula = y~x,\n        method = \"lm\",\n        color = \"black\",\n        se = FALSE\n    ) +\n    geom_text(\n        data = \n            sim_models |&gt;\n            \n            # Extract the p-value\n            unnest(cols = pvalue) |&gt;\n            \n            # Clean up p-value\n            mutate(\n                pvalue = \n                    case_when(\n                        pvalue &lt; 0.001 ~ \"&lt;0.001\",\n                        TRUE ~ as.character(round(pvalue, 3))\n                    )\n            ),\n        aes(\n            x = 60,\n            y = c(160, 496.5),\n            label = paste0(\"P-value: \", pvalue)\n        )\n    ) +\n    facet_wrap(~Effect, scales = \"free_y\") +\n    theme(\n        panel.background = element_blank(),\n        legend.position = \"none\",\n        strip.text = element_blank(),\n        axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16),\n        panel.spacing.x = unit(2, \"lines\")\n    ) +\n    xlab(\"Customer Age (years)\") +\n    scale_y_continuous(\n      name = \"Sales ($)\",\n      labels = \\(x) scales::dollar(x, accuracy = 1)\n    )\nOn the left panel, the range of sales goes from approximately $200 to $1000 per customer in an increasing fashion with age. On the right panel, it goes from about $497 to $503–a few dollars. See the issue yet? To solidify this, let’s look at the graphs when they are on the same scale:\nCode\nsim_dat |&gt;\n    \n    # Make a paneled scatterplot\n    ggplot() +\n    geom_point(\n        aes(\n            x = Age,\n            y = Sales,\n            fill = Effect\n        ),\n        shape = 21,\n        color = \"black\",\n        alpha = .5,\n        size = 2\n    ) +\n    geom_smooth(\n        aes(\n            x = Age,\n            y = Sales\n        ),\n        formula = y~x,\n        method = \"lm\",\n        color = \"black\",\n        se = FALSE\n    ) +\n    geom_text(\n        data = \n            sim_models |&gt;\n            \n            # Extract the p-value\n            unnest(cols = pvalue) |&gt;\n            \n            # Clean up p-value\n            mutate(\n                pvalue = \n                    case_when(\n                        pvalue &lt; 0.001 ~ \"&lt;0.001\",\n                        TRUE ~ as.character(round(pvalue, 3))\n                    )\n            ),\n        aes(\n            x = 60,\n            y = 160,\n            label = paste0(\"P-value: \", pvalue)\n        )\n    ) +\n    facet_wrap(~Effect) +\n    theme(\n        panel.background = element_blank(),\n        legend.position = \"none\",\n        strip.text = element_blank(),\n        axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16),\n        panel.spacing.x = unit(2, \"lines\")\n    ) +\n    xlab(\"Customer Age (years)\") +\n    scale_y_continuous(\n        name = \"Sales ($)\",\n        labels = \\(x) scales::dollar(x, accuracy = 1)\n    )\nThe line in the right panel is basically flat.\nIt turns out that although these two graphs have the same amount of statistical significance, they clearly tell much different stories about how age relates to sales. The lines can be summarized as follows:\nIn the context of performing market segmentation for increased revenue (or whatever else it may be), these magnitudes certainly matter. Statistically, they don’t."
  },
  {
    "objectID": "post/simple-example-why-statistical-significance-is-insufficient/index.html#some-takeaways",
    "href": "post/simple-example-why-statistical-significance-is-insufficient/index.html#some-takeaways",
    "title": "A simple example why statistical significance is insufficient for action",
    "section": "Some takeaways",
    "text": "Some takeaways\n\nStatistical significance only pertains to the existence of a relationship (under the implied assumptions), not the size of it.\nYou must pay attention to the magnitude of the relationship to gain any meaningful insight.\nThe magnitudes should be translated to the real-world implications of using the information for different decisions or courses of action. In the example above, the market age distribution looks like this:\n\n\n\nCode\nsim_dat |&gt;\n  \n  # Make a paneled scatterplot\n  ggplot() +\n  geom_histogram(\n    aes(\n      x = Age\n    ),\n    fill = \"gray\",\n    color = \"black\",\n    alpha = .5\n  ) +\n  theme(\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    strip.text = element_blank(),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 16)\n  ) +\n  xlab(\"Customer Age (years)\") +\n  ylab(\"Customers\")\n\n\n\n\n\n\n\n\n\nAlthough older customers yield higher sales, it may cost more marketing dollars to acquire any given individual from such a small segment, ultimately making the juice not worth the squeeze. Feeding estimates into cost-benefit or what-if scenarios can greatly increase confidence in how a proposed course of action would actually play out, instead of implementing things on the basis of mere existence (i.e., statistical significance)."
  },
  {
    "objectID": "post/a-look-at-collider-bias/index.html",
    "href": "post/a-look-at-collider-bias/index.html",
    "title": "A look at collider bias",
    "section": "",
    "text": "There has been a lot of buzz around “causal inference” and given how fundamental the name seems to statistics, I picked up The Book of Why: The New Science of Cause and Effect as a starting point. It’s been a great resource for introducing causal concepts and thinking about the importance of bringing subjectivity into the modeling process. This article looks into one of the concepts introduced: collider bias. All code snippets are written in R."
  },
  {
    "objectID": "post/a-look-at-collider-bias/index.html#key-question",
    "href": "post/a-look-at-collider-bias/index.html#key-question",
    "title": "A look at collider bias",
    "section": "Key question",
    "text": "Key question\nDo patients who live further from a trial center have more severe disease?"
  },
  {
    "objectID": "post/a-look-at-collider-bias/index.html#the-setup",
    "href": "post/a-look-at-collider-bias/index.html#the-setup",
    "title": "A look at collider bias",
    "section": "The setup",
    "text": "The setup\nThere is a clinical trial that we would like to recruit patients for which will be held at a single location. The study population consists of patients who have a disease of interest and live within 90 miles of the trial center. Suppose the following are true:\n\n65% of patients live near (within 20 miles of) the trial center (denoted N)\n25% of patients have severe disease regardless how far they live from the trial center (denoted S)\n\nSo we have…\n\\[P(N) = 0.65 \\hskip{.25in} P(S|N) = 0.25 \\hskip{.25in} P(S|\\bar{N}) = 0.25\\] Our sampling strategy will be to select patients at random from the entire population until 1000 are enrolled.\n\n\nCode\n# Set a seed\nset.seed(321)\n\n# Base sample size\nbase_size &lt;- 100000\n\n# Enrollment size\nn &lt;- 1000\n\n# Make a data frame\npatients &lt;-\n  tibble(\n    S = rbinom(base_size, 1, .25), # Disease severity\n    N = rbinom(base_size, 1, .65), # Distance from center\n  ) %&gt;%\n  \n  # Add enrollment flag based on\n  mutate(\n    E = \n      # Determine enrollment probability\n      case_when(\n        N == 1 ~ 0.85,\n        S == 1 ~ 0.50,\n        TRUE ~ 0.10\n      ) %&gt;%\n      \n      # Sample based on probability\n      rbinom(\n        n = base_size,\n        size = 1\n      )\n  ) %&gt;%\n  \n  # Filter to enrolled patients\n  filter(\n    E == 1\n  ) %&gt;%\n  \n  # Sample the desired enrollment\n  slice_sample(\n    n = n\n  ) %&gt;%\n  \n  # Convert to factors\n  mutate(\n    S = \n      S %&gt;%\n      factor() %&gt;%\n      fct_recode(\n        Severe = \"1\",\n        `Non-severe` = \"0\"\n      ) %&gt;%\n      fct_relevel(\"Severe\"),\n    N = \n      N %&gt;%\n      factor() %&gt;%\n      fct_recode(\n        Near = \"1\",\n        Far = \"0\"\n      ) %&gt;%\n      fct_relevel(\"Near\")\n  ) %&gt;%\n  \n  # Remove the enrollment indicator\n  select(\n    -E\n  )"
  },
  {
    "objectID": "post/a-look-at-collider-bias/index.html#the-problem",
    "href": "post/a-look-at-collider-bias/index.html#the-problem",
    "title": "A look at collider bias",
    "section": "The problem",
    "text": "The problem\nSuppose we reach the desired enrollment size and we’re given a data set (called patients) to do some preliminary analysis on the sample. Here are the first 5 rows:\n\n\nCode\nprint(patients, n = 5)\n\n\n# A tibble: 1,000 × 2\n  S          N    \n  &lt;fct&gt;      &lt;fct&gt;\n1 Non-severe Near \n2 Severe     Far  \n3 Severe     Near \n4 Non-severe Near \n5 Non-severe Near \n# ℹ 995 more rows\n\n\nWe have all 1000 patients represented in the data with two variables collected so far: disease severity (S) and distance from the trial center (N). Let’s tabulate these columns:\n\n\nCode\npatients %&gt;%\n  \n  # Count the rows\n  group_by(N, S) %&gt;%\n  summarise(\n    Count = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  # Send over the columns\n  pivot_wider(\n    names_from = N,\n    values_from = Count\n  ) %&gt;%\n  \n  # Rename\n  rename(\n    `Disease severity` = S\n  ) %&gt;%\n  \n  # Make a kable\n  knitr::kable(\n    format = \"html\",\n    caption = \"Tabulation of disease severity and distance from trial center for 1000 sampled patients\"\n  ) %&gt;%\n  kableExtra::kable_styling(\n    full_width = FALSE\n  ) %&gt;%\n  kableExtra::add_header_above(c(\"\", \"Distance from trial center\" = 2))\n\n\n\nTabulation of disease severity and distance from trial center for 1000 sampled patients\n\n\n\n\n\n\n\n\n\nDistance from trial center\n\n\n\nDisease severity\nNear\nFar\n\n\n\n\nSevere\n221\n82\n\n\nNon-severe\n650\n47\n\n\n\n\n\n\n\nThere is something strange: Of the patients who live near the trial center, 25.4% have severe disease (which is expected), but of those living far from the trial center, 63.6% have severe disease. We know that only 25% of patients have severe disease regardless of where they live. Is this possible? Did this just happen by chance?\n\n\nCode\nprop.test(sum(patients$S == \"Severe\" & patients$N == \"Far\"), sum(patients$N == \"Far\"), p = 0.25)$p.value\n\n\n[1] 1.321956e-23\n\n\nWell, the p-value isn’t 0 (it will never be), so it is possible. However, there is probably something else going on. What else could be causing this discrepancy?"
  },
  {
    "objectID": "post/a-look-at-collider-bias/index.html#an-explanation",
    "href": "post/a-look-at-collider-bias/index.html#an-explanation",
    "title": "A look at collider bias",
    "section": "An explanation",
    "text": "An explanation\nIf we think about the patients randomly selected and offered enrollment versus those ultimately deciding to enroll, a reasonable assumption might be that those with less-severe disease, especially those who also live far away, may be more reluctant to join. This is exactly what happened.\nIt turns out that the following are also true about this population with respect to their likelihood to join the trial (denoted J):\n\n85% of patients living near the trial center will join if asked regardless of disease severity\nOf those living far from the trial center, 50% will join if they have severe disease and only 10% will join if they don’t.\n\nIn probability notation:\n\\[P(J|N) = 0.85 \\hskip{.25in} P(J|\\bar{N} \\cap S) = 0.50 \\hskip{.25in} P(J|\\bar{N} \\cap \\bar{S}) = 0.10\\] Even though every patient had the same opportunity to enroll in the trial, our realized sample became much more heavily weighted toward those living near the trial center, and much less weighted toward those living far from the trial center without severe disease. Thus, it conditioned on the patients that enrolled. As a result, a correlation between disease severity and the distance from the trial center was induced that isn’t there in the general population. The figure below displays the causal diagram for this relationship.\n\n\n\n\n\nflowchart LR\n  A[Distance from center] --&gt; C[Decision to join trial]\n  B[Disease severity] --&gt; C\n\n\n\n\n\n\n\nDoes the math show it?\nWe know that 25% of patients in the general population have severe disease regardless of where they live. How does this probability change for patients who live far from the trial center when we condition on only those enrolled?\n\\[\n\\begin{equation}\n\\begin{split}\nP(S|\\bar{N} \\cap J) & = \\frac{P(S \\cap \\bar{N} \\cap J)}{P(\\bar{N} \\cap J)} \\\\\n& = \\frac{P(S \\cap \\bar{N} \\cap J)}{P(S \\cap \\bar{N} \\cap J) + P(\\bar{S} \\cap \\bar{N} \\cap J)} \\\\\n& = \\frac{P(J|\\bar{N} \\cap S)P(\\bar{N} \\cap S)}{P(J|\\bar{N} \\cap S)P(\\bar{N} \\cap S) + P(J|\\bar{N} \\cap \\bar{S})P(\\bar{N} \\cap \\bar{S})} \\\\\n& = \\frac{P(J|\\bar{N} \\cap S)P(S|\\bar{N})P(\\bar{N})}{P(J|\\bar{N} \\cap S)P(S|\\bar{N})P(\\bar{N}) + P(J|\\bar{N} \\cap \\bar{S})P(\\bar{S}|\\bar{N})P(\\bar{N})} \\\\\n& = \\frac{P(J|\\bar{N} \\cap S)P(S|\\bar{N})}{P(J|\\bar{N} \\cap S)P(S|\\bar{N})+ P(J|\\bar{N} \\cap \\bar{S})P(\\bar{S}|\\bar{N})} \\\\\n& = \\frac{0.50 \\times 0.25}{0.50 \\times 0.25 + 0.10 \\times (1 - 0.25)} \\\\\n& = 0.625\n\\end{split}\n\\end{equation}\n\\] Ah, much better. Recall that in our sample 63.6% of patients who lived far from the trial center had severe disease. This is much closer to the true probability that we’d expect after conditioning on the correct factors.\nIntuitively, this result makes sense. If patients who live far away and have less-severe disease are the most unlikely to join the trial, then a patient who is enrolled and lives far away is more likely to have severe disease."
  },
  {
    "objectID": "post/nesting-with-tidyr/index.html",
    "href": "post/nesting-with-tidyr/index.html",
    "title": "Nesting with {tidyr}",
    "section": "",
    "text": "I really love the elegance of the nest functionality with the tidyr package. It really allows you to abstract the meaning of a data frame to not just contain rectangular data with scalars, but rather a generalization that has rectangular data of objects. The most intriguing part of it to me is the way we can continue to use typical join operations even with complex objects in some of the columns, which makes it so smooth and intuitive to do complex data operations.\n\n\nCode\n# Load packages\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nFor example, lets say we have a dataset.\n\n\nCode\ndat &lt;- cheese::heart_disease\ndat\n\n\n# A tibble: 303 × 9\n     Age Sex    ChestPain           BP Cholesterol BloodSugar MaximumHR\n   &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;          &lt;dbl&gt;\n 1    63 Male   Typical angina     145         233 TRUE             150\n 2    67 Male   Asymptomatic       160         286 FALSE            108\n 3    67 Male   Asymptomatic       120         229 FALSE            129\n 4    37 Male   Non-anginal pain   130         250 FALSE            187\n 5    41 Female Atypical angina    130         204 FALSE            172\n 6    56 Male   Atypical angina    120         236 FALSE            178\n 7    62 Female Asymptomatic       140         268 FALSE            160\n 8    57 Female Asymptomatic       120         354 FALSE            163\n 9    63 Male   Asymptomatic       130         254 FALSE            147\n10    53 Male   Asymptomatic       140         203 TRUE             155\n# ℹ 293 more rows\n# ℹ 2 more variables: ExerciseInducedAngina &lt;fct&gt;, HeartDisease &lt;fct&gt;\n\n\nAnd we want to compute age percentiles by sex for those who do and don’t have heart disease.\n\n\nCode\n# Nest data frames within sex, heart disease\nnested1 &lt;-\n  dat %&gt;%\n  group_by(Sex, HeartDisease) %&gt;%\n  nest()\nnested1\n\n\n# A tibble: 4 × 3\n# Groups:   Sex, HeartDisease [4]\n  Sex    HeartDisease data              \n  &lt;fct&gt;  &lt;fct&gt;        &lt;list&gt;            \n1 Male   No           &lt;tibble [92 × 7]&gt; \n2 Male   Yes          &lt;tibble [114 × 7]&gt;\n3 Female No           &lt;tibble [72 × 7]&gt; \n4 Female Yes          &lt;tibble [25 × 7]&gt; \n\n\nWe can see that there is now a separate dataset available within each combination of sex and heart disease status in the form of a list column.\n\n\nCode\n# Get the empirical cumulative density function for age\nnested2 &lt;-\n  nested1 %&gt;%\n  mutate(\n    ecdf_col = data %&gt;% map(~ecdf(.x$Age))\n  )\nnested2\n\n\n# A tibble: 4 × 4\n# Groups:   Sex, HeartDisease [4]\n  Sex    HeartDisease data               ecdf_col\n  &lt;fct&gt;  &lt;fct&gt;        &lt;list&gt;             &lt;list&gt;  \n1 Male   No           &lt;tibble [92 × 7]&gt;  &lt;ecdf&gt;  \n2 Male   Yes          &lt;tibble [114 × 7]&gt; &lt;ecdf&gt;  \n3 Female No           &lt;tibble [72 × 7]&gt;  &lt;ecdf&gt;  \n4 Female Yes          &lt;tibble [25 × 7]&gt;  &lt;ecdf&gt;  \n\n\nWe then apply list operations as we normally would. In this case, we use purrr::map to create an empirical cumulative density function for age within each group. The result is then just a list of ecdf functions.\n\n\nCode\n# Make an age grid\nage_grid &lt;-\n  dat %&gt;%\n  select(Sex, HeartDisease) %&gt;%\n  distinct() %&gt;%\n  inner_join(\n    y = tibble(Age = c(40, 50, 60, 70)),\n    by = character()\n  )\n\n\nWarning: Using `by = character()` to perform a cross join was deprecated in dplyr 1.1.0.\nℹ Please use `cross_join()` instead.\n\n\nCode\nage_grid\n\n\n# A tibble: 16 × 3\n   Sex    HeartDisease   Age\n   &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt;\n 1 Male   No              40\n 2 Male   No              50\n 3 Male   No              60\n 4 Male   No              70\n 5 Male   Yes             40\n 6 Male   Yes             50\n 7 Male   Yes             60\n 8 Male   Yes             70\n 9 Female No              40\n10 Female No              50\n11 Female No              60\n12 Female No              70\n13 Female Yes             40\n14 Female Yes             50\n15 Female Yes             60\n16 Female Yes             70\n\n\nWe then made an age grid for each sex/heart disease combination to evaluate the percentiles of each age in the respective groups. Now, we can compute the percentiles by joining to get the ecdf for the respective group, and plugging in each age into the function.\n\n\nCode\nage_grid %&gt;% \n  \n  # Join to get the ecdf for the group\n  inner_join(\n    y = nested2 %&gt;% select(-data),\n    by = c(\"Sex\", \"HeartDisease\")\n  ) %&gt;%\n  \n  # Compute the percentile for the given age\n  mutate(\n    Percentile = map2(.x = ecdf_col, .y = as.list(Age), .f = ~.x(.y)) %&gt;% flatten_dbl()\n  )\n\n\n# A tibble: 16 × 5\n   Sex    HeartDisease   Age ecdf_col Percentile\n   &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt; &lt;list&gt;        &lt;dbl&gt;\n 1 Male   No              40 &lt;ecdf&gt;       0.0761\n 2 Male   No              50 &lt;ecdf&gt;       0.424 \n 3 Male   No              60 &lt;ecdf&gt;       0.859 \n 4 Male   No              70 &lt;ecdf&gt;       1     \n 5 Male   Yes             40 &lt;ecdf&gt;       0.0526\n 6 Male   Yes             50 &lt;ecdf&gt;       0.246 \n 7 Male   Yes             60 &lt;ecdf&gt;       0.719 \n 8 Male   Yes             70 &lt;ecdf&gt;       0.991 \n 9 Female No              40 &lt;ecdf&gt;       0.0694\n10 Female No              50 &lt;ecdf&gt;       0.361 \n11 Female No              60 &lt;ecdf&gt;       0.694 \n12 Female No              70 &lt;ecdf&gt;       0.931 \n13 Female Yes             40 &lt;ecdf&gt;       0     \n14 Female Yes             50 &lt;ecdf&gt;       0.04  \n15 Female Yes             60 &lt;ecdf&gt;       0.52  \n16 Female Yes             70 &lt;ecdf&gt;       1     \n\n\nWe can see, for example, that a 60 year old male is at the \\(86^{th}\\) percentile for those without heart disease, but at the \\(72^{nd}\\) for those who due, suggesting that the age distribution tends to be higher in patients with heart disease."
  },
  {
    "objectID": "post/the-evasive-spline/index.html",
    "href": "post/the-evasive-spline/index.html",
    "title": "The Evasive Spline",
    "section": "",
    "text": "This one always gets me. I’ve learned and forgot how splines work many times over the years, and when I need to relearn it, I read the Moving Beyond Linearity chapter of An Introduction to Statistical Learning.\nBasis Functions are just a general approach for transforming a simple model like:\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\] into a linear combination of transformations of the \\(X_i\\) of the form:\n\\[Y_i = \\beta_0 + \\beta_1b_1(X_i) + \\beta_2b_2(X_i) + ... + \\beta_Kb_k(X_i) + \\epsilon_i\\] where \\(b_i\\) is a known function that transforms the predictor variable. Note: \\(\\beta_1\\) is not the same in both of these, they are just placeholders for an arbitrary parameter. For example, in the case of a piecewise regression where the model is of the form:\n\\[Y_i = \\beta_0 + \\beta_1I(X_i &lt; c_1) + \\beta_2I(c_1 &lt; X_i &lt; c_2) + ... + \\beta_kI(c_{k-1} &lt; X_i &lt; c_k) + \\epsilon_i\\] the indicators are basis functions such that:\n\\[b_j(X_i) = I(c_{j-1} &lt; X_i &lt; c_j) \\hskip.1in \\text{for j=1,..,k}\\] or in a polynomial model, the basis functions are \\(b_j(X_i) = X_i^j\\).\nKnots are points (cutoffs) along \\(X_i\\) that a local regression starts/ends. For example, we might fit a cubic model (e.g., with parameters \\(\\beta_1, \\beta_2, \\beta_3\\)) where \\(X_i &lt; C\\), and another model (with a separate set of \\(\\beta_1, \\beta_2, \\beta_3\\)) where \\(X_i \\geq C\\). \\(C\\) is a knot. In this sense, the piecewise regression above was also a polynomial regression with degree 0, and knots at each value of \\(c_j\\).\nThe general problem with an unconstrained polynomial model is that there are no restrictions that force a smooth function across \\(X\\), so there are discontinuities. Thus, restrictions need to be put in place such as (1) making it continuous at the knot(s), and/or even further, (2) making the first and second derivatives continuous at the knots. These restrictions reduce the complexity of the model (i.e., the number of parameters we estimate).\n\nCubic Splines\nA cubic spline with \\(K\\) knots uses \\(K+4\\) parameters. The best way to do this is to use (1) the basis of a cubic polynomial (\\(x, x^2, x^3\\)) and (2) a truncated power basis for each knot:\n\\[h(x,\\nu) = {(x-\\nu)}_+^3\\] where \\(\\nu\\) is the knot location. Thus, a one-knot model looks like:\n\\[Y_i = \\beta_0 + \\beta_1X_i + \\beta_2X_i^2 + \\beta_3X_i^3 + \\beta_4h(X_i,\\nu_1) + \\epsilon_i\\] We can add more knots as needed, and it simply adds \\(h(x,\\nu)\\) terms only (so 1 more parameter per knot). A function of this form is guaranteed to have continuous first and second derivatives.\nSo how does this relate to what is produced in the rcs function from the rms package?\nWell, the package actually fits a restricted cubic spline, which is a natural spline. This adds even more restrictions that the general cubic spline by forcing it to be linear where \\(X\\) is less than the smallest knot and where \\(X\\) is larger than the largest not (i.e., the boundaries). These add an additional two constraints at each boundary. So if we have a regular cubic spline model above with 3 knots (i.e., 7 parameters), then a restricted cubic spline model with 3 knots should have only 3 parameters.\n\n\nCode\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- .5*x^2 - .75*x + rnorm(100)\nx_trans &lt;- rms::rcs(x, 3)\nhead(x_trans)\n\n\n               x         x'\n[1,] -0.56047565 0.02405534\n[2,] -0.23017749 0.10816181\n[3,]  1.55870831 2.14013743\n[4,]  0.07050839 0.27135366\n[5,]  0.12928774 0.31547103\n[6,]  1.71506499 2.36735648\nattr(,\"class\")\n[1] \"rms\"\nattr(,\"name\")\n[1] \"x\"\nattr(,\"label\")\n[1] \"x\"\nattr(,\"assume\")\n[1] \"rcspline\"\nattr(,\"assume.code\")\n[1] 4\nattr(,\"parms\")\n[1] -1.06822046  0.06175631  1.26449867\nattr(,\"nonlinear\")\n[1] FALSE  TRUE\nattr(,\"colnames\")\n[1] \"x\"  \"x'\"\n\n\nCode\nsummary(lm(y~x_trans))\n\n\n\nCall:\nlm(formula = y ~ x_trans)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9888 -0.7341 -0.0803  0.6900  3.2215 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.3838     0.1842  -2.084   0.0398 *  \nx_transx     -1.6552     0.2449  -6.758 1.04e-09 ***\nx_transx'     1.2922     0.2925   4.417 2.61e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9823 on 97 degrees of freedom\nMultiple R-squared:  0.3805,    Adjusted R-squared:  0.3677 \nF-statistic: 29.79 on 2 and 97 DF,  p-value: 8.214e-11\n\n\nWe can see this model contains three estimated parameters as expected. The actual transformation completed for the restricted cubic spline in producing, in general, the \\(K-2\\) additional predictors is more complex than the cubic spline (although similar). In this case, the 3 knot positions were selected to be:\n\n\nCode\nknots &lt;- attr(x_trans, \"parms\")\nknots\n\n\n[1] -1.06822046  0.06175631  1.26449867\n\n\nNote, these are just the \\(10^{th}, 50^{th}, 90^{th}\\) percentiles:\n\n\nCode\nquantile(x, c(.1,.5,.9))\n\n\n        10%         50%         90% \n-1.06822046  0.06175631  1.26449867 \n\n\nThe following transformation is made (general solution here):\n\\[X_{trans} = (x-\\nu_1)_+^3 - (x-\\nu_2)_+^3\\frac{\\nu_3-\\nu_1}{\\nu_3-\\nu_2} + (x-\\nu_3)_+^3\\frac{\\nu_2-\\nu_1}{\\nu_3-\\nu_2}\\] Let’s check out that transformation on our data:\n\n\nCode\ntibble::tibble(\n  x = as.numeric(x_trans[,\"x\"]),\n  x_trans_actual = as.numeric(x_trans[,\"x'\"]),\n  x_trans_calculated = \n    pmax((x-knots[1])^3, 0) -\n    pmax((x-knots[2])^3, 0) * ((knots[3]-knots[1]) / (knots[3]-knots[2])) +\n    pmax((x-knots[3])^3, 0) * ((knots[2]-knots[1])/(knots[3]-knots[2]))\n)\n\n\n# A tibble: 100 × 3\n         x x_trans_actual x_trans_calculated\n     &lt;dbl&gt;          &lt;dbl&gt;              &lt;dbl&gt;\n 1 -0.560          0.0241             0.131 \n 2 -0.230          0.108              0.589 \n 3  1.56           2.14              11.6   \n 4  0.0705         0.271              1.48  \n 5  0.129          0.315              1.72  \n 6  1.72           2.37              12.9   \n 7  0.461          0.634              3.45  \n 8 -1.27           0                  0     \n 9 -0.687          0.0102             0.0555\n10 -0.446          0.0443             0.241 \n# ℹ 90 more rows\n\n\nFor some reason this is close but off by a factor close to 5? Looking into the documentation/code, it is because of the norm argument in the Hmisc::rcspline.eval function. When we run this, we get the same result that we calculated (which is the original restricted cubic spline calculation):\n\n\nCode\nhead(Hmisc::rcspline.eval(x,nk=3, norm = 0))\n\n\n          [,1]\n[1,]  0.130899\n[2,]  0.588571\n[3,] 11.645726\n[4,]  1.476592\n[5,]  1.716660\n[6,] 12.882156\n\n\nBy default, this function uses norm=2, which “normalizes by the square of the spacing between the first and last knots…has the advantage of making all nonlinear terms be on the x-scale”.\n\n\nCode\nhead(Hmisc::rcspline.eval(x,nk=3, norm = 2))\n\n\n           [,1]\n[1,] 0.02405534\n[2,] 0.10816181\n[3,] 2.14013743\n[4,] 0.27135366\n[5,] 0.31547103\n[6,] 2.36735648"
  },
  {
    "objectID": "post/statistical-significance-is-insignificant/index.html",
    "href": "post/statistical-significance-is-insignificant/index.html",
    "title": "Statistical significance is…insignificant",
    "section": "",
    "text": "The longer I’ve been practicing as a statistician, maybe paradoxically, the more skeptical I’ve become of statistical significance (#1). It manifests as a feeling of dissatisfaction, as if, even though you’ve stated what you “found”, you don’t actually believe it to be true. I recently finished reading The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives–it instantly became one of my favorite books (here are my favorite quotes and passages). It affirms a lot of what I’ve come to suspect, with deep articulation about the vastness of the issue, backed by a thorough historical foundation. I can’t help but wonder about the broader scientific, political, and societal implications this has had over the years (and continues to have). It really lit a fire in me to continue learning about and unraveling statistical history to connect those dots."
  },
  {
    "objectID": "post/statistical-significance-is-insignificant/index.html#arbitrarythreshold",
    "href": "post/statistical-significance-is-insignificant/index.html#arbitrarythreshold",
    "title": "Statistical significance is…insignificant",
    "section": "An arbitrary threshold",
    "text": "An arbitrary threshold\nThe 5% threshold is arbitrary. Despite that common acknowledgement, willful ignorance tends to prevail due to tradition and adherence to norms. The fact that the perceived significance of a result can suddenly change from minute differences speaks to the lack of robustness in the logic. In the book, the authors frequently discuss the importance of a loss function, which focuses on the potential consequences and implications of the result on the real-world decisions that are sought to be made from the information, rather than a predefined threshold based on sampling error probability. In this sense, the allowable risk tolerance can’t be objectively or mechanically determined. It is context-dependent, and not all decisions are created equal. Yes, the p-value above was 31%, but that error rate, along with the plausible range of risks (and benefits), may be sufficient to someone needing to make a treatment decision now.\n\nRisks are subjective\n\n“It always depends on the loss, measured in side effects, treatment cost, death rates. The loss to a cool, scientific, impartial spectator will not be the same as the loss to the patient in question…[the balance between Type I/II errors] ‘must be left to the patient, friends, and family’.”1\n\nBeyond the statistical significance of a result is the question of what to do about it. In the same article, the authors state the following, still in the context of misinformation:\n\n“Claims that myocarditis was common in children who received the vaccine and that the risks of myocarditis outweighed the risk of vaccination were also unfounded.”2\n\nNevermind the fact that the study they reference does show an increase in monthly case volume of myocarditis and pericarditis between pre/post-vaccine periods and the authors state:\n\n“Myocarditis developed rapidly in younger patients, mostly after the second vaccination. Pericarditis affected older patients later, after either the first or second dose.”4\n\nThe more important point is that the weight individuals place on statistical results to inform their decision making is subjective. The risk may be low, maybe even lower than the alternative, but that doesn’t inform how someone should weigh it.\n\n“Imagine that you and your infant child are standing on a sidewalk near a busy street. You have just purchased a hot dog from the street vendor and have safely crossed the street. Scenario 1: You suddenly realize you have forgotten the mustard and if you scurry across the busy street, dodging vehicles, there is a 95% probability you’ll return safe with your mustard. Scenario 2: You forgot your child and you watch as she tries to cross the street herself, if you scurry across the busy street, dodging vehicles, there is a 95% probabiliity you’ll return safe with your child. The sizeless scientist in effect declares ‘they are equally important reasons for crossing the street’”1"
  },
  {
    "objectID": "post/statistical-significance-is-insignificant/index.html#samplesize",
    "href": "post/statistical-significance-is-insignificant/index.html#samplesize",
    "title": "Statistical significance is…insignificant",
    "section": "It can’t depend on sample size",
    "text": "It can’t depend on sample size\n\n“At high sample sizes, all null hypotheses are rejected, by mathematical fact, without having to look at the data.”1\n\nOne pretty simple argument is that of sample size. In most contexts, a statistical test is, by definition, more likely to be declared significant by simply amassing more data, regardless of what the actual effect size is. This, on the other hand, is completely mechanical and dissociated from the real-world context in which the test is being run. Thus, it prioritizes quantity over substance, and when blindly used, potentially promotes results that may lack practical meaning.\n\n“…some cause of natural selection may have a high probability of replicability in additional samples but be trivial. Yet a cause may have a low probability of replicability but be important. This is what we mean when we say that a test of significance is neither necessary nor sufficient for a finding of importance”1\n\nIt also tends to shift focus to attaining statistical significance and using it as a filter, causing the potential to miss meaningful insights that didn’t reach this level."
  },
  {
    "objectID": "post/statistical-significance-is-insignificant/index.html#we-dont-believe-in-zero-sized-effects",
    "href": "post/statistical-significance-is-insignificant/index.html#we-dont-believe-in-zero-sized-effects",
    "title": "Statistical significance is…insignificant",
    "section": "We don’t believe in “zero-sized” effects",
    "text": "We don’t believe in “zero-sized” effects\n\n“Real scientists draw a line between what is large and small.”1\n\nThere is a major contradiction that arises.\nThe typical hypothesis test is conducted under the assumption of a null hypothesis positing no effect. For example, in calculating the p-value above, it is assumed that there is no difference in all-cause mortality rates between the treatment groups. However, I would argue that in any practical context, it’s rare that someone would genuinely believe in the existence of precisely zero effect. Rather, it would stand to reason that what they really mean is “effectively zero” effect, something so small that it is considered inconsequential.\nHerein lies the contradiction: they have now acknowledged some level of substantive significance, albeit undefined. If the true effect happens to be smaller than this threshold, as we just explained, the estimate will still eventually be declared statistically significant with mathematical certainty no matter how minuscule, thus inevitably crossing the unspoken threshold of substantive meaning. Therefore, this begs into question the value of attaining statistical significance at all in favor of the need for explicit consideration of the real-world implications (i.e., the loss function). At the very least, the substantive threshold should be identified and reflected in the null hypothesis so that the p-value is calibrated for substance."
  },
  {
    "objectID": "post/statistical-significance-is-insignificant/index.html#fallacy",
    "href": "post/statistical-significance-is-insignificant/index.html#fallacy",
    "title": "Statistical significance is…insignificant",
    "section": "The fallacy of the transposed conditional",
    "text": "The fallacy of the transposed conditional\nThis is where it gets especially interesting. There are logical errors with the conclusions drawn from hypothesis testing. I think the best way to describe it is jumping into the classic example that arises in Jacob Cohen’s The Earth Is Round (p &lt; .05) from 1994:\n\n“The incidence of schizophrenia in adults is about 2%. A proposed screening test is estimated to have at least 95% accuracy in making the positive diagnosis (sensitivity) and about 97% accuracy in declaring normality (specificity)…With a positive test for schizophrenia at hand, given the more than .95 assumed accuracy of the test, the probability of a positive test given that the case is normal is less than .05, that is, significant at p &lt; .05. One would reject the hypothesis that the case is normal and conclude that the case has schizophrenia, as it happens mistakenly, but within the .05 alpha error. But that’s not the point. The probability of the case being normal, given a positive test, is not what has just been discovered however much it sounds like it and however much it is wished to be. It is not true that the probability that the case is normal is less than .05, nor is it even unlikely that it is a normal case. By a Bayesian maneuver, this inverse probability, the probability that the case is normal, given a positive test for schizophrenia, is about .60!”5\n\nThe desired interpretation of a statistically significant result induces a technical problem. The p-value provides the likelihood of observing the data under the assumption that the null hypothesis is true (a single state of the world), yet we want to interpret it as evidence about the parameter of interest given the data. After all, we did collect it, and want that to be the basis of our conclusions. But that is not the probability we have concerned ourselves with. Using the p-value as a singular basis to determine significance disregards all other possibilities that the true parameter could be. When those possibilities are imbalanced (as they were here, since only 2% of the population had schizophrenia), it confuses which state of the world is most likely given the data with how likely the data is given a state of the world (#5)."
  },
  {
    "objectID": "post/deriving-a-bernoulli-mle/index.html",
    "href": "post/deriving-a-bernoulli-mle/index.html",
    "title": "(Re)deriving a Bernoulli MLE",
    "section": "",
    "text": "Over time, I’ve found myself constantly learning new things or having new ideas, only to later forget them, then trying to relearn the same thing at a later date. So I thought it might be worthwhile to try to create a knowledge base for myself that I continually add to so that I can refer back when needed to maintain intuition. These might be little programming tricks, math and/or statistics concepts, general thoughts or ideas, etc.\nI tried to derive the MLE for a set of Bernoulli responses just to see if I still could, and got stuck. The probability mass function (PMF) is\n\\[P(Y_i = y) = p^y(1-p)^{1-y}\\] First, we need to define the likelihood function:\n\\[L(p) = \\prod_{i=1}^nP(Y_i=y)\\] Then, convert ot a log likelihood:\n\\[\n\\begin{equation}\n\\begin{split}\nlog(L(p))\n&= log(\\prod_{i=1}^nP(Y_i=y)) \\\\\n&= \\sum_{i=1}^nlog(P(Y_i=y)) \\\\\n&= \\sum_{i=1}^nlog(p^{y_i}(1-p)^{1-y_i}) \\\\\n&= log(p)\\sum_{i=1}^ny_i + log(1-p)(n - \\sum_{i=1}^ny_i)\n\\end{split}\n\\end{equation}\n\\] Next, we need to differentiate with respect to \\(p\\).\n\\[\n\\begin{equation}\n\\begin{split}\n\\frac{d}{dp}log(L(p))\n&= \\frac{d}{dp} log(p)\\sum_{i=1}^ny_i + log(1-p)(n - \\sum_{i=1}^ny_i) \\\\\n&= \\sum_{i=1}^ny_i \\frac{d}{dp} log(p) + (n - \\sum_{i=1}^ny_i) \\frac{d}{dp} log(1-p) \\\\\n&= \\sum_{i=1}^ny_i \\frac{1}{p} + (n - \\sum_{i=1}^ny_i) \\frac{1}{1-p} \\frac{d}{dp} (1-p) \\\\\n\\end{split}\n\\end{equation}\n\\] This is where my problem was. I forgot about the chain rule when taking a derivative, so I wasn’t doing the additional step of taking multiplying by the derivative of \\(1-p\\), thus things weren’t cancelling appropriately when solving for \\(p\\). It should reduce to:\n\\[\\sum_{i=1}^ny_i \\frac{1}{p} + (n - \\sum_{i=1}^ny_i) \\frac{-1}{1-p}\\] Finally, we just set that equal to zero:\n\\[\\sum_{i=1}^ny_i \\frac{1}{p} + (n - \\sum_{i=1}^ny_i) \\frac{-1}{1-p} = 0\\] After some algebra:\n\\[\\hat{p} = \\frac{\\sum_{i=1}^ny_i}{n}\\]"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2024\n\nAlexander M Zajichek, Gary L Grunkemeier, Statistical primer: propensity scores used as overlap weights provide exact covariate balance, European Journal of Cardio-Thoracic Surgery, 2024;, ezae318, https://doi.org/10.1093/ejcts/ezae318 Accepted Manuscript\n\n\n\nAminian A, Gasoyan H, Zajichek A, et al. Renoprotective Effects of Metabolic Surgery Versus GLP1 Receptor Agonists on Progression of Kidney Impairment in Patients with Established Kidney Disease. Ann Surg. Published online June 11, 2024. doi:10.1097/SLA.0000000000006379\n\n\n\nMaeve G. Macmurdo, Xinge Ji, Pratik Pimple, Amy L. Olson, Alex Milinovich, Blaine Martyn-Dow, Aman Pande, Alex Zajichek, Janine Bauman, Shaun Bender, Craig Conoscenti, David Sugano, Michael W. Kattan, Daniel A. Culver. Proportion and predictors of FVC decline in patients with interstitial lung disease. Respiratory Medicine. Volume 227. 2024. 107656. ISSN 0954-6111. https://doi.org/10.1016/j.rmed.2024.107656\n\n\n\n2023\n\nXanthopoulos A, Skoularigis J, Briasoulis A, Magouliotis DE, Zajichek A, Milinovich A, Kattan MW, Triposkiadis F, Starling RC. Analysis of the Larissa Heart Failure Risk Score: Predictive Value in 9207 Patients Hospitalized for Heart Failure from a Single Center. Journal of Personalized Medicine. 2023; 13(12):1721. https://doi.org/10.3390/jpm13121721\n\n\n\n2022\n\nChindamporn, Pornprapa & Wang, Lu & Bena, James & Zajichek, Alexander & Milinovic, Alex & Kaw, Roop & Kashyap, Sangeeta & Cetin, Derrick & Aminian, Ali & Kempke, Nancy & Foldvary-Schaefer, Nancy & Aboussouan, Loutfi & Mehra, Reena. (2022). Obesity-associated sleep hypoventilation and increased adverse post-operative bariatric surgery outcomes in a large clinical retrospective cohort. Journal of Clinical Sleep Medicine. 18. 10.5664/jcsm.10216.\n\n\n\nAltahawi, Faysal & Reinke, Emily & Briskin, Isaac & Cantrell, William & Flanigan, David & Fleming, Braden & Huston, Laura & Li, Xiaojuan & Oak, Sameer & Obuchowski, Nancy & Scaramuzza, Erica & Winalski, Carl & Zajichek, Alex & Spindler, Kurt & Jones, Morgan. (2022). Meniscal Treatment as a Predictor of Worse Articular Cartilage Damage on MRI at 2 Years After ACL Reconstruction: The MOON Nested Cohort. The American Journal of Sports Medicine. 50. 951-961. 10.1177/03635465221074662.\n\n\n\n2021\n\nSalem, Hytham & Huston, Laura & Zajichek, Alexander & Mccarty, Eric & Vidal, Armando & Bravman, Jonathan & Spindler, Kurt & Frank, Rachel & Amendola, Annunziato & Andrish, Jack & Brophy, Robert & Jones, Morgan & Kaeding, Christopher & Marx, Robert & Matava, Matthew & Parker, Richard & Wolcott, Michelle & Wolf, Brian & Wright, Rick. (2021). Anterior Cruciate Ligament Reconstruction With Concomitant Meniscal Repair: Is Graft Choice Predictive of Meniscal Repair Success?. Orthopaedic Journal of Sports Medicine. 9. 232596712110335. 10.1177/23259671211033584.\n\n\n\nAminian, Ali & Wilson, Rickesha & Zajichek, Alexander & Tu, Chao & Wolski, Kathy & Schauer, Philip & Kattan, Michael & Nissen, Steven & Brethauer, Stacy. (2021). Cardiovascular Outcomes in Patients With Type 2 Diabetes and Obesity: Comparison of Gastric Bypass, Sleeve Gastrectomy, and Usual Care. Diabetes Care. 44. dc203023. 10.2337/dc20-3023.\n\n\n\nLynch, T. Sean & Oak, Sameer & Cossell, Charles & Strnad, Greg & Zajichek, Alexander & Goodwin, Ryan & Jones, Morgan & Spindler, Kurt & Rosneck, James. (2021). Effect of Baseline Mental Health on 1-Year Outcomes After Hip Arthroscopy: A Prospective Cohort Study. Orthopaedic Journal of Sports Medicine. 9. 232596712110255. 10.1177/23259671211025526.\n\n\n\n2020\n\nSahoo, Sambit & Derwin, Kathleen & Zajichek, Alexander & Entezari, Vahid & Imrey, Peter & Iannotti, Joseph & Ricchetti, Eric & Spindler, Kurt & Strnad, Greg & Seitz, William & Gilot, Gregory & Miniaci, Anthony & Evans, Peter & Sabesan, Vani & Ho, Jason & Turan, Alparslan & Jin, Yuxuan. (2020). Associations of preoperative patient mental health status, sociodemographic and clinical characteristics with baseline pain, function and satisfaction in patients undergoing primary shoulder arthroplasty. Journal of Shoulder and Elbow Surgery. 30. 10.1016/j.jse.2020.08.003.\n\n\n\nMorita Sherman, Marcia & Louis, Shreya & vegh, deborah & Busch, Robyn & Ferguson, Lisa & Bingaman, Justin & Bulacio, Juan & Najm, Imad & Jones, Stephen & Zajichek, Alexander & Kattan, Michael & Hogue, Olivia & Blümcke, Ingmar & Cendes, Fernando & Jehi, Lara. (2020). Outcomes of resections that spare versus remove a MRI-Normal Hippocampus. Epilepsia. 61. 10.1111/epi.16694.\n\n\n\nAnis, Hiba & Emara, Ahmed & Klika, Alison & Barsoum, Wael & Bloomfield, Michael & Brooks, Peter & Higuera, Carlos & Kamath, Atul & Krebs, Viktor & Mesko, Nathan & Murray, Trevor & Muschler, George & Nickodem, Robert & Patel, Preetesh & Schaffer, Jonathan & Stearns, Kim & Strnad, Greg & Warren, Jared & Zajichek, Alexander & Piuzzi, Nicolas. (2020). The Potential Effects of Imposing a Body Mass Index Threshold on Patient-Reported Outcomes After Total Knee Arthroplasty. The Journal of arthroplasty. 36. 10.1016/j.arth.2020.08.060.\n\n\n\nAminian, Ali & Zajichek, Alexander & Tu, Chao & Wolski, Kathy & Brethauer, Stacy & Schauer, Philip & Kattan, Michael & Nissen, Steven. (2020). How Much Weight Loss is Required for Cardiovascular Benefits? Insights From a Metabolic Surgery Matched-cohort Study. Annals of surgery. 272. 639-645. 10.1097/SLA.0000000000004369.\n\n\n\nAnis, Hiba & Strnad, Greg & Klika, Alison & Zajichek, Alexander & Spindler, Kurt & Barsoum, Wael & Higuera, Carlos & Piuzzi, Nicolas. (2020). Developing a personalized outcome prediction tool for knee arthroplasty. The Bone & Joint Journal. 102-B. 1183-1193. 10.1302/0301-620X.102B9.BJJ-2019-1642.R1.\n\n\n\nSullivan, Jaron & Huston, Laura & Zajichek, Alexander & Reinke, Emily & Andrish, Jack & Brophy, Robert & Dunn, Warren & Flanigan, David & Kaeding, Christopher & Marx, Robert & Matava, Matthew & Mccarty, Eric & Parker, Richard & Vidal, Armando & Wolf, Brian & Wright, Rick & Spindler, Kurt. (2020). Incidence and Predictors of Subsequent Surgery After Anterior Cruciate Ligament Reconstruction: A 6-Year Follow-up Study. The American Journal of Sports Medicine. 48. 2418-2428. 10.1177/0363546520935867.\n\n\n\nMisra-Hebert, Anita & Milinovich, Alex & Zajichek, Alex & Ji, Xinge & Hobbs, Todd & Weng, Wayne & Petraro, Paul & Kong, Sheldon & Mocarski, Michelle & Ganguly, Rahul & Bauman, Janine & Pantalone, Kevin & Zimmerman, Robert & Kattan, Michael. (2020). Natural Language Processing Improves Detection of Nonsevere Hypoglycemia in Medical Records Versus Coding Alone in Patients With Type 2 Diabetes but Does Not Improve Prediction of Severe Hypoglycemia Events: An Analysis Using the Electronic Medical Record in a Large Health System. Diabetes Care. 43. dc191791. 10.2337/dc19-1791.\n\n\n\nArnold, Nicholas & Anis, Hiba & Barsoum, Wael & Bloomfield, Michael & Brooks, Peter & Higuera, Carlos & Kamath, Atul & Klika, Alison & Krebs, Viktor & Mesko, Nathan & Molloy, Robert & Mont, Michael & Murray, Trevor & Patel, Preteesh & Strnad, Greg & Stearns, Kim & Warren, Jared & Zajichek, Alexander & Piuzzi, Nicolas. (2020). Preoperative cut-off values for body mass index deny patients clinically significant improvements in patient-reported outcomes after total hip arthroplasty. The Bone & Joint Journal. 102-B. 683-692. 10.1302/0301-620X.102B6.BJJ-2019-1644.R1.\n\n\n\nLi, Adrienne & Zajichek, Alex & Kattan, Michael & Ji, Xinge & Lo, Katherine & Lee, Patricia. (2020). Nomogram to Predict Risk of Postoperative Urinary Retention in Women Undergoing Pelvic Reconstructive Surgery. Journal of Obstetrics and Gynaecology Canada. 42. 10.1016/j.jogc.2020.03.021.\n\n\n\nAminian, Ali & Zajichek, Alexander & Arterburn, David & Wolski, Kathy & Brethauer, Stacy & Schauer, Philip & Nissen, Steven & Kattan, Michael. (2020). Predicting 10-Year Risk of End-Organ Complications of Type 2 Diabetes With and Without Metabolic Surgery: A Machine Learning Approach. Diabetes Care. 43. dc192057. 10.2337/dc19-2057.\n\n\n\nSpindler, Kurt & Huston, Laura & Zajichek, Alexander & Reinke, Emily & Amendola, Annunziato & Andrish, Jack & Brophy, Robert & Dunn, Warren & Flanigan, David & Jones, Morgan & Kaeding, Christopher & Marx, Robert & Matava, Matthew & Mccarty, Eric & Parker, Richard & Vidal, Armando & Wolcott, Michelle & Wolf, Brian & Wright, Rick. (2020). Anterior Cruciate Ligament Reconstruction in High School and College-Aged Athletes: Does Autograft Choice Influence Anterior Cruciate Ligament Revision Rates?. The American Journal of Sports Medicine. 48. 036354651989299. 10.1177/0363546519892991.\n\n\n\nOME Cleveland Clinic Orthopaedics. Value in Research: Achieving Validated Outcome Measurements While Mitigating Follow-up Cost. J Bone Joint Surg Am. 2020 Mar 4;102(5):419-427. doi: 10.2106/JBJS.19.00531. PMID: 31868824.\n\n\n\n2019\n\nSalem, Hytham & Huston, Laura & Zajichek, Alex & Wolcott, Michelle & Mccarty, Eric & Vidal, Armando & Bravman, Jonathan & Group, MOON & Spindler, Kurt & Frank, Rachel. (2019). Meniscal Repair with Concurrent Anterior Cruciate Ligament Reconstruction: Is ACL Graft Choice Predictive of Meniscal Repair Success?. Orthopaedic Journal of Sports Medicine. 7. 2325967119S0035. 10.1177/2325967119S00359.\n\n\n\nChughtai, Morad & Schaffer, Jonathan & Barsoum, Wael & Bloomfield, Michael & Brooks, Peter & George, Joseph & Greene, Kenneth & Hampton, Robert & Higuera, Carlos & Kolczun, Michael & Krebs, Viktor & Mesko, Nathan & Molloy, Robert & Mont, Michael & Murray, Trevor & Muschler, George & Nickodem, Robert & Patel, Preetesh & Piuzzi, Nicolas & Kamath, Atul. (2019). No Evidence to Support Lowering Surgeon Reimbursement for Total Joint Arthroplasty Based on Operative Time: An Institutional Review of 12,567 Cases. The Journal of Arthroplasty. 34. 10.1016/j.arth.2019.06.028.\n\n\n\nWard, Matthew & Lee, Nahyun & Caudell, Jimmy & Zajichek, Alexander & Awan, Musaddiq & Koyfman, Shlomo & Dunlap, Neal & Zakem, Sara & Hassanzadeh, Comron & Marcrom, Samuel & Boggs, Drexell & Isrow, Derek & Vargo, John & Heron, Dwight & Siddiqui, Farzan & Bonner, James & Beitler, Jonathan & Yao, Min & Trotti, Andy & Riaz, Nadeem. (2019). A competing risk nomogram to predict severe late toxicity after modern re-irradiation for squamous carcinoma of the head and neck. Oral Oncology. 90. 80-86. 10.1016/j.oraloncology.2019.01.022.\n\n\n\nSahoo, Sambit & Ricchetti, Eric & Zajichek, Alexander & Evans, Peter & Farrow, Lutul & McCoy, Brett & Jones, Morgan & Miniaci, Anthony & Sabesan, Vani & Schickendantz, Mark & Seitz, William & Spindler, Kurt & Stearns, Kim & Strnad, Greg & Turan, Alparslan & Entezari, Vahid & Imrey, Peter & Iannotti, Joseph & Derwin, Kathleen. (2019). Associations of Preoperative Patient Mental Health and Sociodemographic and Clinical Characteristics With Baseline Pain, Function, and Satisfaction in Patients Undergoing Rotator Cuff Repairs. The American Journal of Sports Medicine. 48. 036354651989257. 10.1177/0363546519892570.\n\n\n\nAminian, Ali & Zajichek, Alexander & Arterburn, David & Wolski, Kathy & Brethauer, Stacy & Schauer, Philip & Kattan, Michael & Nissen, Steven. (2019). Association of Metabolic Surgery With Major Adverse Cardiovascular Outcomes in Patients With Type 2 Diabetes and Obesity. JAMA. 322. 10.1001/jama.2019.14231.\n\n\n\nJones, Morgan & Oak, Sameer & Andrish, Jack & Brophy, Robert & Cox, Charles & Dunn, Warren & Flanigan, David & Fleming, Braden & Huston, Laura & Kaeding, Christopher & Kolosky, Michael & Lynch, T. Sean & Magnussen, Robert & Matava, Matthew & Parker, Richard & Reinke, Emily & Scaramuzza, Erica & Smith, Matthew & Winalski, Carl & Spindler, Kurt. (2019). Predictors of Radiographic Osteoarthritis 2-3 Years after ACL Reconstruction: Data from MOON Onsite Nested Cohort. Orthopaedic Journal of Sports Medicine. 7. 2325967119S0034. 10.1177/2325967119S00348.\n\n\n\nKaeding, Christopher & Spindler, Kurt & Huston, Laura & Zajichek, Alex. (2019). ACL Reconstruction In High School and College-aged Athletes: Does Autograft Choice Affect Recurrent ACL Revision Rates?. Orthopaedic Journal of Sports Medicine. 7. 2325967119S0028. 10.1177/2325967119S00282.\n\n\n\nJones, Morgan & Reinke, Emily & Zajichek, Alexander & Kelley-Moore, JA & Khair, M. & Malcolm, Tennison & Spindler, Kurt & Amendola, Annunziato & Andrish, Jack & Brophy, Robert & Flanigan, David & Huston, Laura & Kaeding, Christopher & Marx, Robert & Matava, Matthew & Parker, Richard & Wolf, Brian & Wright, Rick. (2019). Neighborhood Socioeconomic Status Affects Patient-Reported Outcome 2 Years After ACL Reconstruction. Orthopaedic Journal of Sports Medicine. 7. 232596711985107. 10.1177/2325967119851073.\n\n\n\nLynch, T. Sean & Cossell, Charles & Strnad, Greg & Zajichek, Alex & Pignolet, Isabel & Kattan, Michael & Jones, Morgan & Spindler, Kurt & Rosneck, James. (2019). The Influence of Patient Baseline Data and Mental Health in Predicting Outcomes after Hip Arthroscopy for Femoracetabular Impingement Syndrome: A Prospective Cohort Analysis. Orthopaedic Journal of Sports Medicine. 7. 2325967119S0026. 10.1177/2325967119S00267.\n\n\n\nXanthopoulos, Andrew & Papamichalis, Michail & Zajichek, Alex & Milinovich, Alex & Kattan, Michael & Skoularigis, John & Starling, Randall & Triposkiadis, Filippos. (2019). In‐hospital red blood cell distribution width change in patients with heart failure. European Journal of Heart Failure. 21. 10.1002/ejhf.1546.\n\n\n\nLevey, Helen & Nascimento, Bruno & Miranda, Eduardo & Schofield, E. & Zajichek, A. & Kattan, Michael & Mulhall, Julia. (2019). 156 Development of Nomograms to Predict Erectile Function After Radiation Therapy. The Journal of Sexual Medicine. 16. S79-S80. 10.1016/j.jsxm.2019.01.164.\n\n\n\nVakil, Nimish & Michalopoulos, Steven & Zajichek, Alexander & Wang, Tao & Bauman, Janine & Milinovich, Alex & Daly, Thomas & Pierz, Kerri & Storen, Jeff & Kattan, Michael. (2019). Su1266 – Helicobacter Pylori Testing and Treatment in a Large, Integrated Healthcare Delivery System in the United States. Gastroenterology. 156. S-524. 10.1016/S0016-5085(19)38191-0.\n\n\n\nPiuzzi, Nicolas & Strnad, Greg & Esa, Wael & Barsoum, Wael & Bloomfield, Michael & Brooks, Peter & Higuera, Carlos & Joyce, Michael & Kattan, Michael & Klika, Alison & Krebs, Viktor & Mesko, Nathan & Mont, Michael & Murray, Trevor & Muschler, George & Nickodem, Robert & Patel, Preetesh & Schaffer, Jonathan & Spindler, Kurt & Molloy, Robert. (2019). The Main Predictors of Length of Stay After Total Knee Arthroplasty: Patient-Related or Procedure-Related Risk Factors. The Journal of Bone and Joint Surgery. 101. 1093-1101. 10.2106/JBJS.18.00758.\n\n\n\nBrooks, Michael & Misra-Hebert, Anita & Zajichek, Alexander & Carlsson, Sigrid & Hugosson, Jonas & Kattan, Michael & Stephenson, Andrew. (2019). Prospective evaluation of a new patient decision aid to enhance prostate cancer screening decision-making. Journal of Clinical Oncology. 37. 87-87. 10.1200/JCO.2019.37.7_suppl.87.\n\n\n\nSalter, Carolyn & Zajichek, Alexander & Benfante, Nicole & Kattan, Michael & Mulhall, John. (2019). MP58-05 A NOMOGRAM PREDICTING TESTOSTERONE RESPONSE IN MEN ON CLOMIPHENE. Journal of Urology. 201. 10.1097/01.JU.0000556697.01769.d2.\n\n\n\n2018\n\nBernie, Helen & Nascimento, Bruno & Miranda, Eduardo & Schofield, Elizabeth & Jenkins, Lawrence & Zajichek, Alex & Kattan, Michael & Mulhall, John. (2018). PD27-12 DEVELOPMENT OF NOMOGRAMS TO PREDICT TESTOTERONE RECOVERY AFTER CESSATION OF ANDROGEN DEPRIVATION THERAPY IN MEN WITH PROSTATE CANCER. The Journal of Urology. 199. e562. 10.1016/j.juro.2018.02.1363.\n\n\n\nJuloori, A. & Zajichek, A. & Kattan, Michael & Mullen, D. & Samson, Pamela & Woody, N.M. & Roach, M.C. & Bradley, Jeffrey & Videtic, Gregory & Robinson, Cliff & Stephans, Kevin. (2018). An Externally Validated Nomogram for Predicting Distant Metastasis after Stereotactic Body Radiation Therapy for Early-Stage Non-Small Cell Lung Cancer: Implications for Adjuvant Systemic Therapy. International Journal of Radiation Oncology/Biology/Physics. 102. S11. 10.1016/j.ijrobp.2018.06.120.\n\n\n\nDerwin, Kathleen & Sahoo, Sambit & Zajichek, Alexander & Strnad, Greg & Spindler, Kurt & Iannotti, Joseph & Ricchetti, Eric. (2018). Tear characteristics and surgeon influence repair technique and suture anchor use in repair of superior-posterior rotator cuff tendon tears. Journal of Shoulder and Elbow Surgery. 28. 10.1016/j.jse.2018.07.028.\n\n\n\nBrooks, Michael & Carlsson, Sigrid & Zajichek, Alexander & Chagin, Kevin & Hugosson, Jonas & Kattan, Michael & Stephenson, Andrew. (2018). Development of prostate-specific antigen (PSA) screening nomograms for 15-year prediction of prostate cancer diagnosis (PCDx), mortality (PCM), and all-cause mortality (ACM). Journal of Clinical Oncology. 36. 110-110. 10.1200/JCO.2018.36.6_suppl.110."
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#two-sides-of-a-coin",
    "href": "presentations/the-value-of-open-source/slides.html#two-sides-of-a-coin",
    "title": "The Value of Open-Source",
    "section": "Two Sides of A Coin",
    "text": "Two Sides of A Coin\n\n\nThe way I view it, there are two sides of the coin that are generally not differentiated\nGo out to the marketplace and see what’s out there to help you accomplish something\nI put “purchase” there intentionally\nA productivity tool, like any other sort of tool. We don’t particularly think about how it works.\nThe other side is about what actually goes into building those AI tools themselves\nData infra: How you collect, store data\nThinking: How you report and utilize information and frame problems analytically to be answered with data\nReasoning: How you deduce and infer and take action based on that information\nTools: Tools that are used to facilitate these processes\nBoils down to how data can help make informed decisions\nUltimately, these are building blocks that create advanced AI/data science tools on the left\n\n\n\n\n\n\n\nAI as Tools\n\nPre-built products like ChatGPT, Gemini, etc.\nUse (purchase) to conform to our tasks\nPerceived as productivity tools\nDominates the conversation\n\n\n\n\n\nAI as Data Science\n\nData infrastructure, analytical thinking, statistical reasoning, tools for facilitation\nHow we use data to help inform decision making\nThe building blocks of AI itself"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#blurred-lines",
    "href": "presentations/the-value-of-open-source/slides.html#blurred-lines",
    "title": "The Value of Open-Source",
    "section": "Blurred Lines",
    "text": "Blurred Lines\n\n\n\nWhat I’ve noticed in presentations, etc. is these aren’t really distinguished in the conversation. Focus on tools, talk about needing better data strategy which devolves into how we can use the existing tool for our purpose (specifically ChatGPT).\nSo many conflicting definitions of AI, and how it relates to data science. If you search Google you get so many differing pictures of it (no clear answer)\nI intentionally chose this picture because it’s just as confusing as stating the definition of AI.\nThere’s so much varying overlap that it doesn’t actually tell you anything.\nI’m glad regression analysis is actually in there where it is. Very old technique. Going into my first ML class (after having plenty of statistics training) excited to “allow machines to learn and improve without being explicitly programmed”. Regression was the first thing I learned about in my machine learning course. Regression is a core part of the ML field.\nFrom that point forward I start noticing so many redunancies between these fields, and realized it’s all basically the same thing in kind, just to varying degree\nSo it all points to: building from a solid data strategy and infrastructure will lead toward these more advanced capabilities.\n\n\n\n\n\n\nTend to focus on the first, bypassing the second; conflating views\n\n\n\nLeads to ambiguity and confusion\n\n\n\n\n\n\nFigure: The confusing nature of AI-related fields"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#the-horse-before-the-cart",
    "href": "presentations/the-value-of-open-source/slides.html#the-horse-before-the-cart",
    "title": "The Value of Open-Source",
    "section": "The Horse Before the Cart",
    "text": "The Horse Before the Cart\n\n\nSo can we take a step back and strategically think about what we can do?\nBefore spending tons of money on black box tools in hopes they work…\nWhere does the value lie?\nThis is now a somewhat old statistic, but even if we assume improvements, still a large portion of business at early stage in their data journey\nLow hanging fruit going back to basics.\nThis is not to say AI tools don’t have their place. But we should be thinking strategically about what our needs really are vs. jumping on the AI hype train and spending tons of money because we think we need to.\nYou know your business. Where the value is. A whole world of stuff out there to be utilized…\nThink about what sort of fundamental skills we can enhance and what tools we can integrate to facilitate fundamental analytical thinking.\nHaving solid foundational analytics strategy can then let you better evaluate what and when an AI productivity tool will be useful, rather than just hoping it solves your problem\nStart thinking about these things and the decisions you need to make. Is AI really the solution?\nThis is NOT to say to not use AI tools (on the productivity side) if they have clear and obvious benefit to you\n\n\n\n\n\n\nCurrent State\n\nGartner (2018): 87% of business at low analytics maturity [1]\n\n\n\n\nFigure: Gartner Analytics Maturity Model\n\n\n\n\n\n\nBack to Basics\n\nAre you capturing what’s important? (Data infrastructure)\n\n“If I knew X at time Y, I could do Z”\n\nHow are you using your data? (Reporting/analytical thinking)\nWhat are your limitations? (Tools, skills, time, etc.)\nAre AI tools really the solution?\n\n“AI for the sake of AI is a losing proposition” [2]. Be intentional!"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#what-is-open-source",
    "href": "presentations/the-value-of-open-source/slides.html#what-is-open-source",
    "title": "The Value of Open-Source",
    "section": "What Is Open-Source?",
    "text": "What Is Open-Source?\n\n\nIt is software that is first and foremost free, but also open, so you know exactly how it works\nAlthough open source software can and is built by companies, I think of it as community developed. Namely that the users of the software are building it.\nSo my favorite way to think about what open source is is like a prestine workshop with all the tools you can imagine available. Now it is up to you to build what you want with them to serve your needs. I specifically include “directions” to emphasize that not only you can use the tools, but a plethora of resources to guide you and understand how you do and can use them.\nWhen I talk about open source, I’m implying code-first. In my opinion it is the best way. Allows you to build exactly what you need. Pre-made, point and click tools always have their limitations and restrictions. Not to mention the cost…\nData science in general is as much an art as science. You need to create things in a way that is useful.\nGives you the groundwork and foundation to facilitate the way you want to solve problems. Use it how you want, instead of trying to conform your problems to how pre-existing tools work.\nSyntax to communicate your analytical ideas\nGiven that it’s free, no matter who you are you can start. Just install the software. So it gives empowerment and flexibility for an individual to start using it in their own work, maybe locally, and see how it helps them solve potentially small problems. Build this foundation and iterate towards more widespread use.\n\n\n\n\n\n\nBackground\n\nIn essence, free and open software (Wikipedia)\nThink of as community built\nLike a workshop for your raw materials (with directions)\n\n\n\n\n\nBenefits in Data Science\n\nCode-first approach gives flexibility and control (art + science)\nUse it to facilitate analytical approach\n\nBuilding blocks to AI\n\nLow-cost iteration and experimentation (anyone can do it)\n\n\n\n\n\nToday’s emphasis:: R, Quarto, Shiny."
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#the-r-programming-language",
    "href": "presentations/the-value-of-open-source/slides.html#the-r-programming-language",
    "title": "The Value of Open-Source",
    "section": "The R Programming Language",
    "text": "The R Programming Language\n\n\nAs compared to object oriented, though R also has OOP paradigm\nDatabase connectivity, complex data manipulation, advanced statistical modeling, tables, graphs, interactivity, web applications, maps, etc.\nIf you look through the package list, there may be packages readily available for your use case\n\n\n\n\n\n\nBackground\n\nA free and open-source functional programming language\nDeveloped in the 1990’s for statistical computing, but has long since expanded to much broader usage\nAdvanced through packages developed by the community (Package list)\nCommonly used in the RStudio IDE\n\n\n\n\n\nExample\n\nlibrary(plotly) # Load package\nmy_data &lt;- trees # Assign object (dataset)\nplot_ly(\n  data = my_data,\n  x = ~Height,\n  y = ~Girth,\n  size = ~Volume,\n  color = ~Volume,\n  text = ~paste0(\"Height: \", Height, \"&lt;br&gt;Girth: \", Girth, \"&lt;br&gt;Volume: \", Volume), height = 300, width = 500\n)"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#quarto-for-reproducible-documents",
    "href": "presentations/the-value-of-open-source/slides.html#quarto-for-reproducible-documents",
    "title": "The Value of Open-Source",
    "section": "Quarto for Reproducible Documents",
    "text": "Quarto for Reproducible Documents\n\n\nThese slides and the website it is contained in, all built with Quarto (for free)\nUsed the examples in the previous slides as meta-examples of why Quarto is awesome.\nUse these open source tools to easily deliver powerful interactive analytical content.\nEmbed code chunks for reproducible reporting straight in your document\nImagine now this is connected to your database…\n\n\n\n\n\n\nBackground\n\nQuarto is an open source technical publishing system\nBuild custom analytical documents in programmatic way\nVehicle for dissemination, promoting automation and reproducibility\nIntegrates well with R, Python, and many other tools\n\nThis presentation (and website that it’s contained in) are built in Quarto (see source code)\n\n\n\n\n\nYAML (header/metadata)\n---\ntitle: \"The Value of Open-Source\"\nsubtitle: \"Leveraging free, code-first tools to iterate toward advanced analytics\"\ninstitute: \"Research Data Scientist, Cleveland Clinic\"\nauthor: \"Alex Zajichek\"\ndate: \"2025-02-27\"\ndate-format: long\nformat:\n  revealjs: \n    theme: [serif, custom.scss]\n    footer: \"&lt;em&gt;AI Innovations at Work Conference 2025&lt;/em&gt;\"\n    slide-number: true\n    incremental: true\n---\nMarkdown (body)\n### Background\n\n- [Quarto](https://quarto.org/) is a open source technical publishing system\n- Build custom analytical documents in programmatic way\n- Vehicle for dissemination, promoting automation and reproducibility"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#shiny-for-web-applications",
    "href": "presentations/the-value-of-open-source/slides.html#shiny-for-web-applications",
    "title": "The Value of Open-Source",
    "section": "Shiny for Web Applications",
    "text": "Shiny for Web Applications\n\n\nGood example of separating AI tools from the foundational data strategy stuff: building applications that utilize AI tools\n\n\n\n\n\n\nBackground\n\nShiny is an R package for building custom, interactive web applications\n\nYou can do it in Python too!\n\nAllows users to interact with data, analytics, models, etc. however you see fit\n\n\n\n\nExample: Deploying an LLM-powered Shiny app\n\n\n\n\n\n\nExample (in R)\nlibrary(shiny) # Load package\n\n# The user interface\nui &lt;- \n  fluidPage(\n    title = \"MyApp\", \n    sidebarLayout(\n      sidebarPanel(\n        selectInput(\n          inputId = \"color\",\n          label = \"Choose Color\",\n          choices = c(\"red\", \"blue\", \"green\")\n        )\n      ),\n      mainPanel(\n        plotOutput(\"my_plot\")\n      )\n    )\n  )\n\n# How the inputs turn to outputs\nserver &lt;- \n  function(input, output) {\n    \n    output$my_plot &lt;-\n      renderPlot({\n        with(trees, plot(Height, Girth, col = input$color))\n      })\n    \n  }\n\n# Run the app\nshinyApp(ui, server)"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#deployment-and-integration",
    "href": "presentations/the-value-of-open-source/slides.html#deployment-and-integration",
    "title": "The Value of Open-Source",
    "section": "Deployment and Integration",
    "text": "Deployment and Integration\n\n\nSo you can see when you start using programming languages like R (or Python), in combination with flexible tools like Quarto and custom applications with Shiny, you have all sorts of tools in your toolbox to dream up whatever is going to fit your needs. Out of the box, this is all free so far.\nThe last piece to touch on is how you can start to share/disseminate your work too. When starting out, most likely making these things locally. Ultimately you want other people to see your reports, use the web applications, etc. Even this can be free (to an extent).\nThe beautiful thing is that it can remain free only until you need it not be\nUse what works\nPay as needed\nPosit Connect Cloud (now in beta)\nshinyapps.io\nShiny Server\nQuarto Pub\nIndex\n\n\n\n\n\n\nHow can we share it?\n\nStart with free; iterate and pay as needed\n\nNot all or nothing!\n\nRepeat: use to facilitate analytical strategy\n\n\n\n\n\nOptions to get started\n\n\nPosit Connect Cloud: Deploy Shiny apps, Quarto docs, and more to the web for free from GitHub\n\nNow in beta with private deployment options (for low cost)\n\nshinyapps.io: Deploy Shiny apps to the web; free basic plan\n\nPay for more features like security, domains, and authentication\n\nQuarto Pub: Freely publish Quarto output to the web for public access\nShiny Server: Open source (free) server to deploy Shiny apps and other content\n\nInstall on premises or the cloud\nIntriguing option for orgs new to data science"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#how-can-i-learn",
    "href": "presentations/the-value-of-open-source/slides.html#how-can-i-learn",
    "title": "The Value of Open-Source",
    "section": "How can I learn?",
    "text": "How can I learn?\n\n\nWhat we saw here was a very narrow glimpse into what these tools can do. So how can you learn more yourself?\nFirst, just install them–they are free!\nTake the code from these slides even and run it yourself\nAgain, the workshop analogy. You probably need to be curious and want to learn new skills, but in my opinion it’s the best way to build a foundation.\nStart task based: try reproducing some very small little task, like creating a graph programmatically that you’d normally do in Excel\nYou start to figure things out as you go, which allows you to iterate over time and continuing building and expanding the capabilities\nStart with free stuff, and only incur costs as needed for solutions you are building and need\nUse AI tools to help you learn!\nPrompt: “Make me an app template to dynamically explore customers on a map with various filters”\n\n\n\nInitial strategyFree resourcesUse AI tools!\n\n\n\nFirst, go install them and mess around\n\n\n\nMake a (small) tangible goal, figure it out, then iterate\n\nDocument everything\n\n\n\n\nFocus on automation, reporting and reproducibility to start (low hanging fruit)\n\nBuild the foundation for analytics maturity\n\n\n\n\n\nR for Data Science by Hadley Wickham\nAn Introduction to Statistical Learning for statistics and data science foundations\nA free R course on QuantFish\nEndless YouTube resources for R, Quarto, Shiny…and everything else\nCommunity boards like Posit Community, Stack Overflow, etc.\nVisit Posit PBC’s website and learning resources\n\nMajor developer/maintainer of many tools we talked about today\n\n\n\n\n\n\n\n\nChatGPT\n\nA companion for any and all questions along the way\n\n\n\n\n\n\n\n\nShiny Assistant\n\nAn LLM interface to specifically help you develop Shiny apps\nQuickly get a template started for your concept\nLive demo: https://gallery.shinyapps.io/assistant/"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#what-is-it",
    "href": "presentations/the-value-of-open-source/slides.html#what-is-it",
    "title": "The Value of Open-Source",
    "section": "What is it?",
    "text": "What is it?\n\n\nInteresting thought experiment for what “AI” is.\n\n\n\n\n\n\nBackground\n\nriskcalc.org is a free-to-use repository of risk calculators for individualized medical decision making (~10-15K users/month)\nEmbedded with published predictive models\nEach calculator is a Shiny application\nMajority are regression models"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#the-backend",
    "href": "presentations/the-value-of-open-source/slides.html#the-backend",
    "title": "The Value of Open-Source",
    "section": "The Backend",
    "text": "The Backend\n\n\n\n\n\nFigure: Tools supporting riskcalc.org\n\n\n\n\n\nKey points\n\nCan build working infrastructure for free (to start)\n\n\n\nIncur cost for service as needed\n\n\n\nAWS versus on premises"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#acknowledgements",
    "href": "presentations/the-value-of-open-source/slides.html#acknowledgements",
    "title": "The Value of Open-Source",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\nCo-developers\n\n\nAlex Milinovich\nXinge (Kathy) Ji\nBlaine Martyn-Dow\n\n\n\nIn Memory\n\n\nMichael W. Kattan, PhD  Department Chair (2004-2024)  Quantitative Health Sciences  Cleveland Clinic\n\nCreator of riskcalc.org"
  },
  {
    "objectID": "presentations/the-value-of-open-source/slides.html#references",
    "href": "presentations/the-value-of-open-source/slides.html#references",
    "title": "The Value of Open-Source",
    "section": "References",
    "text": "References\n\n\nhttps://www.gartner.com/en/newsroom/press-releases/2018-12-06-gartner-data-shows-87-percent-of-organizations-have-low-bi-and-analytics-maturity\nhttps://www.kearney.com/service/digital-analytics/ai-assessment-aia-2024-the-drive-for-greater-maturity-scale-and-impact\n\n\n\n\n\nAI Innovations at Work Conference 2025"
  },
  {
    "objectID": "presentations/an-assessment-of-non-traditional-regression-models-for-count-data/index.html",
    "href": "presentations/an-assessment-of-non-traditional-regression-models-for-count-data/index.html",
    "title": "An assessment of non-traditional regression models for count data",
    "section": "",
    "text": "Slides\n\n\nLink to slideshow"
  }
]