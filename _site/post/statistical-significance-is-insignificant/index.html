<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Zajichek">
<meta name="dcterms.date" content="2023-12-22">
<meta name="description" content="“A cheap way to get marketable results” -William Kruskal">

<title>Statistical significance is…insignificant – Zajichek Stats</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-9MW3W3RQD9"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-9MW3W3RQD9', { 'anonymize_ip': true});
</script>


<meta property="og:title" content="Statistical significance is…insignificant – Zajichek Stats">
<meta property="og:description" content="“A cheap way to get marketable results” -William Kruskal">
<meta property="og:image" content="https://www.zajichekstats.com/post/statistical-significance-is-insignificant/feature.png">
<meta property="og:site_name" content="Zajichek Stats">
<meta property="og:image:height" content="400">
<meta property="og:image:width" content="398">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:alexzajichek@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/alexzajichek"> <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zajichek"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/@centralstatz"> <i class="bi bi-youtube" role="img" aria-label="YouTube">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Statistical significance is…insignificant</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">History</div>
    <div class="quarto-category">Philosophy</div>
    <div class="quarto-category">Research</div>
  </div>
  </div>

<div>
  <div class="description">
    “A cheap way to get marketable results” -William Kruskal
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Alex Zajichek </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 22, 2023</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>The longer I’ve been practicing as a statistician, maybe paradoxically, the more skeptical I’ve become of statistical significance (<a href="#sidenotes">#1</a>). It manifests as a feeling of dissatisfaction, as if, even though you’ve stated what you “found”, you don’t <em>actually</em> believe it to be true. I recently finished reading <a href="https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2"><em>The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives</em></a>–it instantly became one of my favorite books (here are my <a href="#favoritequotes">favorite quotes and passages</a>). It affirms a lot of what I’ve come to suspect, with deep articulation about the vastness of the issue, backed by a thorough historical foundation. I can’t help but wonder about the broader scientific, political, and societal implications this has had over the years (and continues to have). It really lit a fire in me to continue learning about and unraveling statistical history to connect those dots.</p>
<section id="my-take" class="level1">
<h1>My take</h1>
<p>The <em>significance</em> of a statistical result cannot be mechanically, mathematically, or systematically determined. It must be driven by a <em>practical</em> relevance, or importance, which is inherently subjective, and a product of the values, beliefs, interests, and/or goals of the individual(s) interpreting the data. That result is always subject to dispute, critique, replication, and refinement, whether those reasons are process error (<a href="#sidenotes">#2</a>), or the value of the information itself. The chance occurrence of a sampling probability crossing an arbitrary threshold is actually irrelevant.</p>
</section>
<section id="whatisstatsig" class="level1">
<h1>What is statistical significance?</h1>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 20px; color: #2e5c46">“It’s embedded like a tax code in the bureaucracy of science.”</span><sub>1</sub></p>
</blockquote>
<p>Technically speaking, it is when the likelihood of observing our data, <em>if</em> an hypothesized state of the world were true (known as the <em>p-value</em>), is so small (<a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913">notoriously</a>, and most often, less than 5%), that the hypothetical state of the world must be false, and therefore, we have “significant” statistical evidence to say so. It positions itself as an objective tool to <em>decide</em> (on the basis of this probability threshold) whether a statistical relationship is “real”, and, often, subsequently that it “matters”.</p>
<p>Take this <a href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808358">recent study</a>, for example, which is meant to characterize physician-propagated misinformation about the COVID-19 pandemic. The authors outline a set of <a href="https://cdn.jamanetwork.com/ama/content_public/journal/jamanetworkopen/939195/zoi230834supp1_prod_1697557763.1365.pdf?Expires=1704642946&amp;Signature=nxggsGG3f~BQcev3DoFQmbURh3vsb1CtFrP4rSviM1XaF8Y9vtyGPmRRBTRDAXyvYzrGvW6vrJFsphYDRTI9LSJD35NYEc8RUdkZK8fJkKcSpr-AbsW1wyhe30CUf-x8GGPI2For6nZNLWoZhBn0m~GrC3JlmuTmCswv~3RH7HolcYV10ZTVgSh4ZvGaBOUKDdNhmITsHrocrTct-xvMnohwhM~6~nHZMATo6~grFfhnPrhgsDHRVkYdLr9o8yFEae4-ylEnzkulOfigZIKZJsj1s5iBbyPAB60k2KYYkXlBZnNAUpkrKSP33m9BiG8YY047fQOSRrqMrap-PaA45w__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">basic premises</a> that are used as a basis for classifying contrarian statements as <em>misinformation</em> (<a href="#sidenotes">#3</a>). At least some of this is built upon the attainment of statistical significance (or lack thereof).</p>
<p>As an example, in the category of <strong>Promoting Unapproved Medications for Prevention or Treatment</strong> (in the Results section), the authors state:</p>
<blockquote class="blockquote">
<p><span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #9e3634">“The 2 most prominent medications promoted were ivermectin and hydroxychloroquine, which have been found to not be effective at treating COVID-19 infections in randomized clinical trials.”</span><sub>2</sub></p>
</blockquote>
<p>This premise drove them to classify social media posts like this:</p>
<blockquote class="blockquote">
<p><span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #45A4CE">“Two of my toughest COVID patients–showed up with oxygen stats of 68% and 84% and would not go to the hospital. We treated them with IVM, steroids, and breathing treatments and here they are now.”</span><sub>2</sub></p>
</blockquote>
<p>as misinformation (see all <a href="https://cdn.jamanetwork.com/ama/content_public/journal/jamanetworkopen/939195/zoi230834t4_1697557763.5423.png?Expires=1704648605&amp;Signature=vIdMdVOE4XQ76IbpK3v-OY0EzN5zbPooMwAeWZTXhWDu5fJnx4hpErMTWryrzYEaJOYO6YckYQIvSqmFFHp7LJ~8NughK380U2JDc2PBtonwbYYmVcXzRXT~oLftOZEXNxq0MWHqDFs1Ov7KNUdoqd1TuhYxCgFKWUvhdb5pXzB0zliNP-28kjQwZF9KLM70oerRsri0XM-HVfdKkPKrM1idAtUBFAZzBDg05y5BBQD8cLzgW4Fa7cINV-~1Dhyg9HpncVHeRACdc-tJwqqs4Z-VQfJPXMgzxP2-eyL6nhSA3IqIW9ax8CYgcYrha6yqq80wqaR4zTTGmXzf~rTWmQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">supportive quotes</a>). This is an actual doctor saying the drug helped <em>their</em> patients, but the authors have deemed it ineffective. What justifies them making such a universal claim?</p>
<p>If you look at <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8394824/">one of their references</a>, the <a href="https://en.wikipedia.org/wiki/Meta-analysis">meta-analysis</a> shows the <a href="https://en.wikipedia.org/wiki/Relative_risk">relative risk</a> for all-cause mortality was estimated to be 37%. That is, the risk of death was 63% lower in patients who received ivermectin versus placebo or standard of care. However, because the 95% <a href="https://en.wikipedia.org/wiki/Confidence_interval">confidence interval</a> ranged from 12% to 113% (i.e., there was <em>plausibility</em> that ivermectin could produce up to a 13% <em>worse</em> mortality rate, but equally plausible an 88% risk reduction), it was deemed <em>not</em> statistically significant, and as the authors state:</p>
<blockquote class="blockquote">
<p><span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #3d2f2f">“IVM [ivermectin], compared with control treatment, did not have an effect on the all-cause mortality rate.”</span><sub>3</sub></p>
</blockquote>
<p>and ultimately,</p>
<blockquote class="blockquote">
<p><span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #3d2f2f">“Ivermectin is not a viable treatment option for COVID-19.”</span><sub>3</sub></p>
</blockquote>
<p>In other words, because the probability of observing this data, under the assumption of no difference in mortality risk (our p-value definition above), was not less than 5% (it was 31%), that gives reason to conclude <em>no difference at all</em> (<a href="#sidenotes">#4</a>). Furthermore, if the confidence interval crossed 100% by any amount, no matter how small, the p-value would have remained above 5% and not reached the threshold for statistical significance.</p>
</section>
<section id="why-is-it-flawed" class="level1">
<h1>Why is it flawed?</h1>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 20px; color: #2e5c46">“Real science changes one’s mind. That’s one way to see that the proliferation of unpersuasive significance tests is not real science.”</span><sub>1</sub></p>
</blockquote>
<section id="arbitrarythreshold" class="level2">
<h2 class="anchored" data-anchor-id="arbitrarythreshold">An arbitrary threshold</h2>
<p>The 5% threshold is arbitrary. Despite that common acknowledgement, willful ignorance tends to prevail due to tradition and adherence to norms. The fact that the perceived significance of a result can suddenly change from minute differences speaks to the lack of robustness in the logic. In <a href="https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2">the book</a>, the authors frequently discuss the importance of a <em>loss function</em>, which focuses on the potential consequences and implications of the result on the real-world decisions that are sought to be made from the information, rather than a predefined threshold based on sampling error probability. In this sense, the allowable risk tolerance can’t be objectively or mechanically determined. It is context-dependent, and not all decisions are created equal. Yes, the p-value <a href="#whatisstatsig">above</a> was 31%, but that error rate, along with the plausible range of risks (and benefits), may be sufficient to someone needing to make a treatment decision <em>now</em>.</p>
<section id="risks-are-subjective" class="level3">
<h3 class="anchored" data-anchor-id="risks-are-subjective">Risks are subjective</h3>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 18px; color: #2e5c46">“It always depends on the loss, measured in side effects, treatment cost, death rates. The loss to a cool, scientific, impartial spectator will not be the same as the loss to the patient in question…[the balance between Type I/II errors] ‘must be left to the patient, friends, and family’.”</span><sub>1</sub></p>
</blockquote>
<p>Beyond the statistical significance of a result is the question of what to do about it. In the <a href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808358">same article</a>, the authors state the following, still in the context of misinformation:</p>
<blockquote class="blockquote">
<p><span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #9e3634">“Claims that myocarditis was common in children who received the vaccine and that the risks of myocarditis outweighed the risk of vaccination were also unfounded.”</span><sub>2</sub></p>
</blockquote>
<p>Nevermind the fact that the <a href="https://jamanetwork.com/journals/jama/fullarticle/2782900">study they reference</a> <em>does</em> show an increase in monthly case volume of myocarditis and pericarditis between pre/post-vaccine periods and the authors state:</p>
<blockquote class="blockquote">
<p><span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #9e8a39">“Myocarditis developed rapidly in younger patients, mostly after the second vaccination. Pericarditis affected older patients later, after either the first or second dose.”</span><sub>4</sub></p>
</blockquote>
<p>The more important point is that the weight individuals place on statistical results to inform their decision making is subjective. The risk may be low, maybe even lower than the alternative, but that doesn’t inform <em>how</em> someone should weigh it.</p>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46">“Imagine that you and your infant child are standing on a sidewalk near a busy street. You have just purchased a hot dog from the street vendor and have safely crossed the street. Scenario 1: You suddenly realize you have forgotten the mustard and if you scurry across the busy street, dodging vehicles, there is a 95% probability you’ll return safe with your mustard. Scenario 2: You forgot your child and you watch as she tries to cross the street herself, if you scurry across the busy street, dodging vehicles, there is a 95% probabiliity you’ll return safe with your child. The sizeless scientist in effect declares ‘they are equally important reasons for crossing the street’”</span><sub>1</sub></p>
</blockquote>
</section>
</section>
<section id="samplesize" class="level2">
<h2 class="anchored" data-anchor-id="samplesize">It can’t depend on sample size</h2>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 18px; color: #2e5c46">“At high sample sizes, all null hypotheses are rejected, by mathematical fact, without having to look at the data.”</span><sub>1</sub></p>
</blockquote>
<p>One pretty simple argument is that of <a href="https://www.omniconvert.com/what-is/sample-size/">sample size</a>. In most contexts, a statistical test is, by definition, more likely to be declared <em>significant</em> by simply <a href="https://en.wikipedia.org/wiki/Standard_error">amassing more data</a>, regardless of what the actual effect size is. This, on the other hand, <em>is</em> completely mechanical and dissociated from the real-world context in which the test is being run. Thus, it prioritizes quantity over substance, and when blindly used, potentially promotes results that may lack practical meaning.</p>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46">“…some cause of natural selection may have a high probability of replicability in additional samples but be trivial. Yet a cause may have a low probability of replicability but be important. This is what we mean when we say that a test of significance is neither necessary nor sufficient for a finding of importance”</span><sub>1</sub></p>
</blockquote>
<p>It also tends to shift focus to attaining statistical significance and using it as a filter, causing the potential to miss meaningful insights that didn’t reach this level.</p>
</section>
<section id="we-dont-believe-in-zero-sized-effects" class="level2">
<h2 class="anchored" data-anchor-id="we-dont-believe-in-zero-sized-effects">We don’t believe in “zero-sized” effects</h2>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 18px; color: #2e5c46">“Real scientists draw a line between what is large and small.”</span><sub>1</sub></p>
</blockquote>
<p>There is a major contradiction that arises.</p>
<p>The typical hypothesis test is conducted under the assumption of a <a href="https://en.wikipedia.org/wiki/Null_hypothesis">null hypothesis</a> positing <em>no effect</em>. For example, in calculating the p-value <a href="#whatisstatsig">above</a>, it is assumed that there is <em>no</em> difference in all-cause mortality rates between the treatment groups. However, I would argue that in any practical context, it’s rare that someone would genuinely believe in the existence of precisely zero effect. Rather, it would stand to reason that what they really mean is “effectively zero” effect, something so small that it is considered inconsequential.</p>
<p>Herein lies the contradiction: they have now acknowledged some level of substantive significance, albeit undefined. If the true effect happens to be smaller than this threshold, as we <a href="#samplesize">just explained</a>, the estimate will still eventually be declared statistically significant with mathematical certainty no matter how minuscule, thus inevitably crossing the unspoken threshold of substantive meaning. Therefore, this begs into question the value of attaining statistical significance at all in favor of the need for explicit consideration of the real-world implications (i.e., the <a href="#arbitrarythreshold">loss function</a>). At the <em>very least</em>, the substantive threshold should be identified and reflected in the null hypothesis so that the p-value is calibrated for substance.</p>
</section>
<section id="fallacy" class="level2">
<h2 class="anchored" data-anchor-id="fallacy">The fallacy of the transposed conditional</h2>
<p>This is where it gets especially interesting. There are logical errors with the conclusions drawn from hypothesis testing. I think the best way to describe it is jumping into the classic example that arises in Jacob Cohen’s <a href="https://doi.org/10.1037/0003-066X.49.12.997"><em>The Earth Is Round (p &lt; .05)</em></a> from 1994:</p>
<blockquote class="blockquote">
<p><span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #45A4CE">“The incidence of schizophrenia in adults is about 2%. A proposed screening test is estimated to have at least 95% accuracy in making the positive diagnosis (sensitivity) and about 97% accuracy in declaring normality (specificity)…With a positive test for schizophrenia at hand, given the more than .95 assumed accuracy of the test, the probability of a positive test given that the case is normal is less than .05, that is, significant at p &lt; .05. One would reject the hypothesis that the case is normal and conclude that the case has schizophrenia, as it happens mistakenly, but within the .05 alpha error. But that’s not the point. The probability of the case being normal, given a positive test, is not what has just been discovered however much it sounds like it and however much it is wished to be. It is not true that the probability that the case is normal is less than .05, nor is it even unlikely that it is a normal case. By a Bayesian maneuver, this inverse probability, the probability that the case is normal, given a positive test for schizophrenia, is about .60!”</span><sub>5</sub></p>
</blockquote>
<p>The desired interpretation of a statistically significant result induces a technical problem. The p-value provides the likelihood of observing the data under the assumption that the null hypothesis is true (a single state of the world), yet we <em>want</em> to interpret it as evidence about the parameter of interest given the data. After all, we did collect it, and want that to be the basis of our conclusions. But that is not the probability we have concerned ourselves with. Using the p-value as a singular basis to determine significance disregards all other possibilities that the true parameter could be. When those possibilities are imbalanced (as they were here, since only 2% of the population had schizophrenia), it confuses which state of the world is most likely given the data with how likely the data is given a state of the world (<a href="#sidenotes">#5</a>).</p>
</section>
</section>
<section id="what-to-do-instead" class="level1">
<h1>What to do instead?</h1>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 20px; color: #2e5c46">“Real science, unlike significance-testing science, is difficult. If it were not, it would not be real science, but instead it would be already established routine. Real science asks you to make real scientific judgements and real scientific arguments within a community of other scientists. It asks you to be quantitatively persuasive, not to be irrelevantely mechanical. Life is hard.”</span><sub>1</sub></p>
</blockquote>
<p>It’s a scary thing to think about. Suppose statistical significance isn’t there to bail you out. What are you supposed to do? How do you know if your results matter or not? I think this passage gives a pretty clear answer:</p>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 16px; color: #2e5c46; font-weight: bold">“She can test her belief in the price effect by looking at the magnitudes, using, for example, the highly advanced technique common in data-heavy articles in physics journals: ‘interocular trauma’. That is, she can look and see if the result hits her between the eyes.”</span><sub>1</sub></p>
</blockquote>
<p>The premise of this article has been that the implications of statistical results are context-dependent, so there isn’t a one-size-fits-all alternative to replace statistical significance. Rather than seeking a systematic approach, the emphasis should be placed on cultivating understanding of the subject matter. It’s akin to relying on intuition, like a feeling of “knowing” that you’ve gotten what you needed. Take this simple analogy: a tape measure is a tool that quantifies information needed to inform subsequent action, and the precision of the measurement is tailored to the specific needs of the task at hand. Sometimes a rough estimate is sufficient, while other times meticulous precision is necessary. The goal is to reach the point where, intuitively, you “know” that you’ve obtained the necessary information to move forward confidently. I see statistics as the same thing. Merely a <em>tool</em> to be used to quantify the desired information needed to <em>inform</em> (i.e., augment, not determine) a decision.</p>
<p>Now I’m not going to claim that I haven’t repeatedly violated the practices I’m arguing against, it’s hard not to, but these are things that I’m going to focus more on moving forward instead of p-values and statistical significance:</p>
<section id="estimation-magnitude" class="level3">
<h3 class="anchored" data-anchor-id="estimation-magnitude">1. Estimation &amp; magnitude</h3>
<p>This is probably the easiest change to start making because it doesn’t require an overhaul of statistical methods, but rather just a shift in focus to the magnitude of the estimates. By deliberately avoiding p-value calculations (and, when reading and consuming research, simply ignoring the concept of statistical significance altogether), the interpretation is governed by (a plausible range of) effect sizes, untainted by arbitrary, context-agnostic significance thresholds, and thus forces a scientific argument to be made on that basis. With a little extra brain power (and humility), this creates a much more contextually-rich, informative interpretation.</p>
</section>
<section id="bayesian-thinking-causal-modeling" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-thinking-causal-modeling">2. Bayesian thinking &amp; causal modeling</h3>
<p>Richard McElreath’s <a href="https://github.com/rmcelreath/stat_rethinking_2023"><em>Statistical Rethinking</em></a> really convinced me that causal inference powered by Bayesian estimation is probably the best framework out there for scientific modeling (and I’ve only made it through the <a href="https://www.zajichekstats.com/post/statistical-rethinking-2023-class-notes/">first couple of chapters</a> so far). It completely shifts the focus from the data itself to the data-generating <em>process</em>, putting the bulk of the hard work upfront, before data is collected, with a focus on mechanism and structure. It also addresses the <a href="#fallacy">fallacy problem</a>. However, it’s definitely harder to start doing on a whim.</p>
<p>First of all, the <a href="https://en.wikipedia.org/wiki/Bayesian_statistics">math itself</a> is different from typical <a href="https://en.wikipedia.org/wiki/Frequentist_inference">frequentist</a> methods, so there is a learning curve there. More difficult though is navigating the <em>practical</em> complexities, such as properly eliciting the necessary subject matter expertise and piecing that together into coherent <a href="https://en.wikipedia.org/wiki/Prior_probability">prior distributions</a> and <a href="https://en.wikipedia.org/wiki/Causal_model">causal models</a>. Nevermind the technical reasons why that is hard, it is simply more demanding from a time, brainpower, and collaboration perspective–and everyone is busy. Nevertheless I think it is a worthy pursuit (<a href="#sidenotes">#6</a>).</p>
</section>
<section id="decision-making-course-of-action" class="level3">
<h3 class="anchored" data-anchor-id="decision-making-course-of-action">3. Decision-making &amp; course of action</h3>
<p>This is where the <a href="#arbitrarythreshold">loss function</a> is most relevant.</p>
<p>Instead of contorting a generic statistical result to tenuously align with real-world implications, I want to be more deliberate. The first step is to target and understand the tangible decision-making processes that the estimates seek to inform, with an identification of the current standards including practical constraints and nuances. Then, rather than passively using standard techniques, deriving tailored statistical methods to facilitate that usage, which may prompt more rigor, customization, or reframing of the statistical problem entirely to suit the specific context at hand. Estimation uncertainty can be fed as input into hypothetical scenarios to gain insight into where/what actions will be triggered and their subsequent downstream effects on the hard outcomes intended to be impacted. At that point, the <em>significance</em> will be clear.</p>
<section id="focus-on-the-end-product" class="level4">
<h4 class="anchored" data-anchor-id="focus-on-the-end-product">Focus on the end-product</h4>
<p>I think a critical piece to this endeavor is to not only focus on the statistics, but also <em>how</em> they will be disseminated. This means specifying the vehicle that will deliver the information to the right person at the right time. The emphasis on something tangible elicits certain practical and technological constraints that may be otherwise unbounding when focusing solely on the math. Further, this perspective acknowledges that the statistical methods are only a fragment of the overall data product, and may be direct cause for further refinement of the statistical approach itself. That is, even with robust statistical methods or results, the information may lose its utility if poorly conveyed or implemented. This could be due to anything from data pipelines and visualization to deployment and computing resources. This also enables the ability to be more forward-thinking about success measures and accountability/validation schemes like continuous monitoring to ensure sustained yet impactful presence in the intended decision-making context.</p>
</section>
</section>
</section>
<section id="some-historical-gold" class="level1">
<h1>Some historical gold</h1>
<p>To conclude this, I wanted to highlight an excerpt from the chapter <em>The Psychology of Psychological Significance Testing</em> in <a href="https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2">the book</a> that I found especially fascinating about the propagation of statistical significance across university education in the United States (pages 142-143):</p>
<blockquote class="blockquote">
<span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46">
<p>
“In this context the 5 percent science was promoted by the new leaders of quantitative psychology and education. European humanists can score themselves by how many generations they are removed from Hegel–that is, in being taught by a teacher who was taught by a teacher who was taught by a teacher who was taught by Hegel at the University of Berlin. Likewise, statisticians can score themselves by how many generations they are from Fisher. Quinn McNemar, for example, of Stanford University, was an important teacher of psychologists who had himself studied statistical methods at Stanford with Harold Hotelling, the chief American disciple of Fisher. Hotelling had worked directly with Fisher. McNemar then taught L.G. Humphreys, Allen Edwards, David Grant, and scores of others. As early as 1935 all graduate students in psychology at Stanford, following the model of Iowa State, were required to master Fisher’s crowning achievement, analysis of variance. Already by 1950, Gigerenzer et al.&nbsp;reckon, about half of the leading departments of psychology required training in Fisherian methods.
</p>
<p>
Even rebels against Fisher were close to him, starting with [William Sealy] Gosset himself. Palmer Johnson of the University of Minnesota studied with Fisher in England, though he later had the bad taste to write articles with Fisher’s erstwhile colleague and eternal enemy Jerzy Neyman, whom Fisher had cast into outer darkness. George Snedecor, an agricultural scientist at Iowa State University at Ames, was a cofounder of the first department of statistics in the United States. His important book <em>Statistical Methods</em> was influenced directly by Fisher himself, who somewhat surprisingly was in the 1930s a visiting professor of statistics at Iowa State. One can think of the Iowa schools then [1940s and 1950s] as one thinks of London’s Gower Street in the 1920s and 1930s–a crucial crossroads of statistical methods and training. In a eulogy for S.S. Wilks, a student in the late 1920s of Henry L. Rietz and Allen T. Craig at the University of Iowa, Frederick Mostellar said that Iowa was then “the center of statistical study in the United States of America”. Rietz, Craig, and Wilks worked closely with Fisher. E.F. Lindquist, the American leader of standardized testing for educators, also of the University of Iowa, was deeply influenced by Snedecor. Lindquist invented the Iowa Test of Basic Skills for schoolchildren. He too spent time with the great man.
</p>
<p>
Some psychologists knew about the work of Neyman and Pearson and some even about that of the Bayesian Harold Jeffreys. But textbook authors, editors, and teachers–inspirited by Fisher’s promise of raising their fields to the level of hard science–helped Fisher win the day. Statistical education narrowed at the same time as it spread. Decision theory and inverse probability, and Gosset’s views on substantive significance, alternative hypotheses, and power, were pushed aside. Too introspective for the hard-boiled.”
</p>
</span></blockquote>
<p>It seems as if Fisher’s mechanization of statistical significance is what ultimately enabled <em>statistics</em> to branch out as its own field of study (and that it took place in Iowa is a fun fact). It makes you wonder how this separation contributed to the subsequent growth of scientific inquiry, results, and knowledge by disrupting the synergy between the intuition held by the practitioner and the intricacies of statistical nuance. While the popular notion of “playing in everyone’s backyard” is commonly portrayed as an advantage (which it is pretty cool), upon closer reflection, it might be a fundamental issue. <a href="https://en.wikipedia.org/wiki/William_Sealy_Gosset">William Sealy Gosset</a>, a.k.a <em>Student</em>, and the inventor of the <a href="https://en.wikipedia.org/wiki/Student%27s_t-test"><em>t-test</em></a>, was first and foremost, a brewer of Guinness beer, and clearly prioritized substantive meaning:</p>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46">“Fisher, not the great transcendent, invented the 5 percent philosophy. By contrast, Gosset’s economic approach to uncertainty prevented him from being able to stop thinking at .05 for fear he’d lose too much information, and profits.”</span><sub>1</sub></p>
</blockquote>
<blockquote class="blockquote">
<p><span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46">“World War I had been under way for more than a year when Gosset–who wanted to serve in the war but was rejected because of nearsightedness–wrote to his elderly friend, the great Karl Pearson: ‘My own war work is obviously to brew Guinness stout in such a way as to waste as little labor and material as possible, and I am hoping to help to do something fairly creditible in that way.’ It seems he did.”</span><sub>1</sub></p>
</blockquote>
<p>He had a problem to solve: <em>“to brew the best tasting stout at a satisfying price.”</em>. My takeaway: be like Gosset.</p>
</section>
<section id="sidenotes" class="level1">
<h1>Side notes</h1>
<ol type="1">
<li>I don’t think this has much to do with <em>statistical</em> advancement, but rather the experience of observing its implications over time. <br><br></li>
<li>By <em>error</em>, I’m talking about the inevitable consequences of statistical analysis in the real-world. Data is messy and inaccurate, samples contain unintended biases and nuances, and estimation methods always produce a much more simplified version of reality. It probably doesn’t need to be repeated, but as George Box <a href="https://en.wikipedia.org/wiki/All_models_are_wrong">famously said</a>, <em>“all models are wrong, some are useful”</em>. <br><br></li>
<li>In the article, they defined <em>COVID-19 misinformation</em> as <em>“assertions unsupported by or contradicting US Centers for Disease Control and Prevention (CDC) guidance on COVID-19 prevention and treatment during the period assessed or contradicting the existing state of scientific evidence for any topics not covered by the CDC”</em>. <br><br></li>
<li>To give them the benefit of the doubt, they also use a “certainty of evidence” criteria in their decision making which is meant to rate the confidence they have in the result with respect to estimation accuracy, risk of bias, etc. However, the conclusion that there is <em>“no effect”</em> seems questionable to say the least, and that suggesting otherwise is <em>misinformation</em> is asinine. <br><br></li>
<li>Search for the <em>‘Quinn is dead’</em> quote <a href="#favoritequotes">below</a> for another intuitive example of the <em>fallacy of the transposed conditional</em>. <br><br></li>
<li>A couple other points on Bayesian modeling. First, on sample size. The required number of samples needed to estimate something is <em>N=0</em>. That is, I can get parameter estimates solely based on the prior distributions that are driven by what is already known. Thinking of it this way, the data becomes secondary to the model, and is merely collected as a way to nudge parameters one way or another as more of it comes in. The <em>model</em> always exists, relaying the best available information at that point in time, and I don’t need to wait to cross arbitrary sample size thresholds in order to obtain my estimates. This seems to naturally lend itself better to the scientific process. Second, a criticism of Bayesian modeling is that it is too subjective because individual judgement is being used to inform prior distributions. However, I see this as an unequivocal strength. Frequentist methods (and noninformative priors) are not “objective”. They carry assumptions that we probably wouldn’t see as realistic, it is just convenient to use them. In that sense, they become <em>more</em> arbitrary than utilizing pre-existing knowledge. There is an excellent <a href="https://learnbayesstats.com/episode/45-biostats-clinical-trial-design-frank-harrell/">podcast episode</a> where this is discussed.</li>
</ol>
</section>
<section id="favoritequotes" class="level1">
<h1>My favorite quotes</h1>
<p>These are my favorite quotes and passages from <a href="https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2">the book</a>:</p>
<ul>
<li><em>“The sizeless scientists have adopted a method of deciding which numbers are significant that has little to do with humanly significant numbers…Imagine that you and your infant child are standing on a sidewalk near a busy street. You have just purchased a hot dog from the street vendor and have safely crossed the street. Scenario 1: You suddenly realize you have forgotten the mustard and if you scurry across the busy street, dodging vehicles, there is a 95% probability you’ll return safe with your mustard. Scenario 2: You forgot your child and you watch as she tries to cross the street herself, if you scurry across the busy street, dodging vehicles, there is a 95% probabiliity you’ll return safe with your child. The sizeless scientist in effect declares ‘they are equally important reasons for crossing the street’”</em> (chapter 0, page 10) <br><br></li>
<li><em>“…since the arrival of the desktop computer with its ability to invert big matrices at the punch of a key, ‘checking’ on sampling variability effortlessly…electronic computation of statistical significance has cheapened to near zero…‘Decision’ has become socialized and bureaucratized–heedless of the social margins.”</em> (chapter 0, page 13) <br><br></li>
<li><em>“It’s hard to do, unlike calculating t-statistics, which is a simpleton’s parlor game. But actual science at the frontier is supposed to be difficult. If it wasn’t, you wouldn’t be at the frontier.”</em> (chapter 0, page 16) <br><br></li>
<li><em>“…some cause of natural selection may have a high probability of replicability in additional samples but be trivial. Yet a cause may have a low probability of replicability but be important. This is what we mean when we say that a test of significance is neither necessary nor sufficient for a finding of importance”</em> (chapter 1, page 26) <br><br></li>
<li><em>“Unreasoning anger is a quite common reaction to challenges to the Fisherian orthodoxy.”</em> (chapter 1, page 31) <br><br></li>
<li><em>“Significance unfortunately is a useful means toward personal ends in the advance of science…Precision, knowledge, and control. In a narrow and cynical sense statistical significance is the way to achieve these. Design experiment. Then calculate statistical significance. Publish articles showing ‘significant’ results. Enjoy promotion.”</em> (chapter 1, page 32) <br><br></li>
<li><em>“An arbitrary level of statistical significance is the only standard in force–regardless of size, of loss, of cost, of ethics, of scientific persuasiveness. That is, regardless of oomph.”</em> (chapter 2, page 41) <br><br></li>
<li><em>“Gosset’s economic approach to uncertainty prevented him from be able to stop thinking at .05 for fear he’d lose too much information, and profits…[Fisher] turned away from Gosset and sought a mechanical, uniform, and bureaucratic line of demarcation–an ‘impenetrable’ end, to scientific argument. So the insecure sciences, eager to establish an ‘objective basis’ for their research ‘communicable to other rational minds’, were pleased and materially rewarded by Fisher’s 5 percent philosophy…With the low fee he set for them to rise to the rank of Sciences with a big S…”</em> (chapter 3, page 46) <br><br></li>
<li><em>“Fisher’s procedure appeals to scientists uncomfortable with any sort of argument…To avoid debate they seek certitude such as statistical significance. The unhappy result is that mere opinion and unargued crankery are <strong>more</strong> likely to rule the sizeless sciences, not less…A technique that was supposed to end arguments has in fact merely concealed the arguments behind a facade of testing that does not test.”</em> (chapter 3, page 47) <br><br></li>
<li><em>“‘The goal of an empirical economist should not be to determine the truthfulness of a model but rather the domain of its usefulness’ [Edward Leamer]”</em> (chapter 3, page 52) <br><br></li>
<li><em>“Ten million tests of significance, in economics, done annually. If the ten million tests were in fact as conclusive as their own rhetoric requires, whether accepting or rejecting, then nearly every issue in economics would long since have been settled. By now there would therefore be far fewer tests per year, not, as is the case, more and more.”</em> (chapter 3, page 53) <br><br></li>
<li><em>“Real scientists draw a line between what is large and small.”</em> (chapter 3, page 54) <br><br></li>
<li><em>“Real science, unlike significance-testing science, is difficult. If it were not, it would not be real science, but instead it would be already established routine. Real science asks you to make real scientific judgements and real scientific arguments within a community of other scientists. It asks you to be quantitatively persuasive, not to be irrelevantely mechanical. Life is hard.”</em> (chapter 3, page 55) <br><br></li>
<li><em>“…seems to be today’s prepublication attitude: merely increase the N [sample size] to get a still lower [standard error]…Notice the implication of such reasoning. It implies that something must be very wrong with the notion that statistical significance is <strong>necessary</strong> for substantive significance, a preliminary screen in which one puts one’s data.”</em> (chapter 5, page 67) <br><br></li>
<li><em>“She can test her belief in the price effect by looking at the magnitudes, using, for example, the highly advanced technique common in data-heavy articles in physics journals: ‘interocular trauma’. That is, she can look and see if the result hits her between the eyes.”</em> (chapter 5, page 72) <br><br></li>
<li><em>“‘Pushing’ an economically large <strong>though noisily estimated</strong> effect is not a misuse–or a ‘stretch’ of professional ethics. It is precisely the ethical thing to do. To argue otherwise is to fall into the mistaken belief that statistical significance <strong>can</strong> provide a screen through which the results can be put, to be examined then for <strong>substantive</strong> significance if they make it through the significance screen.”</em> (chapter 7, page 86) <br><br></li>
<li><em>“‘Young people have to have careers’ [former editor of the American Economic Review]”</em> (chapter 8, page 89) <br><br></li>
<li><em>“Any scientific hypothesis is a matter of being close enough. The decisions the scientist makes on what constitutes ‘closeness’ ‘depend entirely on the special purposes of the investigator’.”</em> (chapter 8, page 97) <br><br></li>
<li><em>“Real scientific tests are always a matter of how close to zero or how close to large or how close to some parameter value, and the standard of how close must be a substantive one, inclusive of tolerable loss.”</em> (chapter 9, page 98) <br><br></li>
<li><em>“…‘the overall benefit-cost ratio for the Employer Experiment is 4.29, but it is not statistically different from zero. The benefit-cost ratio for white women…however, is 7.07, and is statistically different from zero…The Employer Experiment affected only white women.’ The 7.07 ratio <strong>affects</strong>, they said, the 4.29 did not. This is a mistake. The best guess of the researchers was that the state got $4.29 for every dollar spent. The estimate was fuzzy, speaking of random sampling error alone. But that <strong>does not mean it is to be taken as zero</strong>.”</em> (chapter 9, page 99) <br><br></li>
<li><em>“Notice the respect for the approximate nature of social statistics in his very phrasing of ‘around 0.4’ instead of the 0.40768934 that his computer undoubtedly spewed out.”</em> (chapter 9, page 101) <br><br></li>
<li><em>“Real science changes one’s mind. That’s one way to see that the proliferation of unpersuasive significance tests is not real science.”</em> (chapter 9, page 101) <br><br></li>
<li><em>“At high sample sizes, all null hypotheses are rejected, by mathematical fact, without having to look at the data. No magic of instrumental variables is going to change that.”</em> (chapter 9, page 104) <br><br></li>
<li><em>“‘Caution, common sense, and patience…are quite likely to keep [the experimenter] more free from error…than the man of little caution and common sense who guides himself by a mechanical application of sampling rules. He will be more likely to remember that there are sources of error more important than fluctuations of sampling.’”</em> (chapter 10, page 114) <br><br></li>
<li><em>“‘It is possible for a result to be useful and possess wide standard error. A result obtained by definitions and techniques drawn up with care, and carried out by excellent interviewing and supervision may have wide standard error because the sample was small; yet such a result might be well preferable to one obtained with a bigger sample, with a smaller standard error, but whose definitions, techniques, and interviewing were out of line with best practice and knowledge of the subject matter.’ [W. Edwards Deming]”</em> (chapter 10, page 117) <br><br></li>
<li><em>“It’s embedded like a tax code in the bureaucracy of science.”</em> (chapter 11, page 124) <br><br></li>
<li><em>“…why actually replicate when the logic of Fisherian procedures gives you a virtual replication without the bother and expense? Why not go ahead and use the alloys F1 and F2 in airplanes? After all, p&lt;.05.”</em> (chapter 11, page 127) <br><br></li>
<li><em>“In denying the plurality of overlapping hypotheses, the Fisherian tester asks very little of the data. She sees the world through the lens of one hypothesis–the null.”</em> (chapter 12, page 133) <br><br></li>
<li><em>“If you are a Fisherian, the fact of a large sample becomes your problem. You’re deluded, thinking you’ve proved oomph before you’ve considered what it is.”</em> (chapter 12, page 135) <br><br></li>
<li><em>“It always depends on the loss, measured in side effects, treatment cost, death rates. The loss to a cool, scientific, impartial spectator will not be the same as the loss to the patient in question…[the balance between Type I/II errors] ‘must be left to the patient, friends, and family’.”</em> (chapter 12, page 137) <br><br></li>
<li><em>“Designing experiments to find the maximal and minimal effect size is a better way to get powerful results and to keep the focus where is should be, on the effect size itself…[William Sealy Gosset]: ‘We tend to think of effect size (when we think of it at all) as a fixed and immutable quantity that we attempt to detect. It may be more useful to think of effect size as a manipulable parameter than can, in a sense, be made larger through greater measurement accuracy.’”</em> (chapter 12, page 139) <br><br></li>
<li><em>“Some psychologists knew about the work of Neyman and Pearson and some even about that of the Bayesian Harold Jeffreys. But textbook authors, editors, and teachers–inspirited by Fisher’s promise of raising their fields to the level of hard science–helped Fisher win the day. Statistical education narrowed at the same time as it spread. Decision theory and inverse probability, and Gosset’s views on substantive significance, alternative hypotheses, and power, were pushed aside. Too introspective for the hard-boiled.”</em> (chapter 13, page 143) <br><br></li>
<li><em>“Fisher wrote in 1955, ‘In the US also the great importance of organized technology has I think made it easy to confuse the process appropriate for drawing correct conclusions, with those aimed rather at , let us say, speeding production, or saving money’. Notice the sneer by the new aristocracy of merit, as the clerisy fancied itself. Bourgeois production and money making, Fisher avers, are <strong>not</strong> the appropriate currencies of science.”</em> (chapter 13, page 145) <br><br></li>
<li><em>“Early on in an elementary statistics or psychometrics or econometrics book there might appear a loss function–‘what if it rains the day of the company picnic?’. But the loss function disappears when the book gets down to producing a formula for science.”</em> (chapter 13, page 146) <br><br></li>
<li><em>“Power, simulation, a variety of experiments, triangulation, actual replication, and exploratory data analysis leading to interocular trauma from the effect of magnitudes are different modes of affirming the consequent and are more generally a reasonable program of Gosset or Bayesian and Feynman confirmationism than is the dogma of Fisherian or Popperian falsificationism.”</em> (chapter 13, page 153) <br><br></li>
<li><em>“The Fisher test can shed light on the probability that ‘Quinn is dead’ given that ‘Quinn was hanged’. What the Fisher test wants to know and claims to measure is the opposite, the probability that Quinn was hanged, given that Quinn is dead…this probability is close to zero…In a nonhanging society people die for many reasons other than hanging…therefore being dead is very weak evidence indeed that Quinn was hanged…Being dead is ‘consistent with’ the hypothesis that Quinn was hanged as the positivist rhetoric of the Fisherian argument emphasizes. But so what? A myriad of other hypotheses…such as catching pneumonia or breaking your neck in a fall from your horse, are also consistent with it–‘it’ being the fact of being dead.”</em> (chapter 14, page 155) <br><br></li>
<li><em>“One of us has an elderly aunt who can sit in the garden of a hot, Indiana summer evening untouched by mosquitoes. She chalks up her immunity to a side effect of a ‘nuclear treatment’ received at midcentury to attack a tumor…Well, who’s to deny her? Medical science since the arrival of Fisher’s methods has had a problem with narrative…people believed that the use of p’s and t’s in the design and evaluation of clinical trials would mark an advance over old wive’s tales, crankery, anecdote, folkways, and fast-talking patent medicine salesmen. The dream of mechanization was as compelling in medicine as it was in war, social work, and philosophy of mind…‘Let the table decide’. At 5 percent the medical scientists suddenly submitted eyes locked hard in a sizeless stare. But the new method is just a mutation of old husband’s tales, statistical crankery, probabilistic anecdote, scientific folkways, and fast-talking, twenty-first-century, statistical patent medicine salesmen.”</em> (chapter 14, page 160) <br><br></li>
<li><em>“Even the rare courageous Fisherians do not deign to make a case for their procedures. They merely complain that the procedures are being criticized…being comfortably in control, appear inclined to leave things as they are…If you don’t have any arguments for an intellectual habit of a lifetime perhaps it is best to keep quiet”</em> (chapter 15, page 169) <br><br></li>
<li><em>“If one can see or hear the problem, one does not need to rely on correlations…doctors have lost many of their skills of physical assessment, even with the stethoscope (and certainly with their hands) and have come to rely on a medical literature deeply infected with Fisherianism.”</em> (chapter 15, page 175) <br><br></li>
<li><em>“The Fisherian tests of significance, the only tests employed by the original authors of the seventy-one studies, literally could not see the beneficial effects of the therapies under study, though staring at them.”</em> (chapter 16, page 179) <br><br></li>
<li><em>“The ‘sunshine herb’ [St.&nbsp;John’s wort] is frequently under attack (perhaps, one suspects, because it seems to be a cheap substitute for drugs)…the authors…concluded from the p-value that St.&nbsp;John’s-wort is not clinically effective. Doesn’t help, they said.”</em> (chapter 16, page 182) <br><br></li>
<li><em>“‘…They were made on different days at different hours. They all relate to the same nest’. Since Edgeworth had collected his own data, he knew his observations intimately; for example, he controlled exactly for nest and time-of-day heterogeneity, reducing error in observations that cannot be matched with a mere test of statistical significance on a data set downloaded from the Internet, no matter how mathematically advanced the ‘correction’.”</em> (chapter 17, page 189) <br><br></li>
<li><em>“Statistical significance can indicate the likelihood of the presence of an effect…But…so what?…Hoover an Siegler want to assign the responsibility to a man they call ‘practical’. Shades of Fisher: the scientist is replaced by a mechanical puppet who acknowledges a signal at p=.05, and the puppet–not the scientist who knows why it might matter–is called ‘practical’.”</em> (chapter 17, page 191) <br><br></li>
<li><em>“Statistics was not by any means the primary science on the Gower Street agenda. Biometry, but especially eugenics, was…Pearson’s papers and the archives of the Biometric and Galton labs survive. One finds in them the ephemera of a scientific racism common to the age, and to which Galton, Pearson, and Fisher were leading contributors…Value judgements–arguments about the arguments–and Gosset’s personal probability, were to be kept out of the neighborhood of their new sciences. Pearson would write in the 1920s against Jewish migration to Britain, and Fisher would write in the 1930s against material relief for poor people and literally in favor of relief for the rich on eugenic grounds. Such stuff was in the air…”</em> (chapter 18, page 199) <br><br></li>
<li><em>“An early case, applied to the eggs of the cuckoo bird, illustrates literally the feel of substantive as against statistical significance.”</em> (chapter 19, page 203) <br><br></li>
<li><em>“There are ways other than getting inside the mind of the victim to know what matters to her. For instance, one could measure with some difficulty and sacrifice (but good science is difficult and sacrificial)…”</em> (chapter 19, page 205) <br><br></li>
<li><em>“But Gosset in this study and others often found z or t beside the point. ‘You want to be able to say ’if farmers [or whomever] in general do this [i.e., follow a certain experimental method] they will make money by it’’. A criterion of merely statistical significance could not satisfy such taste.</em> (chapter 20, page 209) <br><br></li>
<li><em>“‘Fisher was vague. Karl Pearson was vague. Egon Pearson vague. Neyman vague. Fisher and Neyman were fiery. Silly! Egon Pearson was on the outside. They were all jealous of one another, afraid somebody would get ahead. Gosset didn’t have a jealous bone in his body. He asked the question [about power and alternative hypotheses]. Egon Pearson to a certain extent rephrased the question which Gosset had asked in statistical parlance. Neyman solved the problem mathematically.’ [Florence Nightingale David]”</em> (chapter 20, page 211) <br><br></li>
<li><em>“‘There must be essential similarity to ordinary practice…Experiments must be so arranged as to obtain the maximum possible correlation [not the maximum possible statistical significance] between figures which are to be compared [like Leamer and other oomph-ful scientists, Gosset thought in terms of upper and lower bound estimates, best and worst case scenarios]…Repetitions should be so arranged as to have the minimum possible correlation between repetitions (or the highest possible negative correlation)…There should be economy of effory [net pecuniary advantage in the 1905 sense]’ [Student (William Sealy Gosset)]. Fisher shrugged. The economic approach to the design of experiments was too difficult. He never did try Gosset’s way.”</em> (chapter 21, page 216) <br><br></li>
<li><em>“An ethical life of science seems to require an emotional life outside of it. ‘…he [Fisher] is glad to discuss…things early in the morning or late at night. But he is not glad or even willing to have others work on the purely theoretical aspects of his work. He expects others to accept his discoveries without even questioning them. He does <strong>not</strong> admit that anything he ever said or wrote was wrong. But he goes much further than that. He does not admit even that the <strong>way</strong> he said anything or the nomenclature he used could be improved in any way.’ [Raymond Birge]. Birge told Deming that Fisher was the most conceited man he ever met.”</em> (chapter 21, page 222) <br><br></li>
<li><em>“‘Though recognizable as a psychological condition of reluctance, or resistance to the acceptance of a proposition, the feeling induced by a test of significance has an objective basis in that the probability statement on which it is based is a fact communicable to and verifiable by, other rational minds. The level of significance in such cases fulfils the conditions of a measure of the rational grounds for the disbelief it engenders.’ [R.A. Fisher]”</em> (chapter 21, page 223) <br><br></li>
<li><em>“To evaluate size matters/how much would have forced Fisher to listen to and cooperate with others. Determining whether something matters to people depends on actually listening to people, as a heart surgeon listens to a radiologist, as a beer brewer listens to a customer. Admitting that size matters would have required Fisher to admit that regression coefficients ‘are capable of evaluation in any currency’. It would have put him in the unhappy position of having to communicate with others about the meaning of his findings. This, we have shown, he would not do.”</em> (chapter 21, page 224) <br><br></li>
<li><em>“Scientists, Fisher said, should ‘not assume’ their research is ‘capable of evaluation’. They must not work to ‘maximize profit’, he said in 1955, only for ‘faith’–a secular faith, he means, in the possibility that another mechanically calculated output of p-values by themselves could contribute to scientific progress. The scientist should not worry…whether their samples are random: just test, test, test, <strong>as if</strong> random. A 5 percent level of Type I error is, when ‘formally’ considered, says Fisher, the final judge of Science.”</em> (chapter 21, page 226) <br><br></li>
<li><em>“It is our experience that the more training a person has undergone in Fisherian methods the less easy it is for her to grasp our very elementary point…People who are highly trained in conventional economics have an especially difficult time. Most of them have no idea what we are talking about, though they are sure they do not approve. By contrast, undergraduates who have never had a statistics course, science and engineering professionals we work with or meet in our travels, businesspeople, musicians, activists, various colleagues in nonstatistical fields…as soon as they are able to grasp that we are <strong>not</strong> attacking statistics as such…these have no difficulty understanding our point and immediately begin wondering what the controversy is about.”</em> (chapter 23, page 239) <br><br></li>
<li><em>“One can take null-hypothesis significance testing as a sort of astrology, giving ‘decisions’ mechanically, justified within the system of astrology itself…Fisherisnism is <strong>bad</strong> input, straightforwardly misleading advice, erroneous astrology. Misleading advice is not made into good advice merely by its mechanical and pecuniary cheapness.”</em> (chapter 23, page 241/242) <br><br></li>
<li><em>“‘Adherence to the rules originally conceived as a means, becomes transformed into an end-in-itself’ [Robert Merton]. That seems about right: statistical significance, originally conceived as a means to substantive significance, became transformed by Fisher and then by bureaucracies of science into an end in itself. A t-tested certified fact will be ‘equally convincing to all rational minds, irrespective of any intentions they may have in utilizing knowledge inferred’.”</em> (chapter 23, page 243) <br><br></li>
<li><em>“If we were to assemble our socioeconomic observations into a single chain of thought its strongest link would be coupling Merton’s ‘bureaucracy’ with Hayek’s ‘scientism’. Scientism describes, ‘of course, an attitude which is decidedly unscientific in the true sense of the word, since it involves a mechanical and uncritical application of habits of thought to fields different from those in which they have been formed. The scentistic as distinguished from the scientific view is not an unprejudiced but a very prejudiced approach which, before it has considered its subject, claims to know what is the most appropriate way of investigating it’. [Hayek]. The trick is to unshackle the bureaucracy of scientism, to break its mechanical rules, change its prejudice incentives, create new rituals, train capacity. No simple trick.”</em> (chapter 23, page 244) <br><br></li>
<li><em>“They need to acquire the virtues necessary for performing repeated experiments on the same material. They need to hear that random error is one out of many dozens of errors and seldom the biggest.”</em> (chapter 24, page 246) <br><br></li>
<li><em>“In science, as against careerism or pure mathematics, it is better to be approximately correct and scientifically relevant than it is to be precisely correct but humanly irrelevant. Not even the fully specified power function, balancing the risk of errors from random sampling, provides a full solution to a scientific problem. In truth, as Kruskal never tired of remarking, statistical ‘significance’ poses no scientific problem at all. With the aid of a personal computer and a grant such significance is easy to achieve.”</em> (chapter 24, page 246) <br><br></li>
<li><em>“Statistical scientists can teach substance without sacrificing the rigor they so passionately seek. Real rigor will <strong>rise</strong> with increased attention to substance.”</em> (chapter 24, page 247) <br><br></li>
<li><em>“The textbooks are wrong. The teaching is wrong. The seminar you just attended is wrong. The most prestigious journal in your scientific field is wrong…Science is mainly a series of approximations to discovering the sources of error. Science is a systematic way of reducing wrongs or can be.”</em> (chapter 24, page 251) <br><br></li>
<li><em>“Perhaps you feel frustrated by the random epistemology of the mainstream but don’t know what to do. Perhaps you’ve been sedated by significance and lulled into silence. Perhaps you sense that the power of a Rothamsted test against a plausible Dublin alternative is statistically speaking low but are dazzled by the one-sided rhetoric of statistical significance. Perhaps you feel oppressed by the instrumental variable one should dare not to wield. Perhaps you feel frazzled by the ‘social psychological rhetoric of fear’ that keeps the abuse of significance in circulation. You want to come out of it. But perhaps you are cowed by the pretige of Fisherian dogma. Or, worse thought, perhaps you are cynically willing to be corrupted if it will keep a nice job. Repent, we say. Embrace your inner Gosset…‘Who are you going to believe–us or your own lying eyes?’”</em> (chapter 24, page 251)</li>
</ul>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ol type="1">
<li>Deirdre McCloskey, Steve Ziliak. <a href="https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2"><em>The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives</em></a>. University of Michigan Press. 2008. https://doi.org/10.3998/mpub.186351 (subtitle quote: chapter 10, page 112) <br><br></li>
<li>Sule S, DaCosta MC, DeCou E, Gilson C, Wallace K, Goff SL. <a href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808358">Communication of COVID-19 Misinformation on Social Media by Physicians in the US</a>. JAMA Netw Open. 2023;6(8):e2328928. doi:10.1001/jamanetworkopen.2023.28928 <br><br></li>
<li>Roman YM, Burela PA, Pasupuleti V, Piscoya A, Vidal JE, Hernandez AV. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8394824/">Ivermectin for the Treatment of Coronavirus Disease 2019: A Systematic Review and Meta-analysis of Randomized Controlled Trials.</a> Clin Infect Dis. 2022 Mar 23;74(6):1022-1029. doi: 10.1093/cid/ciab591. PMID: 34181716; PMCID: PMC8394824. <br><br></li>
<li>Diaz GA, Parsons GT, Gering SK, Meier AR, Hutchinson IV, Robicsek A. <a href="https://jamanetwork.com/journals/jama/fullarticle/2782900">Myocarditis and Pericarditis After Vaccination for COVID-19</a>. JAMA. 2021;326(12):1210–1212. doi:10.1001/jama.2021.13443 <br><br></li>
<li>Cohen, J. (1994). <a href="https://doi.org/10.1037/0003-066X.49.12.997">The earth is round (p &lt; .05)</a>. American Psychologist, 49(12), 997–1003. https://doi.org/10.1037/0003-066X.49.12.997</li>
</ol>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.zajichekstats\.com\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Statistical significance is...insignificant"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> '"A cheap way to get marketable results" -William Kruskal'</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Alex Zajichek"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "12/22/2023"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "feature.png"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - History</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - Philosophy</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - Research</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>The longer I've been practicing as a statistician, maybe paradoxically, the more skeptical I've become of statistical significance (<span class="co">[</span><span class="ot">#1</span><span class="co">](#sidenotes)</span>). It manifests as a feeling of dissatisfaction, as if, even though you've stated what you "found", you don't _actually_ believe it to be true. I recently finished reading [_The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives_](https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2)--it instantly became one of my favorite books (here are my <span class="co">[</span><span class="ot">favorite quotes and passages</span><span class="co">](#favoritequotes)</span>). It affirms a lot of what I've come to suspect, with deep articulation about the vastness of the issue, backed by a thorough historical foundation. I can't help but wonder about the broader scientific, political, and societal implications this has had over the years (and continues to have). It really lit a fire in me to continue learning about and unraveling statistical history to connect those dots.</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu"># My take</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>The _significance_ of a statistical result cannot be mechanically, mathematically, or systematically determined. It must be driven by a _practical_ relevance, or importance, which is inherently subjective, and a product of the values, beliefs, interests, and/or goals of the individual(s) interpreting the data. That result is always subject to dispute, critique, replication, and refinement, whether those reasons are process error (<span class="co">[</span><span class="ot">#2</span><span class="co">](#sidenotes)</span>), or the value of the information itself. The chance occurrence of a sampling probability crossing an arbitrary threshold is actually irrelevant.</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="fu"># What is statistical significance? {#whatisstatsig}</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 20px; color: #2e5c46"&gt;<span class="at">"It's embedded like a tax code in the bureaucracy of science."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>Technically speaking, it is when the likelihood of observing our data, _if_ an hypothesized state of the world were true (known as the _p-value_), is so small ([notoriously](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913), and most often, less than 5%), that the hypothetical state of the world must be false, and therefore, we have "significant" statistical evidence to say so. It positions itself as an objective tool to _decide_ (on the basis of this probability threshold) whether a statistical relationship is "real", and, often, subsequently that it "matters". </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>Take this <span class="co">[</span><span class="ot">recent study</span><span class="co">](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808358)</span>, for example, which is meant to characterize physician-propagated misinformation about the COVID-19 pandemic. The authors outline a set of <span class="co">[</span><span class="ot">basic premises</span><span class="co">](https://cdn.jamanetwork.com/ama/content_public/journal/jamanetworkopen/939195/zoi230834supp1_prod_1697557763.1365.pdf?Expires=1704642946&amp;Signature=nxggsGG3f~BQcev3DoFQmbURh3vsb1CtFrP4rSviM1XaF8Y9vtyGPmRRBTRDAXyvYzrGvW6vrJFsphYDRTI9LSJD35NYEc8RUdkZK8fJkKcSpr-AbsW1wyhe30CUf-x8GGPI2For6nZNLWoZhBn0m~GrC3JlmuTmCswv~3RH7HolcYV10ZTVgSh4ZvGaBOUKDdNhmITsHrocrTct-xvMnohwhM~6~nHZMATo6~grFfhnPrhgsDHRVkYdLr9o8yFEae4-ylEnzkulOfigZIKZJsj1s5iBbyPAB60k2KYYkXlBZnNAUpkrKSP33m9BiG8YY047fQOSRrqMrap-PaA45w__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)</span> that are used as a basis for classifying contrarian statements as _misinformation_ (<span class="co">[</span><span class="ot">#3</span><span class="co">](#sidenotes)</span>). At least some of this is built upon the attainment of statistical significance (or lack thereof).</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>As an example, in the category of **Promoting Unapproved Medications for Prevention or Treatment** (in the Results section), the authors state:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #9e3634"&gt;<span class="at">"The 2 most prominent medications promoted were ivermectin and hydroxychloroquine, which have been found to not be effective at treating COVID-19 infections in randomized clinical trials."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">2</span>&lt;/sub&gt;</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>This premise drove them to classify social media posts like this: </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #45A4CE"&gt;<span class="at">"Two of my toughest COVID patients--showed up with oxygen stats of 68% and 84% and would not go to the hospital. We treated them with IVM, steroids, and breathing treatments and here they are now."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">2</span>&lt;/sub&gt;</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>as misinformation (see all <span class="co">[</span><span class="ot">supportive quotes</span><span class="co">](https://cdn.jamanetwork.com/ama/content_public/journal/jamanetworkopen/939195/zoi230834t4_1697557763.5423.png?Expires=1704648605&amp;Signature=vIdMdVOE4XQ76IbpK3v-OY0EzN5zbPooMwAeWZTXhWDu5fJnx4hpErMTWryrzYEaJOYO6YckYQIvSqmFFHp7LJ~8NughK380U2JDc2PBtonwbYYmVcXzRXT~oLftOZEXNxq0MWHqDFs1Ov7KNUdoqd1TuhYxCgFKWUvhdb5pXzB0zliNP-28kjQwZF9KLM70oerRsri0XM-HVfdKkPKrM1idAtUBFAZzBDg05y5BBQD8cLzgW4Fa7cINV-~1Dhyg9HpncVHeRACdc-tJwqqs4Z-VQfJPXMgzxP2-eyL6nhSA3IqIW9ax8CYgcYrha6yqq80wqaR4zTTGmXzf~rTWmQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)</span>). This is an actual doctor saying the drug helped _their_ patients, but the authors have deemed it ineffective. What justifies them making such a universal claim?</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>If you look at <span class="co">[</span><span class="ot">one of their references</span><span class="co">](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8394824/)</span>, the <span class="co">[</span><span class="ot">meta-analysis</span><span class="co">](https://en.wikipedia.org/wiki/Meta-analysis)</span> shows the <span class="co">[</span><span class="ot">relative risk</span><span class="co">](https://en.wikipedia.org/wiki/Relative_risk)</span> for all-cause mortality was estimated to be </span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>37%. That is, the risk of death was 63% lower in patients who received ivermectin versus placebo or standard of care. However, because the 95% <span class="co">[</span><span class="ot">confidence interval</span><span class="co">](https://en.wikipedia.org/wiki/Confidence_interval)</span> ranged from 12% to 113% (i.e., there was _plausibility_ that ivermectin could produce up to a 13% _worse_ mortality rate, but equally plausible an 88% risk reduction), it was deemed _not_ statistically significant, and as the authors state:</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #3d2f2f"&gt;<span class="at">"IVM </span><span class="co">[</span><span class="ot">ivermectin</span><span class="co">]</span><span class="at">, compared with control treatment, did not have an effect on the all-cause mortality rate."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">3</span>&lt;/sub&gt;</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>and ultimately,</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #3d2f2f"&gt;<span class="at">"Ivermectin is not a viable treatment option for COVID-19."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">3</span>&lt;/sub&gt;</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>In other words, because the probability of observing this data, under the assumption of no difference in mortality risk (our p-value definition above), was not less than 5% (it was 31%), that gives reason to conclude _no difference at all_ (<span class="co">[</span><span class="ot">#4</span><span class="co">](#sidenotes)</span>). Furthermore, if the confidence interval crossed 100% by any amount, no matter how small, the p-value would have remained above 5% and not reached the threshold for statistical significance.</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="fu"># Why is it flawed?</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 20px; color: #2e5c46"&gt;<span class="at">"Real science changes one's mind. That's one way to see that the proliferation of unpersuasive significance tests is not real science."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="fu">## An arbitrary threshold {#arbitrarythreshold}</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>The 5% threshold is arbitrary. Despite that common acknowledgement, willful ignorance tends to prevail due to tradition and adherence to norms. The fact that the perceived significance of a result can suddenly change from minute differences speaks to the lack of robustness in the logic. In <span class="co">[</span><span class="ot">the book</span><span class="co">](https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2)</span>, the authors frequently discuss the importance of a _loss function_, which focuses on the potential consequences and implications of the result on the real-world decisions that are sought to be made from the information, rather than a predefined threshold based on sampling error probability. In this sense, the allowable risk tolerance can't be objectively or mechanically determined. It is context-dependent, and not all decisions are created equal. Yes, the p-value [above](#whatisstatsig) was 31%, but that error rate, along with the plausible range of risks (and benefits), may be sufficient to someone needing to make a treatment decision _now_.</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="fu">### Risks are subjective</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 18px; color: #2e5c46"&gt;<span class="at">"It always depends on the loss, measured in side effects, treatment cost, death rates. The loss to a cool, scientific, impartial spectator will not be the same as the loss to the patient in question...</span><span class="co">[</span><span class="ot">the balance between Type I/II errors</span><span class="co">]</span><span class="at"> 'must be left to the patient, friends, and family'."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>Beyond the statistical significance of a result is the question of what to do about it. In the <span class="co">[</span><span class="ot">same article</span><span class="co">](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808358)</span>, the authors state the following, still in the context of misinformation:</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #9e3634"&gt;<span class="at">"Claims that myocarditis was common in children who received the vaccine and that the risks of myocarditis outweighed the risk of vaccination were also unfounded."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">2</span>&lt;/sub&gt;</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>Nevermind the fact that the <span class="co">[</span><span class="ot">study they reference</span><span class="co">](https://jamanetwork.com/journals/jama/fullarticle/2782900)</span> _does_ show an increase in monthly case volume of myocarditis and pericarditis between pre/post-vaccine periods and the authors state:</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #9e8a39"&gt;<span class="at">"Myocarditis developed rapidly in younger patients, mostly after the second vaccination. Pericarditis affected older patients later, after either the first or second dose."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">4</span>&lt;/sub&gt;</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>The more important point is that the weight individuals place on statistical results to inform their decision making is subjective. The risk may be low, maybe even lower than the alternative, but that doesn't inform _how_ someone should weigh it.</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46"&gt;<span class="at">"Imagine that you and your infant child are standing on a sidewalk near a busy street. You have just purchased a hot dog from the street vendor and have safely crossed the street. Scenario 1: You suddenly realize you have forgotten the mustard and if you scurry across the busy street, dodging vehicles, there is a 95% probability you'll return safe with your mustard. Scenario 2: You forgot your child and you watch as she tries to cross the street herself, if you scurry across the busy street, dodging vehicles, there is a 95% probabiliity you'll return safe with your child. The sizeless scientist in effect declares 'they are equally important reasons for crossing the street'"</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="fu">## It can't depend on sample size {#samplesize}</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 18px; color: #2e5c46"&gt;<span class="at">"At high sample sizes, all null hypotheses are rejected, by mathematical fact, without having to look at the data."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>One pretty simple argument is that of <span class="co">[</span><span class="ot">sample size</span><span class="co">](https://www.omniconvert.com/what-is/sample-size/)</span>. In most contexts, a statistical test is, by definition, more likely to be declared _significant_ by simply [amassing more data](https://en.wikipedia.org/wiki/Standard_error), regardless of what the actual effect size is. This, on the other hand, _is_ completely mechanical and dissociated from the real-world context in which the test is being run. Thus, it prioritizes quantity over substance, and when blindly used, potentially promotes results that may lack practical meaning. </span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46"&gt;<span class="at">"...some cause of natural selection may have a high probability of replicability in additional samples but be trivial. Yet a cause may have a low probability of replicability but be important. This is what we mean when we say that a test of significance is neither necessary nor sufficient for a finding of importance"</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>It also tends to shift focus to attaining statistical significance and using it as a filter, causing the potential to miss meaningful insights that didn't reach this level.</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="fu">## We don't believe in "zero-sized" effects</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 18px; color: #2e5c46"&gt;<span class="at">"Real scientists draw a line between what is large and small."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>There is a major contradiction that arises. </span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>The typical hypothesis test is conducted under the assumption of a <span class="co">[</span><span class="ot">null hypothesis</span><span class="co">](https://en.wikipedia.org/wiki/Null_hypothesis)</span> positing _no effect_. For example, in calculating the p-value [above](#whatisstatsig), it is assumed that there is _no_ difference in all-cause mortality rates between the treatment groups. However, I would argue that in any practical context, it's rare that someone would genuinely believe in the existence of precisely zero effect. Rather, it would stand to reason that what they really mean is "effectively zero" effect, something so small that it is considered inconsequential. </span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>Herein lies the contradiction: they have now acknowledged some level of substantive significance, albeit undefined. If the true effect happens to be smaller than this threshold, as we <span class="co">[</span><span class="ot">just explained</span><span class="co">](#samplesize)</span>, the estimate will still eventually be declared statistically significant with mathematical certainty no matter how minuscule, thus inevitably crossing the unspoken threshold of substantive meaning. Therefore, this begs into question the value of attaining statistical significance at all in favor of the need for explicit consideration of the real-world implications (i.e., the <span class="co">[</span><span class="ot">loss function</span><span class="co">](#arbitrarythreshold)</span>). At the _very least_, the substantive threshold should be identified and reflected in the null hypothesis so that the p-value is calibrated for substance.</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="fu">## The fallacy of the transposed conditional {#fallacy}</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>This is where it gets especially interesting. There are logical errors with the conclusions drawn from hypothesis testing. I think the best way to describe it is jumping into the classic example that arises in Jacob Cohen's <span class="co">[</span><span class="ot">_The Earth Is Round (p &lt; .05)_</span><span class="co">](https://doi.org/10.1037/0003-066X.49.12.997)</span> from 1994:</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Verdana, sans-serif; font-style: italic; font-size: 12px; color: #45A4CE"&gt;<span class="at">"The incidence of schizophrenia in adults is about 2%. A proposed screening test is estimated to have at least 95% accuracy in making the positive diagnosis (sensitivity) and about 97% accuracy in declaring normality (specificity)...With a positive test for schizophrenia at hand, given the more than .95 assumed accuracy of the test, the probability of a positive test given that the case is normal is less than .05, that is, significant at p &lt; .05. One would reject the hypothesis that the case is normal and conclude that the case has schizophrenia, as it happens mistakenly, but within the .05 alpha error. But that's not the point. The probability of the case being normal, given a positive test, is not what has just been discovered however much it sounds like it and however much it is wished to be. It is not true that the probability that the case is normal is less than .05, nor is it even unlikely that it is a normal case. By a Bayesian maneuver, this inverse probability, the probability that the case is normal, given a positive test for schizophrenia, is about .60!"</span>&lt;/span&gt;&lt;sub&gt;<span class="at">5</span>&lt;/sub&gt;</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>The desired interpretation of a statistically significant result induces a technical problem. The p-value provides the likelihood of observing the data under the assumption that the null hypothesis is true (a single state of the world), yet we _want_ to interpret it as evidence about the parameter of interest given the data. After all, we did collect it, and want that to be the basis of our conclusions. But that is not the probability we have concerned ourselves with. Using the p-value as a singular basis to determine significance disregards all other possibilities that the true parameter could be. When those possibilities are imbalanced (as they were here, since only 2% of the population had schizophrenia), it confuses which state of the world is most likely given the data with how likely the data is given a state of the world (<span class="co">[</span><span class="ot">#5</span><span class="co">](#sidenotes)</span>).</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a><span class="fu"># What to do instead?</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 20px; color: #2e5c46"&gt;<span class="at">"Real science, unlike significance-testing science, is difficult. If it were not, it would not be real science, but instead it would be already established routine. Real science asks you to make real scientific judgements and real scientific arguments within a community of other scientists. It asks you to be quantitatively persuasive, not to be irrelevantely mechanical. Life is hard."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>It's a scary thing to think about. Suppose statistical significance isn't there to bail you out. What are you supposed to do? How do you know if your results matter or not? I think this passage gives a pretty clear answer:</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 16px; color: #2e5c46; font-weight: bold"&gt;<span class="at">"She can test her belief in the price effect by looking at the magnitudes, using, for example, the highly advanced technique common in data-heavy articles in physics journals: 'interocular trauma'. That is, she can look and see if the result hits her between the eyes."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>The premise of this article has been that the implications of statistical results are context-dependent, so there isn't a one-size-fits-all alternative to replace statistical significance. Rather than seeking a systematic approach, the emphasis should be placed on cultivating understanding of the subject matter. It's akin to relying on intuition, like a feeling of "knowing" that you've gotten what you needed. Take this simple analogy: a tape measure is a tool that quantifies information needed to inform subsequent action, and the precision of the measurement is tailored to the specific needs of the task at hand. Sometimes a rough estimate is sufficient, while other times meticulous precision is necessary. The goal is to reach the point where, intuitively, you "know" that you've obtained the necessary information to move forward confidently. I see statistics as the same thing. Merely a _tool_ to be used to quantify the desired information needed to _inform_ (i.e., augment, not determine) a decision. </span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>Now I'm not going to claim that I haven't repeatedly violated the practices I'm arguing against, it's hard not to, but these are things that I'm going to focus more on moving forward instead of p-values and statistical significance:</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Estimation &amp; magnitude</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>This is probably the easiest change to start making because it doesn't require an overhaul of statistical methods, but rather just a shift in focus to the magnitude of the estimates. By deliberately avoiding p-value calculations (and, when reading and consuming research, simply ignoring the concept of statistical significance altogether), the interpretation is governed by (a plausible range of) effect sizes, untainted by arbitrary, context-agnostic significance thresholds, and thus forces a scientific argument to be made on that basis. With a little extra brain power (and humility), this creates a much more contextually-rich, informative interpretation.</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Bayesian thinking &amp; causal modeling</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>Richard McElreath's <span class="co">[</span><span class="ot">_Statistical Rethinking_</span><span class="co">](https://github.com/rmcelreath/stat_rethinking_2023)</span> really convinced me that causal inference powered by Bayesian estimation is probably the best framework out there for scientific modeling (and I've only made it through the <span class="co">[</span><span class="ot">first couple of chapters</span><span class="co">](https://www.zajichekstats.com/post/statistical-rethinking-2023-class-notes/)</span> so far). It completely shifts the focus from the data itself to the data-generating _process_, putting the bulk of the hard work upfront, before data is collected, with a focus on mechanism and structure. It also addresses the <span class="co">[</span><span class="ot">fallacy problem</span><span class="co">](#fallacy)</span>. However, it's definitely harder to start doing on a whim.</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>First of all, the <span class="co">[</span><span class="ot">math itself</span><span class="co">](https://en.wikipedia.org/wiki/Bayesian_statistics)</span> is different from typical <span class="co">[</span><span class="ot">frequentist</span><span class="co">](https://en.wikipedia.org/wiki/Frequentist_inference)</span> methods, so there is a learning curve there. More difficult though is navigating the _practical_ complexities, such as properly eliciting the necessary subject matter expertise and piecing that together into coherent <span class="co">[</span><span class="ot">prior distributions</span><span class="co">](https://en.wikipedia.org/wiki/Prior_probability)</span> and <span class="co">[</span><span class="ot">causal models</span><span class="co">](https://en.wikipedia.org/wiki/Causal_model)</span>. Nevermind the technical reasons why that is hard, it is simply more demanding from a time, brainpower, and collaboration perspective--and everyone is busy. Nevertheless I think it is a worthy pursuit (<span class="co">[</span><span class="ot">#6</span><span class="co">](#sidenotes)</span>).</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. Decision-making &amp; course of action</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>This is where the <span class="co">[</span><span class="ot">loss function</span><span class="co">](#arbitrarythreshold)</span> is most relevant. </span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>Instead of contorting a generic statistical result to tenuously align with real-world implications, I want to be more deliberate. The first step is to target and understand the tangible decision-making processes that the estimates seek to inform, with an identification of the current standards including practical constraints and nuances. Then, rather than passively using standard techniques, deriving tailored statistical methods to facilitate that usage, which may prompt more rigor, customization, or reframing of the statistical problem entirely to suit the specific context at hand. Estimation uncertainty can be fed as input into hypothetical scenarios to gain insight into where/what actions will be triggered and their subsequent downstream effects on the hard outcomes intended to be impacted. At that point, the _significance_ will be clear.</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Focus on the end-product</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>I think a critical piece to this endeavor is to not only focus on the statistics, but also _how_ they will be disseminated. This means specifying the vehicle that will deliver the information to the right person at the right time. The emphasis on something tangible elicits certain practical and technological constraints that may be otherwise unbounding when focusing solely on the math. Further, this perspective acknowledges that the statistical methods are only a fragment of the overall data product, and may be direct cause for further refinement of the statistical approach itself. That is, even with robust statistical methods or results, the information may lose its utility if poorly conveyed or implemented. This could be due to anything from data pipelines and visualization to deployment and computing resources. This also enables the ability to be more forward-thinking about success measures and accountability/validation schemes like continuous monitoring to ensure sustained yet impactful presence in the intended decision-making context. </span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a><span class="fu"># Some historical gold</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>To conclude this, I wanted to highlight an excerpt from the chapter _The Psychology of Psychological Significance Testing_ in <span class="co">[</span><span class="ot">the book</span><span class="co">](https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2)</span> that I found especially fascinating about the propagation of statistical significance across university education in the United States (pages 142-143):</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46"&gt;</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>&lt;p&gt;<span class="at">"In this context the 5 percent science was promoted by the new leaders of quantitative psychology and education. European humanists can score themselves by how many generations they are removed from Hegel--that is, in being taught by a teacher who was taught by a teacher who was taught by a teacher who was taught by Hegel at the University of Berlin. Likewise, statisticians can score themselves by how many generations they are from Fisher. Quinn McNemar, for example, of Stanford University, was an important teacher of psychologists who had himself studied statistical methods at Stanford with Harold Hotelling, the chief American disciple of Fisher. Hotelling had worked directly with Fisher. McNemar then taught L.G. Humphreys, Allen Edwards, David Grant, and scores of others. As early as 1935 all graduate students in psychology at Stanford, following the model of Iowa State, were required to master Fisher's crowning achievement, analysis of variance. Already by 1950, Gigerenzer et al. reckon, about half of the leading departments of psychology required training in Fisherian methods.</span>&lt;/p&gt;</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>&lt;p&gt;<span class="at">Even rebels against Fisher were close to him, starting with </span><span class="co">[</span><span class="ot">William Sealy</span><span class="co">]</span><span class="at"> Gosset himself. Palmer Johnson of the University of Minnesota studied with Fisher in England, though he later had the bad taste to write articles with Fisher's erstwhile colleague and eternal enemy Jerzy Neyman, whom Fisher had cast into outer darkness. George Snedecor, an agricultural scientist at Iowa State University at Ames, was a cofounder of the first department of statistics in the United States. His important book </span>&lt;em&gt;<span class="at">Statistical Methods</span>&lt;/em&gt;<span class="at"> was influenced directly by Fisher himself, who somewhat surprisingly was in the 1930s a visiting professor of statistics at Iowa State. One can think of the Iowa schools then </span><span class="co">[</span><span class="ot">1940s and 1950s</span><span class="co">]</span><span class="at"> as one thinks of London's Gower Street in the 1920s and 1930s--a crucial crossroads of statistical methods and training. In a eulogy for S.S. Wilks, a student in the late 1920s of Henry L. Rietz and Allen T. Craig at the University of Iowa, Frederick Mostellar said that Iowa was then "the center of statistical study in the United States of America". Rietz, Craig, and Wilks worked closely with Fisher. E.F. Lindquist, the American leader of standardized testing for educators, also of the University of Iowa, was deeply influenced by Snedecor. Lindquist invented the Iowa Test of Basic Skills for schoolchildren. He too spent time with the great man.</span>&lt;/p&gt;</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>&lt;p&gt;<span class="at">Some psychologists knew about the work of Neyman and Pearson and some even about that of the Bayesian Harold Jeffreys. But textbook authors, editors, and teachers--inspirited by Fisher's promise of raising their fields to the level of hard science--helped Fisher win the day. Statistical education narrowed at the same time as it spread. Decision theory and inverse probability, and Gosset's views on substantive significance, alternative hypotheses, and power, were pushed aside. Too introspective for the hard-boiled."</span>&lt;/p&gt;</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>It seems as if Fisher's mechanization of statistical significance is what ultimately enabled _statistics_ to branch out as its own field of study (and that it took place in Iowa is a fun fact). It makes you wonder how this separation contributed to the subsequent growth of scientific inquiry, results, and knowledge by disrupting the synergy between the intuition held by the practitioner and the intricacies of statistical nuance. While the popular notion of "playing in everyone's backyard" is commonly portrayed as an advantage (which it is pretty cool), upon closer reflection, it might be a fundamental issue. [William Sealy Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset), a.k.a _Student_, and the inventor of the [_t-test_](https://en.wikipedia.org/wiki/Student%27s_t-test), was first and foremost, a brewer of Guinness beer, and clearly prioritized substantive meaning:</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46"&gt;<span class="at">"Fisher, not the great transcendent, invented the 5 percent philosophy. By contrast, Gosset's economic approach to uncertainty prevented him from being able to stop thinking at .05 for fear he'd lose too much information, and profits."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span>&lt;span style="font-family: Garamond, serif; font-size: 14px; color: #2e5c46"&gt;<span class="at">"World War I had been under way for more than a year when Gosset--who wanted to serve in the war but was rejected because of nearsightedness--wrote to his elderly friend, the great Karl Pearson: 'My own war work is obviously to brew Guinness stout in such a way as to waste as little labor and material as possible, and I am hoping to help to do something fairly creditible in that way.' It seems he did."</span>&lt;/span&gt;&lt;sub&gt;<span class="at">1</span>&lt;/sub&gt;</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>He had a problem to solve: _"to brew the best tasting stout at a satisfying price."_. My takeaway: be like Gosset.</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a><span class="fu"># Side notes {#sidenotes}</span></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>I don't think this has much to do with _statistical_ advancement, but rather the experience of observing its implications over time.</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>By _error_, I'm talking about the inevitable consequences of statistical analysis in the real-world. Data is messy and inaccurate, samples contain unintended biases and nuances, and estimation methods always produce a much more simplified version of reality. It probably doesn't need to be repeated, but as George Box [famously said](https://en.wikipedia.org/wiki/All_models_are_wrong), _"all models are wrong, some are useful"_.</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>In the article, they defined _COVID-19 misinformation_ as _"assertions unsupported by or contradicting US Centers for Disease Control and Prevention (CDC) guidance on COVID-19 prevention and treatment during the period assessed or contradicting the existing state of scientific evidence for any topics not covered by the CDC"_.</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>To give them the benefit of the doubt, they also use a "certainty of evidence" criteria in their decision making which is meant to rate the confidence they have in the result with respect to estimation accuracy, risk of bias, etc. However, the conclusion that there is _"no effect"_ seems questionable to say the least, and that suggesting otherwise is _misinformation_ is asinine.</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Search for the _'Quinn is dead'_ quote [below](#favoritequotes) for another intuitive example of the _fallacy of the transposed conditional_.</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>A couple other points on Bayesian modeling. First, on sample size. The required number of samples needed to estimate something is _N=0_. That is, I can get parameter estimates solely based on the prior distributions that are driven by what is already known. Thinking of it this way, the data becomes secondary to the model, and is merely collected as a way to nudge parameters one way or another as more of it comes in. The _model_ always exists, relaying the best available information at that point in time, and I don't need to wait to cross arbitrary sample size thresholds in order to obtain my estimates. This seems to naturally lend itself better to the scientific process. Second, a criticism of Bayesian modeling is that it is too subjective because individual judgement is being used to inform prior distributions. However, I see this as an unequivocal strength. Frequentist methods (and noninformative priors) are not "objective". They carry assumptions that we probably wouldn't see as realistic, it is just convenient to use them. In that sense, they become _more_ arbitrary than utilizing pre-existing knowledge. There is an excellent <span class="co">[</span><span class="ot">podcast episode</span><span class="co">](https://learnbayesstats.com/episode/45-biostats-clinical-trial-design-frank-harrell/)</span> where this is discussed.</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a><span class="fu"># My favorite quotes {#favoritequotes}</span></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>These are my favorite quotes and passages from <span class="co">[</span><span class="ot">the book</span><span class="co">](https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2)</span>:</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"The sizeless scientists have adopted a method of deciding which numbers are significant that has little to do with humanly significant numbers...Imagine that you and your infant child are standing on a sidewalk near a busy street. You have just purchased a hot dog from the street vendor and have safely crossed the street. Scenario 1: You suddenly realize you have forgotten the mustard and if you scurry across the busy street, dodging vehicles, there is a 95% probability you'll return safe with your mustard. Scenario 2: You forgot your child and you watch as she tries to cross the street herself, if you scurry across the busy street, dodging vehicles, there is a 95% probabiliity you'll return safe with your child. The sizeless scientist in effect declares 'they are equally important reasons for crossing the street'"_ (chapter 0, page 10)</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"...since the arrival of the desktop computer with its ability to invert big matrices at the punch of a key, 'checking' on sampling variability effortlessly...electronic computation of statistical significance has cheapened to near zero...'Decision' has become socialized and bureaucratized--heedless of the social margins."_ (chapter 0, page 13)</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"It's hard to do, unlike calculating t-statistics, which is a simpleton's parlor game. But actual science at the frontier is supposed to be difficult. If it wasn't, you wouldn't be at the frontier."_ (chapter 0, page 16)</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"...some cause of natural selection may have a high probability of replicability in additional samples but be trivial. Yet a cause may have a low probability of replicability but be important. This is what we mean when we say that a test of significance is neither necessary nor sufficient for a finding of importance"_ (chapter 1, page 26)</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Unreasoning anger is a quite common reaction to challenges to the Fisherian orthodoxy."_ (chapter 1, page 31)</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Significance unfortunately is a useful means toward personal ends in the advance of science...Precision, knowledge, and control. In a narrow and cynical sense statistical significance is the way to achieve these. Design experiment. Then calculate statistical significance. Publish articles showing 'significant' results. Enjoy promotion."_ (chapter 1, page 32)</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"An arbitrary level of statistical significance is the only standard in force--regardless of size, of loss, of cost, of ethics, of scientific persuasiveness. That is, regardless of oomph."_ (chapter 2, page 41)</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Gosset's economic approach to uncertainty prevented him from be able to stop thinking at .05 for fear he'd lose too much information, and profits...[Fisher] turned away from Gosset and sought a mechanical, uniform, and bureaucratic line of demarcation--an 'impenetrable' end, to scientific argument. So the insecure sciences, eager to establish an 'objective basis' for their research 'communicable to other rational minds', were pleased and materially rewarded by Fisher's 5 percent philosophy...With the low fee he set for them to rise to the rank of Sciences with a big S..."_ (chapter 3, page 46)</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Fisher's procedure appeals to scientists uncomfortable with any sort of argument...To avoid debate they seek certitude such as statistical significance. The unhappy result is that mere opinion and unargued crankery are **more** likely to rule the sizeless sciences, not less...A technique that was supposed to end arguments has in fact merely concealed the arguments behind a facade of testing that does not test."_ (chapter 3, page 47)</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'The goal of an empirical economist should not be to determine the truthfulness of a model but rather the domain of its usefulness' [Edward Leamer]"_ (chapter 3, page 52)</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Ten million tests of significance, in economics, done annually. If the ten million tests were in fact as conclusive as their own rhetoric requires, whether accepting or rejecting, then nearly every issue in economics would long since have been settled. By now there would therefore be far fewer tests per year, not, as is the case, more and more."_ (chapter 3, page 53)</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Real scientists draw a line between what is large and small."_ (chapter 3, page 54)</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Real science, unlike significance-testing science, is difficult. If it were not, it would not be real science, but instead it would be already established routine. Real science asks you to make real scientific judgements and real scientific arguments within a community of other scientists. It asks you to be quantitatively persuasive, not to be irrelevantely mechanical. Life is hard."_ (chapter 3, page 55)</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"...seems to be today's prepublication attitude: merely increase the N [sample size] to get a still lower [standard error]...Notice the implication of such reasoning. It implies that something must be very wrong with the notion that statistical significance is **necessary** for substantive significance, a preliminary screen in which one puts one's data."_ (chapter 5, page 67)</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"She can test her belief in the price effect by looking at the magnitudes, using, for example, the highly advanced technique common in data-heavy articles in physics journals: 'interocular trauma'. That is, she can look and see if the result hits her between the eyes."_ (chapter 5, page 72)</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'Pushing' an economically large **though noisily estimated** effect is not a misuse--or a 'stretch' of professional ethics. It is precisely the ethical thing to do. To argue otherwise is to fall into the mistaken belief that statistical significance **can** provide a screen through which the results can be put, to be examined then for **substantive** significance if they make it through the significance screen."_ (chapter 7, page 86)</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'Young people have to have careers' [former editor of the American Economic Review]"_ (chapter 8, page 89)</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Any scientific hypothesis is a matter of being close enough. The decisions the scientist makes on what constitutes 'closeness' 'depend entirely on the special purposes of the investigator'."_ (chapter 8, page 97)</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Real scientific tests are always a matter of how close to zero or how close to large or how close to some parameter value, and the standard of how close must be a substantive one, inclusive of tolerable loss."_ (chapter 9, page 98)</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"...'the overall benefit-cost ratio for the Employer Experiment is 4.29, but it is not statistically different from zero. The benefit-cost ratio for white women...however, is 7.07, and is statistically different from zero...The Employer Experiment affected only white women.' The 7.07 ratio **affects**, they said, the 4.29 did not. This is a mistake. The best guess of the researchers was that the state got $4.29 for every dollar spent. The estimate was fuzzy, speaking of random sampling error alone. But that **does not mean it is to be taken as zero**."_ (chapter 9, page 99)</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Notice the respect for the approximate nature of social statistics in his very phrasing of 'around 0.4' instead of the 0.40768934 that his computer undoubtedly spewed out."_ (chapter 9, page 101)</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Real science changes one's mind. That's one way to see that the proliferation of unpersuasive significance tests is not real science."_ (chapter 9, page 101)</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"At high sample sizes, all null hypotheses are rejected, by mathematical fact, without having to look at the data. No magic of instrumental variables is going to change that."_ (chapter 9, page 104)</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'Caution, common sense, and patience...are quite likely to keep [the experimenter] more free from error...than the man of little caution and common sense who guides himself by a mechanical application of sampling rules. He will be more likely to remember that there are sources of error more important than fluctuations of sampling.'"_ (chapter 10, page 114)</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'It is possible for a result to be useful and possess wide standard error. A result obtained by definitions and techniques drawn up with care, and carried out by excellent interviewing and supervision may have wide standard error because the sample was small; yet such a result might be well preferable to one obtained with a bigger sample, with a smaller standard error, but whose definitions, techniques, and interviewing were out of line with best practice and knowledge of the subject matter.' [W. Edwards Deming]"_ (chapter 10, page 117)</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"It's embedded like a tax code in the bureaucracy of science."_ (chapter 11, page 124)</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"...why actually replicate when the logic of Fisherian procedures gives you a virtual replication without the bother and expense? Why not go ahead and use the alloys F1 and F2 in airplanes? After all, p&lt;.05."_ (chapter 11, page 127)</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"In denying the plurality of overlapping hypotheses, the Fisherian tester asks very little of the data. She sees the world through the lens of one hypothesis--the null."_ (chapter 12, page 133)</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"If you are a Fisherian, the fact of a large sample becomes your problem. You're deluded, thinking you've proved oomph before you've considered what it is."_ (chapter 12, page 135)</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"It always depends on the loss, measured in side effects, treatment cost, death rates. The loss to a cool, scientific, impartial spectator will not be the same as the loss to the patient in question...[the balance between Type I/II errors] 'must be left to the patient, friends, and family'."_ (chapter 12, page 137)</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Designing experiments to find the maximal and minimal effect size is a better way to get powerful results and to keep the focus where is should be, on the effect size itself...[William Sealy Gosset]: 'We tend to think of effect size (when we think of it at all) as a fixed and immutable quantity that we attempt to detect. It may be more useful to think of effect size as a manipulable parameter than can, in a sense, be made larger through greater measurement accuracy.'"_ (chapter 12, page 139)</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Some psychologists knew about the work of Neyman and Pearson and some even about that of the Bayesian Harold Jeffreys. But textbook authors, editors, and teachers--inspirited by Fisher's promise of raising their fields to the level of hard science--helped Fisher win the day. Statistical education narrowed at the same time as it spread. Decision theory and inverse probability, and Gosset's views on substantive significance, alternative hypotheses, and power, were pushed aside. Too introspective for the hard-boiled."_ (chapter 13, page 143)</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Fisher wrote in 1955, 'In the US also the great importance of organized technology has I think made it easy to confuse the process appropriate for drawing correct conclusions, with those aimed rather at , let us say, speeding production, or saving money'. Notice the sneer by the new aristocracy of merit, as the clerisy fancied itself. Bourgeois production and money making, Fisher avers, are **not** the appropriate currencies of science."_ (chapter 13, page 145)</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Early on in an elementary statistics or psychometrics or econometrics book there might appear a loss function--'what if it rains the day of the company picnic?'. But the loss function disappears when the book gets down to producing a formula for science."_ (chapter 13, page 146)</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Power, simulation, a variety of experiments, triangulation, actual replication, and exploratory data analysis leading to interocular trauma from the effect of magnitudes are different modes of affirming the consequent and are more generally a reasonable program of Gosset or Bayesian and Feynman confirmationism than is the dogma of Fisherian or Popperian falsificationism."_ (chapter 13, page 153)</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"The Fisher test can shed light on the probability that 'Quinn is dead' given that 'Quinn was hanged'. What the Fisher test wants to know and claims to measure is the opposite, the probability that Quinn was hanged, given that Quinn is dead...this probability is close to zero...In a nonhanging society people die for many reasons other than hanging...therefore being dead is very weak evidence indeed that Quinn was hanged...Being dead is 'consistent with' the hypothesis that Quinn was hanged as the positivist rhetoric of the Fisherian argument emphasizes. But so what? A myriad of other hypotheses...such as catching pneumonia or breaking your neck in a fall from your horse, are also consistent with it--'it' being the fact of being dead."_ (chapter 14, page 155)</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"One of us has an elderly aunt who can sit in the garden of a hot, Indiana summer evening untouched by mosquitoes. She chalks up her immunity to a side effect of a 'nuclear treatment' received at midcentury to attack a tumor...Well, who's to deny her? Medical science since the arrival of Fisher's methods has had a problem with narrative...people believed that the use of p's and t's in the design and evaluation of clinical trials would mark an advance over old wive's tales, crankery, anecdote, folkways, and fast-talking patent medicine salesmen. The dream of mechanization was as compelling in medicine as it was in war, social work, and philosophy of mind...'Let the table decide'. At 5 percent the medical scientists suddenly submitted eyes locked hard in a sizeless stare. But the new method is just a mutation of old husband's tales, statistical crankery, probabilistic anecdote, scientific folkways, and fast-talking, twenty-first-century, statistical patent medicine salesmen."_ (chapter 14, page 160)</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Even the rare courageous Fisherians do not deign to make a case for their procedures. They merely complain that the procedures are being criticized...being comfortably in control, appear inclined to leave things as they are...If you don't have any arguments for an intellectual habit of a lifetime perhaps it is best to keep quiet"_ (chapter 15, page 169)</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"If one can see or hear the problem, one does not need to rely on correlations...doctors have lost many of their skills of physical assessment, even with the stethoscope (and certainly with their hands) and have come to rely on a medical literature deeply infected with Fisherianism."_ (chapter 15, page 175)</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"The Fisherian tests of significance, the only tests employed by the original authors of the seventy-one studies, literally could not see the beneficial effects of the therapies under study, though staring at them."_ (chapter 16, page 179)</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"The 'sunshine herb' [St. John's wort] is frequently under attack (perhaps, one suspects, because it seems to be a cheap substitute for drugs)...the authors...concluded from the p-value that St. John's-wort is not clinically effective. Doesn't help, they said."_ (chapter 16, page 182)</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'...They were made on different days at different hours. They all relate to the same nest'. Since Edgeworth had collected his own data, he knew his observations intimately; for example, he controlled exactly for nest and time-of-day heterogeneity, reducing error in observations that cannot be matched with a mere test of statistical significance on a data set downloaded from the Internet, no matter how mathematically advanced the 'correction'."_ (chapter 17, page 189)</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Statistical significance can indicate the likelihood of the presence of an effect...But...so what?...Hoover an Siegler want to assign the responsibility to a man they call 'practical'. Shades of Fisher: the scientist is replaced by a mechanical puppet who acknowledges a signal at p=.05, and the puppet--not the scientist who knows why it might matter--is called 'practical'."_ (chapter 17, page 191)</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Statistics was not by any means the primary science on the Gower Street agenda. Biometry, but especially eugenics, was...Pearson's papers and the archives of the Biometric and Galton labs survive. One finds in them the ephemera of a scientific racism common to the age, and to which Galton, Pearson, and Fisher were leading contributors...Value judgements--arguments about the arguments--and Gosset's personal probability, were to be kept out of the neighborhood of their new sciences. Pearson would write in the 1920s against Jewish migration to Britain, and Fisher would write in the 1930s against material relief for poor people and literally in favor of relief for the rich on eugenic grounds. Such stuff was in the air..."_ (chapter 18, page 199)</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"An early case, applied to the eggs of the cuckoo bird, illustrates literally the feel of substantive as against statistical significance."_ (chapter 19, page 203)</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"There are ways other than getting inside the mind of the victim to know what matters to her. For instance, one could measure with some difficulty and sacrifice (but good science is difficult and sacrificial)..."_ (chapter 19, page 205)</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"But Gosset in this study and others often found z or t beside the point. 'You want to be able to say 'if farmers [or whomever] in general do this [i.e., follow a certain experimental method] they will make money by it''. A criterion of merely statistical significance could not satisfy such taste._ (chapter 20, page 209)</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'Fisher was vague. Karl Pearson was vague. Egon Pearson vague. Neyman vague. Fisher and Neyman were fiery. Silly! Egon Pearson was on the outside. They were all jealous of one another, afraid somebody would get ahead. Gosset didn't have a jealous bone in his body. He asked the question [about power and alternative hypotheses]. Egon Pearson to a certain extent rephrased the question which Gosset had asked in statistical parlance. Neyman solved the problem mathematically.' [Florence Nightingale David]"_ (chapter 20, page 211)</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'There must be essential similarity to ordinary practice...Experiments must be so arranged as to obtain the maximum possible correlation [not the maximum possible statistical significance] between figures which are to be compared [like Leamer and other oomph-ful scientists, Gosset thought in terms of upper and lower bound estimates, best and worst case scenarios]...Repetitions should be so arranged as to have the minimum possible correlation between repetitions (or the highest possible negative correlation)...There should be economy of effory [net pecuniary advantage in the 1905 sense]' [Student (William Sealy Gosset)]. Fisher shrugged. The economic approach to the design of experiments was too difficult. He never did try Gosset's way."_ (chapter 21, page 216)</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"An ethical life of science seems to require an emotional life outside of it. '...he [Fisher] is glad to discuss...things early in the morning or late at night. But he is not glad or even willing to have others work on the purely theoretical aspects of his work. He expects others to accept his discoveries without even questioning them. He does **not** admit that anything he ever said or wrote was wrong. But he goes much further than that. He does not admit even that the **way** he said anything or the nomenclature he used could be improved in any way.' [Raymond Birge]. Birge told Deming that Fisher was the most conceited man he ever met."_ (chapter 21, page 222)</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'Though recognizable as a psychological condition of reluctance, or resistance to the acceptance of a proposition, the feeling induced by a test of significance has an objective basis in that the probability statement on which it is based is a fact communicable to and verifiable by, other rational minds. The level of significance in such cases fulfils the conditions of a measure of the rational grounds for the disbelief it engenders.' [R.A. Fisher]"_ (chapter 21, page 223)</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"To evaluate size matters/how much would have forced Fisher to listen to and cooperate with others. Determining whether something matters to people depends on actually listening to people, as a heart surgeon listens to a radiologist, as a beer brewer listens to a customer. Admitting that size matters would have required Fisher to admit that regression coefficients 'are capable of evaluation in any currency'. It would have put him in the unhappy position of having to communicate with others about the meaning of his findings. This, we have shown, he would not do."_ (chapter 21, page 224)</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Scientists, Fisher said, should 'not assume' their research is 'capable of evaluation'. They must not work to 'maximize profit', he said in 1955, only for 'faith'--a secular faith, he means, in the possibility that another mechanically calculated output of p-values by themselves could contribute to scientific progress. The scientist should not worry...whether their samples are random: just test, test, test, **as if** random. A 5 percent level of Type I error is, when 'formally' considered, says Fisher, the final judge of Science."_ (chapter 21, page 226)</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"It is our experience that the more training a person has undergone in Fisherian methods the less easy it is for her to grasp our very elementary point...People who are highly trained in conventional economics have an especially difficult time. Most of them have no idea what we are talking about, though they are sure they do not approve. By contrast, undergraduates who have never had a statistics course, science and engineering professionals we work with or meet in our travels, businesspeople, musicians, activists, various colleagues in nonstatistical fields...as soon as they are able to grasp that we are **not** attacking statistics as such...these have no difficulty understanding our point and immediately begin wondering what the controversy is about."_ (chapter 23, page 239)</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"One can take null-hypothesis significance testing as a sort of astrology, giving 'decisions' mechanically, justified within the system of astrology itself...Fisherisnism is **bad** input, straightforwardly misleading advice, erroneous astrology. Misleading advice is not made into good advice merely by its mechanical and pecuniary cheapness."_ (chapter 23, page 241/242)</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"'Adherence to the rules originally conceived as a means, becomes transformed into an end-in-itself' [Robert Merton]. That seems about right: statistical significance, originally conceived as a means to substantive significance, became transformed by Fisher and then by bureaucracies of science into an end in itself. A t-tested certified fact will be 'equally convincing to all rational minds, irrespective of any intentions they may have in utilizing knowledge inferred'."_ (chapter 23, page 243)</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"If we were to assemble our socioeconomic observations into a single chain of thought its strongest link would be coupling Merton's 'bureaucracy' with Hayek's 'scientism'. Scientism describes, 'of course, an attitude which is decidedly unscientific in the true sense of the word, since it involves a mechanical and uncritical application of habits of thought to fields different from those in which they have been formed. The scentistic as distinguished from the scientific view is not an unprejudiced but a very prejudiced approach which, before it has considered its subject, claims to know what is the most appropriate way of investigating it'. [Hayek]. The trick is to unshackle the bureaucracy of scientism, to break its mechanical rules, change its prejudice incentives, create new rituals, train capacity. No simple trick."_ (chapter 23, page 244)</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"They need to acquire the virtues necessary for performing repeated experiments on the same material. They need to hear that random error is one out of many dozens of errors and seldom the biggest."_ (chapter 24, page 246)</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"In science, as against careerism or pure mathematics, it is better to be approximately correct and scientifically relevant than it is to be precisely correct but humanly irrelevant. Not even the fully specified power function, balancing the risk of errors from random sampling, provides a full solution to a scientific problem. In truth, as Kruskal never tired of remarking, statistical 'significance' poses no scientific problem at all. With the aid of a personal computer and a grant such significance is easy to achieve."_ (chapter 24, page 246)</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Statistical scientists can teach substance without sacrificing the rigor they so passionately seek. Real rigor will **rise** with increased attention to substance."_ (chapter 24, page 247)</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"The textbooks are wrong. The teaching is wrong. The seminar you just attended is wrong. The most prestigious journal in your scientific field is wrong...Science is mainly a series of approximations to discovering the sources of error. Science is a systematic way of reducing wrongs or can be."_ (chapter 24, page 251)</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>_"Perhaps you feel frustrated by the random epistemology of the mainstream but don't know what to do. Perhaps you've been sedated by significance and lulled into silence. Perhaps you sense that the power of a Rothamsted test against a plausible Dublin alternative is statistically speaking low but are dazzled by the one-sided rhetoric of statistical significance. Perhaps you feel oppressed by the instrumental variable one should dare not to wield. Perhaps you feel frazzled by the 'social psychological rhetoric of fear' that keeps the abuse of significance in circulation. You want to come out of it. But perhaps you are cowed by the pretige of Fisherian dogma. Or, worse thought, perhaps you are cynically willing to be corrupted if it will keep a nice job. Repent, we say. Embrace your inner Gosset...'Who are you going to believe--us or your own lying eyes?'"_ (chapter 24, page 251)</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a><span class="fu"># References</span></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Deirdre McCloskey, Steve Ziliak. <span class="co">[</span><span class="ot">_The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives_</span><span class="co">](https://press.umich.edu/Books/T/The-Cult-of-Statistical-Significance2)</span>. University of Michigan Press. 2008. https://doi.org/10.3998/mpub.186351 (subtitle quote: chapter 10, page 112)</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sule S, DaCosta MC, DeCou E, Gilson C, Wallace K, Goff SL. <span class="co">[</span><span class="ot">Communication of COVID-19 Misinformation on Social Media by Physicians in the US</span><span class="co">](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2808358)</span>. JAMA Netw Open. 2023;6(8):e2328928. doi:10.1001/jamanetworkopen.2023.28928</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Roman YM, Burela PA, Pasupuleti V, Piscoya A, Vidal JE, Hernandez AV. <span class="co">[</span><span class="ot">Ivermectin for the Treatment of Coronavirus Disease 2019: A Systematic Review and Meta-analysis of Randomized Controlled Trials.</span><span class="co">](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8394824/)</span> Clin Infect Dis. 2022 Mar 23;74(6):1022-1029. doi: 10.1093/cid/ciab591. PMID: 34181716; PMCID: PMC8394824.</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Diaz GA, Parsons GT, Gering SK, Meier AR, Hutchinson IV, Robicsek A. <span class="co">[</span><span class="ot">Myocarditis and Pericarditis After Vaccination for COVID-19</span><span class="co">](https://jamanetwork.com/journals/jama/fullarticle/2782900)</span>. JAMA. 2021;326(12):1210–1212. doi:10.1001/jama.2021.13443</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Cohen, J. (1994). <span class="co">[</span><span class="ot">The earth is round (p &lt; .05)</span><span class="co">](https://doi.org/10.1037/0003-066X.49.12.997)</span>. American Psychologist, 49(12), 997–1003. https://doi.org/10.1037/0003-066X.49.12.997</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2024 Zajichek Stats</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>