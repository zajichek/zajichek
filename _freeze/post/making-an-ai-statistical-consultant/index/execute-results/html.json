{
  "hash": "4fbc5a5d71027e2b3968f90fe0277cef",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Making an AI statistical consultant\"\ndescription: \"Exploring some new R packages for AI.\"\nauthor: \"Alex Zajichek\"\ndate: \"5/13/2025\"\nimage: \"feature.png\"\ncategories:\n  - AI\n  - Shiny\n  - Web Applications\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n---\n\n{{< video https://www.youtube.com/embed/WKB548RjE9o >}}\n\n\n\n\nThe stuff [Posit](https://posit.co/) has been [doing with AI](https://posit.co/use-cases/ai/) has been really refreshing for data scientists, especially R and Python developers. With [ChatGPT](https://chatgpt.com/) and the other popular chat [GUIs](https://en.wikipedia.org/wiki/Graphical_user_interface) out there, using AI tools was sort of fun, but now with the ability to programmatically interact with LLM's as part of my data science tech stack, I'm actually excited about using them. \n\nI attended [ShinyConf 2025](https://www.shinyconf.com/) and there were a lot of great talks that made these tools very appealing to work with. They introduced the [ellmer](https://ellmer.tidyverse.org/) package in R, and other ones such as [shinychat](https://posit-dev.github.io/shinychat/) and [querychat](https://github.com/posit-dev/querychat). Having no experience using these yet, I wanted to see how simple it would be to build and deploy a chat application from scratch that acts as a statistical consultant, which is what is in the video above ðŸ‘† ðŸ‘† ðŸ‘†\n\nHere are the steps taken, in text form:\n\n# Live app + source code\n\nFirst, if you wanted to see how the app works and the full code behind it, you can find these here:\n\n* [Live app](https://01962be4-3359-d652-ac24-9641c956445a.share.connect.posit.cloud/)\n* [Source code](https://github.com/centralstatz/statistical_consultant_chat/)\n\nNow on to the tutorial ðŸ‘‡\n\n# 1. Install the libraries\n\nOf course, we need to make sure [`shiny`](https://shiny.posit.co/r/getstarted/shiny-basics/lesson1/) is installed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"shiny\")\n```\n:::\n\n\n\nThen the brunt of the work here is going to be done by:\n\n* [`ellmer`](https://ellmer.tidyverse.org/): What we use to send prompts and receive responses from an LLM\n* [`shinychat`](https://posit-dev.github.io/shinychat/): How we create the chat user interface for our application\n\nBoth of these are on [CRAN](https://cran.r-project.org/), so we can also install them easily.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"ellmer\")\ninstall.packages(\"shinychat\")\n```\n:::\n\n\n\n# 2. Choose an LLM provider\n\nYou need to choose which LLM you want to facilitate the chat function in your application. [`ellmer`](https://ellmer.tidyverse.org/) supports all the major ones, and you can scan through the list [here](https://ellmer.tidyverse.org/reference/index.html). \n\nMany of these require some sort of payment for API usage, but [Google Gemini](https://gemini.google.com/app) offers free-tier usage, so that's what I went with. So from that list above, I chose the [`chat_gemini`](https://ellmer.tidyverse.org/reference/chat_gemini.html) option.\n\n## Setting up the API\n\nIn order to get this working, you need to configure an API key to send prompts to the model programmatically.\n\n### a. Retrieve the API key\n\nYou can go to the [Google AI Studio](https://aistudio.google.com/apikey) (and login with your Google account and agree to terms), click _Create API Key_ in the top-right corner, and copy the key to your clipboard (see [this part of the video above](https://youtu.be/WKB548RjE9o?t=199) for a visual).\n\n### b. Set your local environment variable {#apikey}\n\nAs stated in the [function documentation](https://ellmer.tidyverse.org/reference/chat_gemini.html), rather than hard-coding your API key into your app code, you just put in your local R environment file (`.Renviron`) as `GOOGLE_API_KEY`. As always, this is made simple with the [`usethis`](https://usethis.r-lib.org/) package.\n\nIn [RStudio](https://posit.co/download/rstudio-desktop/), just type `usethis::edit_r_environ()` and your `.Renviron` file will open. Then on a new line, enter your API key:\n\n```\nGOOGLE_API_KEY=XXXXXXXXXXXXXXXXXXXXXXXX <-- FILL IN WITH YOUR KEY\n```\n\nSave it, and that's it.\n\nYou should immediately be able to send/receive a response from Gemini:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package\nlibrary(ellmer)\n\n# Set up chat\nchat <- chat_gemini()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing model = \"gemini-2.0-flash\".\n```\n\n\n:::\n\n```{.r .cell-code}\n# Send a prompt\nchat$chat(\"What's a word that rhymes with orange?\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis is a classic riddle! The truth is, there aren't any perfect rhymes for \n\"orange\" in the English language. \n\nHowever, people sometimes use near rhymes or words that rhyme in specific \ndialects. For example:\n\n*   **\"Door hinge\"** - If you combine two words, you can get a near rhyme.\n*   **\"Lozenge\"** - This is the closest near rhyme you'll find as it sounds \nalmost like \"lore-inj\"\n\n**So, the best answer is: There isn't a perfect rhyme for \"orange.\"**\n```\n\n\n:::\n:::\n\n\n\n# 3. Initialize a new R project {#createproject}\n\nI store my code on [GitHub](https://github.com/), so I first went there to create a new _public_ repository (you can see mine [here](https://github.com/centralstatz/statistical_consultant_chat/)).\n\nThen in [RStudio](https://posit.co/download/rstudio-desktop/), you can create a new project (_File -> New Project -> Version Control -> Git_). Clone the repository you just made so that your local project is pre-configured to push changes to the remote repo (see [here](https://youtu.be/WKB548RjE9o?t=34)).\n\n# 4. Develop the system prompt {#systemprompt}\n\nOne of the most important concepts during [ShinyConf 2025](https://www.shinyconf.com/) was the [_system prompt_](https://ellmer.tidyverse.org/articles/ellmer.html#what-is-a-prompt). It is how you shape the broader LLM that you'll be using (in my case, [Gemini](https://gemini.google.com/app)) to take on a given behavior or persona that you'd like for your application. You set this *once* upon application start up.\n\nFor my purposes, I wanted the chatbot to act like a statistical consultant. Not just any statistical consultant, but one that was very focused on _practical_ application (not statistical significance), as if a business owner was interacting with it trying to use data to make a decision that they needed to take action on soon. This was my system prompt:\n\n> \"You are a statistical consultant. You are interacting subject matter experts and/or business leaders who are trying to use statistics to take action and make decisions. The person you are talking to is interested in tangible, real-world results and outcomes, so you should problem solve with insofar as to help them achieve those goals. We are not necessarily concerned with things like statistical signifcance or just running tests, unless it is actually relevant to the action or decision point. Instead, focus on the practical steps this person may take, in the context of their problem, to move closer to their desired goal. To do this, you will ask questions about what they are trying to solve, and how we know that what we've done provides real value. Then once you've gained enough information, you can develop a pragmatic strategy for how they should get started, and what the roadmap may look like. You're suggestions may include technical things where it makes sense (like tools, methodology, etc.), but should very much be focused on what a non-statistical person might be able to do to further this effort (e.g., better data collection, personnell, etc.)\"\n\nYou can see the raw file [here](https://github.com/centralstatz/statistical_consultant_chat/blob/main/system_prompt.txt).\n\n## Placing it into the app {#systempromptinapp}\n\nSince this was long text, I just put it in [its own text file](https://github.com/centralstatz/statistical_consultant_chat/blob/main/system_prompt.txt) in my [app's directory](https://github.com/centralstatz/statistical_consultant_chat/tree/main). Then when app starts, load it into memory (see [this line](https://github.com/centralstatz/statistical_consultant_chat/blob/03151165178f75d70520e952ab0efed9a89ed9c6/app.R#L8C1-L8C49)):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem_prompt <- readLines(\"system_prompt.txt\") \n```\n:::\n\n\n\nThen when the `chat_gemini` object is created, the imported prompt is entered there in the `system_prompt` argument (see [this line](https://github.com/centralstatz/statistical_consultant_chat/blob/03151165178f75d70520e952ab0efed9a89ed9c6/app.R#L22C3-L22C75):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat <- chat_gemini(system_prompt = paste(system_prompt, collapse = \"\"))\n```\n:::\n\n\n\n# 5. Put the app together\n\nWe haven't talked about the [`shinychat`](https://posit-dev.github.io/shinychat/) package yet, so we'll do that here as we build the main application file, which you can find [here](https://github.com/centralstatz/statistical_consultant_chat/blob/main/app.R).\n\n## a. Load the required packages\n\nThe first part of the app just loads the packages we need (and imports the system prompt that we [discussed above](#systemprompt)).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ellmer)\nlibrary(shinychat)\n\n# Import system prompt\nsystem_prompt <- readLines(\"system_prompt.txt\") \n```\n:::\n\n\n\n## b. Make the user interface {#userinterface}\n\nAll we want here is the chat interface, nothing else, so it's quite simple. First, we use the [`bslib`](https://rstudio.github.io/bslib/index.html) package to [create a page](https://rstudio.github.io/bslib/reference/page.html). \n\nThen within there is where [`shinychat`](https://posit-dev.github.io/shinychat/) comes in, which very easily makes a chat user interface with the [`chat_ui`](https://posit-dev.github.io/shinychat/reference/chat_ui.html) function. \n\nWe place that in the page, and specify the `id` and `placeholder` for chat initialization, and the entire user interface just looks like this (see [these lines](https://github.com/centralstatz/statistical_consultant_chat/blob/03151165178f75d70520e952ab0efed9a89ed9c6/app.R#L10)):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nui <- page_fluid(\n  chat_ui(\n    id = \"chat\",\n    placeholder = \"Hi, I'm a statistical consultant, how can I help you?\"\n  )\n)\n```\n:::\n\n\n\n## c. Define the server\n\nFirst we create the chat object like [we did above](#systempromptinapp). Then we have to figure out how to continuously send/receive messages while the app is running. That's where [`ellmer`](https://ellmer.tidyverse.org/) comes back into play.\n\nWhen we create the `chat_gemini` object, it is an [R6](https://r6.r-lib.org/articles/Introduction.html) object of type [_Chat_](https://ellmer.tidyverse.org/reference/Chat.html), and we use the [`stream_async`](https://ellmer.tidyverse.org/reference/Chat.html#method-stream-async-) method to feed responses back from the LLM, but we allow it to provide response as it goes so we're not just waiting for all of it to complete. In that method, we provide the `input$chat_user_input` which is just the message we type into the chat interface we [created above](#userinterface) (and is automatically accessible via the creation of the [`chat_ui`](https://posit-dev.github.io/shinychat/reference/chat_ui.html)).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstream <- chat$stream_async(input$chat_user_input)\n```\n:::\n\n\n\nOnce we create that stream, then we go back to [`shinychat`](https://posit-dev.github.io/shinychat/) and use the [`chat_append`](https://posit-dev.github.io/shinychat/reference/chat_append.html) function to feed our [`chat_ui`](https://posit-dev.github.io/shinychat/reference/chat_ui.html) the results. And that's it. \n\nThe full server function looks like this (see it [here](https://github.com/centralstatz/statistical_consultant_chat/blob/03151165178f75d70520e952ab0efed9a89ed9c6/app.R#L19)):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nserver <- function(input, output, session) {\n  \n  # Setup the chat\n  chat <- chat_gemini(system_prompt = paste(system_prompt, collapse = \"\"))\n  \n  # Stream responses as they come\n  observeEvent(input$chat_user_input, {\n    stream <- chat$stream_async(input$chat_user_input)\n    chat_append(\"chat\", stream)\n  })\n  \n}\n```\n:::\n\n\n\nAnd our app is complete. We should now be able to run it locally.\n\n# 6. Deploy the app\n\nThe live app is [here](https://01962be4-3359-d652-ac24-9641c956445a.share.connect.posit.cloud/).\n\nI used [Posit Connect Cloud](https://connect.posit.cloud/), which is an awesome platform for deploying data science apps to the web. Basically all complex configuration is done there, we just need to supply the code. And you can do it for [free](https://connect.posit.cloud/plans).\n\n## a. Capturing your environment\n\nThe last thing you'll want to do before deploying to [Connect Cloud](https://connect.posit.cloud/) is to create a file called `manifest.json` and store it at the root of your app directory. You do this after you've made all changes to your app code, and it's meant to capture your R environment and packages/dependencies so that it can be recreated upon deployment. Yet again, this is done with a simple line of code (assuming your working directory is your app's location):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsconnect::writeManifest()\n```\n:::\n\n\n\nYou should now see `manifest.json`.\n\n## b. Push to GitHub\n\nWe can then commit changes and push the code to GitHub in the repository that was [created earlier](#createproject). [This](https://github.com/centralstatz/statistical_consultant_chat/commit/d22d2bf925040525624113e762cba373da131a99) was my commit for the app. \n\n## c. Setup on Connect Cloud\n\nI created my account using GitHub credentials (see [my profile](https://connect.posit.cloud/zajichek)), so when I want to deploy something, it already has access to relevant metadata. \n\nFirst, hit the _Publish_ button from your homepage:\n\n![](publish.png)\n\nSelect _Shiny_:\n\n![](shiny.png)\n\nThen configure the location of your code:\n\n![](configure.png)\n\nThe repository name should auto-populate in the list if you signed up with GitHub. Then select the file that contains your app code (in my case, `app.R`). I chose to _Automatically publish on push_ so that if I change my code and push to GitHub, the live app will automatically reflect updates.\n\n### (Re) configure your API key\n\nSince we're no longer local, we need to re-establish the connection to the LLM provider by setting the `GOOGLE_API_KEY` environment variable on Connect Cloud. Click _Advanced settings_:\n\n![](advancedsettings1.png)\n\nThen _Add variable_, and enter your API key [from above](#apikey):\n\n![](advancedsettings2.png)\n\nThe hit _Publish_. Your app is now live. You can copy the public link to your app in the top-right of the page, and share it for the world to use!\n\n<br>\n\nMy live app is [here](https://01962be4-3359-d652-ac24-9641c956445a.share.connect.posit.cloud/). This is a snippet of how it works:\n\n![](livechatapp.gif)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}