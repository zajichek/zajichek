---
title: "Investigating a Hospital-Specific Report (HSR)"
description: "Analysis strategies to gain insight into HRRP program results"
author: "Alex Zajichek"
date: "12/26/2025"
image: "feature.png"
categories:
  - Software Development
  - Healthcare
format:
  html:
    code-fold: show
    code-tools: true
    toc: true
---

_This article is a copy of the [`readmit`](https://centralstatz.github.io/readmit/index.html) package tutorial. See it on the package website [here](https://centralstatz.github.io/readmit/articles/investigating-an-hsr.html)._

_**Note**: CMS changed the format of Hospital-Specific Reports (HSRs) for FY2026 (see [here](https://qualitynet.cms.gov/inpatient/hrrp/reports#tab2)). The current HSR functions support Excel-based formats through FY2025. However, analysis strategies are still relevant._

As part of the [Hospital Readmissions Reduction Program (HRRP)](https://www.cms.gov/medicare/payment/prospective-payment-systems/acute-inpatient-pps/hospital-readmissions-reduction-program-hrrp), the [Centers for Medicare & Medicaid Services (CMS)](https://www.cms.gov/) provides a detailed, annual program summary report (called the [Hospital-Specific Report (HSR)](https://qualitynet.cms.gov/inpatient/hrrp/reports)) to hospitals that includes details on the penalty calculation for the upcoming fiscal year, such as discharge-level data, dually-eligible discharge lists, cohort-level rollup, and the penalty amount. There is a defined [review and correction period](https://qualitynet.cms.gov/inpatient/hrrp/resources#tab1:~:text=FY%202026%20Hospital%20Readmissions%20Reduction%20Program%20Key%20Dates%20(08/11/25)) in which hospitals can use these reports to ensure the penalty being enforced by CMS is accurate. It occurs approximately 1 month before the new fiscal year, thus it is a time-critical event (see historical date ranges below for reference with the built-in package datasets):

```{r}
# Extract date ranges
readmit::hrrp_keydates |>
  dplyr::select(
    ProgramYear,
    dplyr::matches("^(Payment|Review)")
  ) |>
  dplyr::distinct()
```

The report file itself (through FY2025) is a large, multi-tab Microsoft Excel document where the structured part of the data is ambiguously placed throughout, thus we need tools to parse it out into a usable format. That is what some functions in the `readmit` package are for. In this article, we go through the tools that are available, what they do, and then provide some strategies/approaches for how hospitals can use these tools to analyze their own HSR's to gain deeper insight into HRRP results and readmissions more broadly.

# The Toolbox

First, we'll start by taking a look at what relevant functions are available to us, what they do, and how to use them. For our purposes, these are all of the functions prefixed like `hsr_*`. We'll do this roughly in order of how the report is laid out, and how the HRRP results roll up.

```{r}
library(readmit)
```

## 0. Mock Reports

As the developer of this package, I don't have access to hospitals' actual HSR's, as they contain senstivie patient information (i.e., [PHI](https://cphs.berkeley.edu/hipaa/hipaa18.html)) and thus are not publicly available. So what we have to work with are _mock_ reports that [CMS provides](https://qualitynet.cms.gov/inpatient/hrrp/reports#tab3) to the public that are meant to mimick the format a hospital can expect their report to be in. It just includes fake data.

_**Note**: CMS changed the format of Hospital-Specific Reports (HSRs) for FY2026 (see [here](https://qualitynet.cms.gov/inpatient/hrrp/reports#tab2)). The current HSR functions support Excel-based formats through FY2025._

Nevertheless, these provide a useful playground to analyze the mechanics of the program. We'll start by finding a report with the `hsr_mock_reports()` function:

* Using no arguments lists the various mock files included in the package

```{r}
hsr_mock_reports()
```

* Entering one of the listed file names will return the complete path to the file in the packages location on your computer

```{r}
my_report <- hsr_mock_reports("FY2025_HRRP_MockHSR.xlsx")
my_report
```

Now we can use that HSR path with other package functions. Of course, you would just point to your own HSR when analyzing your hospital's reports.

## 1. Program Summary {#programsummary}

We'll start with high level program results. Ultimately, all of the moving parts in the HRRP roll up into a single number: the penalty amount applied to your hospital. This is typically the first table in your report. We can parse it out of the report with the `hsr_payment_summary()` function:

```{r}
my_payment_summary <- hsr_payment_summary(my_report)
my_payment_summary
```

We now have this information in a data frame that we can manipulate as needed:

```{r}
my_payment_summary |>
  tidyr::pivot_longer(dplyr::everything())
```

There are also helper functions to extract specific components from this table.

```{r}
hsr_payment_penalty(my_report)
hsr_dual_proportion(my_report)
hsr_peer_group(my_report)
```

See `?hsr_payment_summary` for all of them.

## 2. Cohort Summary {#cohortsummary}

The overall payment penalty a hospital receives is a weighted average of penalities applied to the individual cohorts. These details are typically in the second table (tab) of the HSR, which we can import with `hsr_cohort_summary()`:

```{r}
cohort_summary <- hsr_cohort_summary(my_report)
cohort_summary
```

We can then, for example, reconcile the overall penalty amount based on what is in this table:

```{r}
cohort_summary |>

  # Filter to cohorts with a penalty
  dplyr::filter(`Penalty Indicator (Yes/No) [h]` == "Yes") |>
  
  # Compute the contribution of each penalized cohort
  dplyr::mutate(
    Contribution = 
      `Ratio of DRG Payments Per Measure to Total Payments [i]` *
      (
        `Excess Readmission Ratio (ERR) [f]` - 
          `Peer Group Median ERR [g]`
      )
  ) |>
  
  # Roll up into final calculation
  dplyr::summarize(
    Penalty = sum(Contribution) * hsr_neutrality_modifier(my_report)
  )
```

What we did here was:

1. Find cohorts who received a penalty _who were eligible_
    + A cohort must have at least 25 discharges to be eligible
    + Then, the excess readmission ratio (ERR) must be greater than the assigned peer group's median ERR
2. Find the difference between the hospital's ERR compared to the peer group median ERR
3. Multiply that by the ratio of DRG payments for that cohort
    + This is a measure of the volume of patients with this condition are treated at the hospital
4. Sum those contributions over each cohort
5. Multiply that by the neutrality modifier in the `hsr_payment_summary()` table

That's how the penalty is computed, from cohort summary level.

## 3. Discharges

_Note: Because we are using mock reports, the dates in these files are erroneous and thus R doesn't interpret them as dates. However, your hospital report has real dates and thus R should automatically parse them as such._

The HSR also contains discharge-level data on the individual patients that actually contributed to the program. There is a separate table/tab for each of the cohorts. We can use the `hsr_discharges()` function to import them for a specified cohort:

```{r}
hsr_discharges(my_report, "COPD")
```

We get some patient identifying information, including the specific dates associated with the index and readmission hospitalizations, whether or not the readmission occurred at the same hospital, diagnosis codes, etc., which is all very valuable information that we can explore to gain insights from (and what we'll do [later](#analysisstrategies)).

There are also options available in the function to refine the result:

### Eligible Cases

The `eligible_only` argument can be used to only included discharges that were actually included in HRRP evaluation:

```{r}
hsr_discharges(
  file = my_report, 
  cohort = "COPD",
  eligible_only = TRUE
)
```

Notice that this row count matches what was reported as the COPD denominator in the `cohort_summary`:

```{r}
cohort_summary |>
  dplyr::filter(`Measure [a]` == "COPD") |>
  dplyr::pull(`Number of Eligible Discharges [b]`)
```

### Risk Factors

Also included in these tables are the indicators of risk factors that are used in the statistical models to estimate individual adjusted readmission risks. We can use the `risk_factors` argument to extract those for each patient:

```{r}
hsr_discharges(
  file = my_report, 
  cohort = "COPD",
  eligible_only = TRUE,
  risk_factors = TRUE,
  discharge_phi = FALSE
)
```

This data can then be explored further to understand risk factor prevalence and how that relates to model weights, etc. (again, covered [later](#analysisstrategies)). Notice the `discharge_phi` argument was used to prevent the date information from being returned.

## 4. Model Coefficients {#modelcoefficients}

The ERR is calculated based on an aggregated roll up of individual adjusted readmission risks derived from a random-intercept logistic regression model. The first row in the discharge table contains the coefficients for this model. We can use the `hsr_coefficients()` function to extract them:

```{r}
copd_model <- hsr_coefficients(my_report, "COPD")
copd_model
```

This allows us to do things like assess the relative contribution of risk factors to the estimated readmission rates or use the risk factor dataset above to compute individual level readmission risks.

### Intercept Terms

The _predicted_ and _expected_ readmission rates only differ in the intercept terms applied to the prediction (thus it is a constant shift for all patients). We can see these at the end of this data frame:

```{r}
copd_model |> tail()
```

## 5. Readmission Risks

We could take the risk factor output from `hsr_discharges()` combined with the coefficients from `hsr_coefficients()` and reconcile each patient's _predicted_ and _expected_ readmission risk. But the `hsr_readmission_risks()` function can do all of that for us:

```{r}
risks <- hsr_readmission_risks(my_report, "COPD")
risks
```

This just takes a weighted-sum of the risk factors and coefficients, adds the corresponding intercept, and then maps it to a probability through the logistic function.

The cohort-level _predicted_ and _expected_ readmission rates are just the averages of these columns across all eligible patients:

```{r}
risks |>
  dplyr::summarize(
    Discharges = dplyr::n(),
    Predicted = mean(Predicted),
    Expected = mean(Expected),
    ERR = Predicted / Expected
  )
```

Again, looking at our cohort summary, we can see these match:

```{r}
cohort_summary |> 
  dplyr::filter(
    `Measure [a]` == "COPD"
  ) |>
  dplyr::select(
    Discharges = `Number of Eligible Discharges [b]`,
    Predicted = `Predicted Readmission Rate [d]`,
    Expected = `Expected Readmission Rate [e]`,
    ERR = `Excess Readmission Ratio (ERR) [f]`
  )
```

## 6. Dual Stays

CMS puts hospitals into peer groups based on the relative proportion of Medicare patients who are also eligible for Medicaid. This is a measure of socioeconomic status for the hospital population so hospitals are being compared only against other hospitals that are similar (in this regard). These aggregated quantities were found in `hsr_payment_summary()` result:

```{r}
my_payment_summary
```

The `hsr_dual_stays()` function extracts the discharge-level data corresponding to the numerator of the ratio:

```{r}
hsr_dual_stays(my_report)
```

We can see that the row count of this discharge level data matches the first number in the preceding table.

# Analysis Strategies {#analysisstrategies}

In this section we'll go through a collection data analyses that can be conducted, using functions in `readmit` as support, to validate HSR calculations and/or to gain deeper insights into HRRP results.

## 1. Validating the Penalty Calculation

We [previously calculated](#cohortsummary) the payment penalty starting from the cohort-level results. However, as an initial validation step, it is important to go through the mechanics of reconciling the penalty calculation from the discharge-level data to ensure comprehension of how it works. 

_Note: We'll go the slower, more tedious way to do this in the steps below in order to capture all intermediate details for understanding, but will callout where certain steps can be more efficient._

Let's extract the `cohort` strings we need to plug into various function arguments:

```{r}
cohorts <- setdiff(names(hrrp_cohort_inclusion), "ProgramYear")
cohorts
```

### i. Extract Discharges

The first thing we need to is extract the set of discharges (i.e., the _denominator_) that contribute to program for each cohort. To do this, we'll iterate through the different cohorts and sequentially use the `hsr_discharges()` function to get the row identifiers that should be included:

```{r}
eligible_discharges <-
  setdiff(cohorts, "CABG") |>

    # Iterate each cohort
    purrr::map_df(

      # Import eligible discharges
      ~hsr_discharges(
        file = my_report,
        cohort = .x,
        eligible_only = TRUE
      ) |>

      # Keep the row identifiers
      dplyr::select(`ID Number`) |>

      # Add cohort identifier
      tibble::add_column(Cohort = .x)
    )
eligible_discharges
```

We can check the counts of these:

```{r}
table(eligible_discharges$Cohort)
```

You can validate that we obtained the correct counts by looking at the [cohort summary](#cohortsummary) we previously created.

```{r}
cohort_summary
```

One caveat was that we already knew there weren't any `CABG` discharges (via the `NA` in the `cohort_summary` table), so we pre-excluded this from our cohort list we iterated through (as it would have caused an error otherwise).

### ii. Extract Risk Factors {#riskfactors}

Next we need to extract the sets of risk factors for each cohort that go into the readmission risk model. We can again do this by iterating through the `cohort` list with `hsr_discharges`, but extracting the risk factors as well with `risk_factors=TRUE`:

```{r}
setdiff(cohorts, "CABG") |>

    # Iterate each cohort
    purrr::map(

      # Import eligible discharges
      ~hsr_discharges(
        file = my_report,
        cohort = .x,
        risk_factors = TRUE,
        discharge_phi = FALSE
      )
    )
```

Notice that we get the risk factors for each cohort, but all of the columns are different since each cohort has a different model. So one thing we can do is pivot the data with `tidyr::pivot_longer()` to make a long and narrow data frame so that it can all be binded together.

```{r}
risk_factors <-
  setdiff(cohorts, "CABG") |>

      # Iterate each cohort
      purrr::map_df(

        # Import eligible discharges
        ~hsr_discharges(
          file = my_report,
          cohort = .x,
          risk_factors = TRUE,
          discharge_phi = FALSE
        ) |>
        
        # Send risk factors down the rows
        tidyr::pivot_longer(
          cols = -`ID Number`,
          names_to = "Factor",
          values_to = "Value"
        ) |>
        
        # Indicate cohort
        tibble::add_column(Cohort = .x)
      )
risk_factors
```

We can then merge these (using `dplyr::inner_join()`) with the eligilble discharges we previously identified to get the set of risk factors for each eligible discharge:

```{r}
risk_factors <-
  risk_factors |>

    # Join to get eligible only
    dplyr::inner_join(
      y = eligible_discharges,
      by = 
        c(
          "ID Number",
          "Cohort"
        )
    )
risk_factors
```

Of course, we didn't _have_ to do this step separately from step (i), as we could have just used the `eligible_only` argument simultaneously, and ended up in the same place.

```{r}
risk_factors <-
  setdiff(cohorts, "CABG") |>

      # Iterate each cohort
      purrr::map_df(

        # Import eligible discharges
        ~hsr_discharges(
          file = my_report,
          cohort = .x,
          risk_factors = TRUE,
          discharge_phi = FALSE,
          eligible_only = TRUE
        ) |>
        
        # Send risk factors down the rows
        tidyr::pivot_longer(
          cols = -`ID Number`,
          names_to = "Factor",
          values_to = "Value"
        ) |>
        
        # Indicate cohort
        tibble::add_column(Cohort = .x)
      )
risk_factors
```

### iii. Compute Individual Readmission Risks {#individualreadmissionrisk}

The _predicted_ and _expected_ readmission rates are computed for each discharge by plugging in each patient's set of risk factors into risk models developed by CMS. We can use the `hsr_coefficients()` function to extract these:

```{r}
model_weights <-
  cohorts |>

  # Iterate each cohort
  purrr::map_df(

    # Import eligible discharges
    ~hsr_coefficients(
      file = my_report,
      cohort = .x
    ) |>
    
    # Indicate cohort
    tibble::add_column(Cohort = .x)
  )
model_weights
```

To use these in our calculations, we need to attach the model weights for each cohort to our current `risk_factors` dataset using `dplyr::inner_join()`:

```{r}
risk_factors <-
  risk_factors |>

    # Join to get weights
    dplyr::inner_join(
      y = 
        model_weights |> 
          
        # Rename the column
        dplyr::rename(
          Weight = Value
        ),
      by = 
        c(
          "Cohort",
          "Factor"
        )
    )
risk_factors
```

#### Compute the Linear Predictor {#linearpredictor}

These are [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) models, so to convert to a risk estimate, we need to take the _weighted-sum_ of each factor weight with the risk factor value for each discharge.

```{r}
linear_predictors <-
  risk_factors |>

    # Compute weighted sum
    dplyr::summarize(
      LP = sum(Weight * Value),
      .by = 
        c(
          Cohort,
          `ID Number`
        )
    )
linear_predictors
```

You can think of this as the "risk-adjustment" part, where we've adjusted each patient's readmission risk based on their own clinical history. These are on the [logit](https://en.wikipedia.org/wiki/Logit) scale (so not yet probability/risk estimates), but it is what we assume to be linearly related to the outcome, thus we call it the "linear predictor". However, we are still missing something: **the intercept terms**.

#### Add In the Intercepts

The _predicted_ and _expected_ readmission risks are derived from the same risk-adjusted model, the only difference being in the intercept term that is added to complete the linear predictor. We can see those in our model weight list:

```{r}
model_weights |>
  dplyr::filter(
    stringr::str_detect(
      Factor,
      pattern = "EFFECT$"
    )
  )
```

* The `AVG_EFFECT` corresponds to the risk-shift (intercept) associated with being treated at the "average" hospital
* The `HOSP_EFFECT` corresponds to the risk-shift (intercept) associated with being treated at _your_ hospital

These are estimated by CMS using your discharge lists. They are done so _after_ accounting for all of the risk factors, so by comparing them we get a measure of how much more or less likely a patient is to be readmitted at your hospital versus the average hospital, after risk-adjustment.

We can add each one into our current `linear_predictors` to get the complete linear predictors for the predicted and expected readmission rates:

```{r}
linear_predictors <-
  linear_predictors |>

    # Join to get intercepts
    dplyr::inner_join(
      y = 
        model_weights |>

        # Filter to intercepts
        dplyr::filter(
          stringr::str_detect(
            Factor,
            pattern = "_EFFECT$"
          )
        ) |>
        
        # Send over columns
        tidyr::pivot_wider(
          names_from = Factor,
          values_from = Value
        ),
      by = "Cohort"
    ) |>
    
    # Finish LP's
    dplyr::mutate(
      LP_Predicted = LP + HOSP_EFFECT,
      LP_Expected = LP + AVG_EFFECT
    )
linear_predictors
```

#### Transform to Probability Scale

The last thing we need to do is transform the linear predictors to the probabilty scale using the [logistic function](https://en.wikipedia.org/wiki/Logistic_function) in order for our result to be a risk (i.e., a percentage between 0%-100%). 

```{r}
readmission_risks <-
  linear_predictors |>

    # Transform LP's
    dplyr::mutate(
      dplyr::across(
        c(LP_Predicted, LP_Expected),
        \(x) 1 / (1 + exp(-x))
      )
    ) |>
    
    # Keep a few columns
    dplyr::select(
      `ID Number`,
      Predicted = LP_Predicted,
      Expected = LP_Expected,
      Cohort
    )
readmission_risks
```

Now we have the _predicted_ and _expected_ readmission risk for each discharge.

#### Doing it Easier {#readmissionrisks}

We went through those computations to see how deriving the readmission risks works starting with the discharges. However, we can get this automatically by using the `hsr_readmission_risks()` function:

```{r}
readmission_risks <-
  setdiff(cohorts, "CABG") |>

    # Iterate each cohort
    purrr::map_df(

      # Import eligible discharges
      ~hsr_readmission_risks(
        file = my_report,
        cohort = .x
      ) |>
      
      # Indicate cohort
      tibble::add_column(Cohort = .x)
    )
readmission_risks
```

This function extracts eligible discharges and computes the readmission risk, so it captures everything we just did up to this point.

### iv. Compute Cohort-Level Results

Now that we have [individual level readmission risks](#readmissionrisks), we can roll these up to get cohort-level results. The main calculation we need to do is compute the cohort-level predicted and expected readmission rates, which is just the _average_ of the individual ones.

```{r}
cohort_rates <-
  readmission_risks |>

    # Compute cohort-level stats
    dplyr::summarize(
      Discharges = dplyr::n(),
      Predicted = mean(Predicted),
      Expected = mean(Expected),
      .by = Cohort
    )
cohort_rates
```

We can compare this to our [previous cohort summary](#cohortsummary) and see that it matches what was already in the table:

```{r}
cohort_summary |>
    dplyr::select(
      Cohort = `Measure [a]`,
      Discharges = `Number of Eligible Discharges [b]`,
      Predicted = `Predicted Readmission Rate [d]`,
      Expected = `Expected Readmission Rate [e]`
    )
```

#### Pulling in Additional Info {#cohortcontributions}

Recall [earlier](#cohortsummary) how we calculated the payment penalty from the cohort-level data. It was done by computing the:

1. Excess readmission ratio (ERR) for each cohort
2. Difference between (1) and the peer group median ERR for each cohort
3. Sum (2) across cohorts, weighted by DRG ratios

We can do (1) right away with our current `cohort_rates` dataset:

```{r}
cohort_rates <-
  cohort_rates |>

    # Compute the ERR
    dplyr::mutate(
      ERR = Predicted / Expected
    )
cohort_rates
```

For the rest, we need to add the peer group medians and DRG ratios to our `cohort_rates` dataset, which we can get from our existing `cohort_summary` dataset that we [extracted earlier](#cohortsummary):

```{r}
cohort_rates <-
  cohort_rates |>

    # Join to get reference info
    dplyr::inner_join(
      y = 
        cohort_summary |>

        # Make matching names
        dplyr::mutate(
          Cohort = `Measure [a]`,
          Cohort = 
            dplyr::case_when(
              Cohort == "Pneumonia" ~ "PN",
              Cohort == "THA/TKA" ~ "HK",
              TRUE ~ Cohort
            )
        ) |>
        
        # Keep a few columns
        dplyr::select(
          Cohort,
          PeerGroupERR = `Peer Group Median ERR [g]`,
          DRGRatio = `Ratio of DRG Payments Per Measure to Total Payments [i]`
        ),
      by = "Cohort"
    )
cohort_rates
```

Finally, we can indicate which cohorts received penalty and how much they contributed. Recall, a cohort is only eligible to receive penalty if:

1. They have at least 25 discharges
2. The ERR is greater than the peer group median ERR

Thus, we can see, for example, that the COPD group had an ERR greater than the peer group median, but won't actually contribute penalty because there were too few cases. Let's compute the actual contribution for each cohort:

```{r}
cohort_rates <-
  cohort_rates |>

    # Compute penalty contribution
    dplyr::mutate(
      IsPenalized = as.numeric(ERR > PeerGroupERR & Discharges >= 25),
      PenaltyContribution = (ERR - PeerGroupERR) * DRGRatio,
      PenaltyContribution = IsPenalized * PenaltyContribution
    )
cohort_rates
```

Ultimately, based on these criteria, this hospital is only penalized in the Pneumonia cohort. 

### v. Aggregate to the Program Result

Now that we have the [contributions of each cohort](#cohortcontributions), we can aggregate them into our final penalty amount. We'll do this step by step:

1. Add up the cohort contributions

We just computed these in the prior step.

```{r}
temp_penalty <- sum(cohort_rates$PenaltyContribution)
temp_penalty
```

2. Multiply (1) by the neutrality modifier

The neutrality modifier was found in our [program summary](#programsummary) earlier. We can just extract from there.

```{r}
penalty <- temp_penalty * my_payment_summary$`Neutrality Modifier [e]`
penalty
```

We can see that matches what was reported in the [program summary](#programsummary):

```{r}
my_payment_summary$`Payment Reduction Percentage [f]`
```

This is the final penalty amount.

#### Implication

Recall that the (mock) report we've been working with is from FY2025.

```{r}
my_report
```

Let's remind ourselves of the payment period for this program year:

```{r}
my_payment_period <-
  hrrp_payment_periods |>
    dplyr::filter(ProgramYear == 2025)
my_payment_period
```

So from `r format(my_payment_period$StartDate, "%m/%d/%Y")` through `r format(my_payment_period$EndDate, "%m/%d/%Y")`, all Medicare payments for this hospital were reduced by `r paste0(round(penalty * 100, 3), "%")`.

### Key Observations

Some important things to note about the penalty calculation:

#### Readmissions Themselves Don't Compute Penalty

Notice that in the calculation of the penalty, we never explicity used or pulled out the actual discharges that readmitted, or used an "observed" readmission rate (i.e., the simple fraction of discharges that were readmitted). Instead, _all_ discharges were fed into a statistical model to compute the predicted and expected readmission rates for the whole group, and then were aggregated. The actual readmission cases at your hospital were used upstream to fit that statistical model, in a pool along with data from all other hospitals. Thus, the readmissions from your hospital only contribute in an indirect way: they inform the resulting _hospital effect_ estimated from the statistical model, after adjusting for clinical history, which in turn is built into the calculated _predicted_ readmission rate that is computed on each discharge and used downstream in the penalty calculation.

#### Cohorts Aren't Weighted Equally

First, remember that even if a cohort has excessive readmissions, they may not contribute to the penalty if there aren't enough discharges. But beyond that point: the cohort-level contribution to the penalty is not necessarily proportional to how many readmissions they had. It also has to do with how many of those patients are seen at the hospital. Recall that we took the penalty contribution amount and multiplied it by the ratio of DRG's relevant to that cohort out of all payments. This means that it is possible that a cohort with a slight excess in readmissions can contribute much more to the overall penalty amount than another cohort with huge excess, if the former is a much more high-volume diagnosis at your hospital. So you must consider the full picture.

#### Statistical Models Aren't Perfect

Recall that CMS fits random-intercept logistic regression models on a combined dataset from all participating hospitals for each cohort. There are tons of assumptions and nuances that go into these models that make them far from perfect arbitors of truth. For example, each risk factor is added to the model as an independent factor, so no interactions between them are assumed. Additionally, effects are estimated on a combined dataset for all hospitals, which therefore assumes the effect of each clinical factor for risk adjustment is the same for all hospitals. On top of that, each of these risk factors is defined by claims documentation, and, specifically, groupings of ICD codes that may be inconsistent across hospitals. Then, these models are used to estimate individual level readmission risks, leading to group-level excess readmission ratios, that are then compared to a peer group _median_ value to indicate penalty--so by definition, half of hospitals are getting flagged no matter what, even if everyone is doing well. We could keep going down the list of nuances, but it's important to at least acknowledge and understand the mechanics of how these things work. 

## 2. Understanding Model Weights

As described [previously](#modelcoefficients), the _predicted_ and _expected_ readmission rates are based on risk models developed by CMS that estimate hospital-level effects so that individual hospitals can be compared to an "average" hospital for penalty determination. Additionally, these models _risk-adjust_ for individual patient clinical history as to tease out impacts due to the _hospital_ itself instead of the overall morbidity of the population it serves. Thus, each model contains a long list of covariates (risk factors) which can be explored to gain further insight into the mechanics of the program. You can find more details about the model methodology [here](https://qualitynet.cms.gov/inpatient/measures/readmission/methodology).

### What Do They Mean?

The first thing worth understanding are what the model estimates mean and how to interpret them. Recall from [above](#modelcoefficients) that we can extract the model coefficients, which are found in the first row of the discharge-level data for each cohort, from the HSR with the `hsr_coefficients()` function, which we previously extracted [above](#individualreadmissionrisk):

```{r}
model_weights
```

These are the weights (coefficients) of the regression equation that we use to weight individual patient risk factors. Remember that these are currently on the scale of the [linear predictor](#linearpredictor). To make them intuitive and interpretable. we can _exponentiate_ them to put them on the _[odds ratio](https://en.wikipedia.org/wiki/Odds_ratio)_ scale.

```{r}
model_weights |>
  dplyr::mutate(
    OR = exp(Value)
  )
```

For example, according to these estimates, the odds of a readmission for males are `r paste0(round(100 * (1-exp(model_weights$Value[model_weights$Factor == "Male" & model_weights$Cohort == "AMI"])), 1), "%")` lower than females on average for the AMI cohort. So one thing we can do is assess these across all factors to better understand how each risk factor is weighted, by which direction, and how much.

#### Relative Importance 

A more tractable way to organize them for understanding is to rank them by the magnitude of their effects to get a sense of which factors have the most impact on the readmission risk calculation. Here we'll make a plot of the top five (5) most heavily-weighted factors for each cohort:

```{r}
library(ggplot2)
model_weights |>

  # Remove intercepts
  dplyr::filter(!stringr::str_detect(Factor, "_EFFECT$")) |>
  
  # Rank by group
  dplyr::mutate(
    Rank = order(order(abs(Value), decreasing = TRUE)),
    .by = "Cohort"
  ) |>
  
  # Filter to top 10
  dplyr::filter(Rank <= 5) |>
  
  # Make a plot
  ggplot() +
  geom_linerange(
    aes(
      x = stringr::str_sub(Factor, 1, 20),
      ymin = 1,
      ymax = exp(Value),
      color = stringr::str_sub(Factor, 1, 20)
    ),
    linewidth = 1,
    show.legend = FALSE
  ) +
  geom_hline(yintercept = 1) +
  facet_wrap(~Cohort, scales = "free_y", nrow = 3) +
  coord_flip() +
  theme_minimal() +
  xlab("Risk Factor") +
  ylab("Odds Ratio")

```

We can see, for example, that _Dialysis Status_ is an important factor that shows up across multiple cohort models, thus having heavy impact on program results.

We can create the full listing in a table format:

```{r}
library(reactable)
model_weights |>

  # Remove intercepts
  dplyr::filter(!stringr::str_detect(Factor, "_EFFECT$")) |>
  
  # Rank by group
  dplyr::mutate(
    Rank = order(order(abs(Value), decreasing = TRUE)),
    .by = "Cohort"
  ) |>
  
  # Compute odds ratio
  dplyr::mutate(
    OR = exp(Value)
  ) |>
  
  # Rearrange
  dplyr::select(
    Cohort,
    Factor,
    Rank,
    Value,
    OR
  ) |>
  dplyr::arrange(
    Cohort,
    Rank
  ) |>
  
  # Make a table
  reactable(
    groupBy = "Cohort",
    columns = 
      list(
        Factor = colDef(name = "Risk Factor"),
        Value = colDef(name = "Coefficient", format = colFormat(digits = 2)),
        OR = colDef(name = "Odds-Ratio", format = colFormat(digits = 2))
      ),
      searchable = TRUE,
      sortable = TRUE,
      filterable = TRUE,
      resizable = TRUE
  )

```

### Risk Factor Prevalence

We now understand the model output, but how can we incorporate the risk factors in our discharge datasets to gain additional insight?

Recall, the way we calculated the readmission risks was by taking a weighted-sum of the risk factors in our dataset with the model weights (and then doing some transformations to turn it into a probability). We just established that different factors yield different effects (weight) on the readmission risks from the model side and how that can be useful for understanding how CMS weights different clinical history factors. 

The next thing we can seek to understand is how prevalent each of the risk factors are for your cohort. This is useful for a few reasons:

1. You can compare to see if the rates of various risk factors are similar for your hospital versus peer group hospitals
2. It gives insight into the difference between model importance for readmission risk vs. how prevalent the risk factor is
3. We can use the combination of model weights and prevalence of risk factors to understand overall impact. For example, a risk factor that has an average impact in terms of odds-ratio but has very high prevalence at your hospital may have more overall net impact on your HRRP readmission metrics than the most important factor according to the model but has very few patients with it at your hospital.

The first step get at this analysis is to compute these from your datasets. We can do this by manipulating the output of `hsr_discharges()` after using the `risk_factors=TRUE` argument. Here is an example of how we'd do this for AMI:

```{r}
hsr_discharges(
  file = my_report,
  cohort = "AMI",
  discharge_phi = FALSE,
  risk_factors = TRUE,
  eligible_only = TRUE
)
```

However, we've already [done this earlier](#riskfactors) in our exercises, so we'll use that `risk_factors` dataset:

```{r}
risk_factors
```

Now we can compute the prevalence for each risk factor (in each cohort) by simply taking the average `Value`, because all of these are binary factors (except for Age):

```{r}
prevalence <-
  risk_factors |>

    # Remove age (for demo purposes)
    dplyr::filter(!stringr::str_detect(Factor, "^Years")) |>

    # Compute average
    dplyr::summarize(
      N = dplyr::n(),
      Count = sum(Value),
      Rate = Count / N,
      .by = 
        c(
          Cohort,
          Factor
        )
    )
prevalence
```

Then we could put all of these into a navigatable table:

```{r}
prevalence |>
  
    # Arrange
    dplyr::arrange(Cohort, desc(Rate)) |>
    
    # Make a table
    reactable(
      groupBy = "Cohort",
      columns = 
        list(
          Factor = colDef(name = "Risk Factor"),
          N = colDef(name = "Discharge Count"),
          Count = colDef(name = "Count"),
          Rate = colDef(name = "Percent", format = colFormat(digits = 1, percent = TRUE))
        ),
      columnGroups = 
        list(
          colGroup(
            name = "Risk Factor Prevalence",
            columns = c("Count", "Rate")
          )
        ),
      searchable = TRUE,
      sortable = TRUE,
      filterable = TRUE,
      resizable = TRUE
    )
```

### Net Factor Influence

We can then combine the two concepts above (model weights + risk factor prevalence) to explore which factors may have the most overall impact. One way to quantify this is by computing the total weight of a risk factor by taking the number of patients with the risk factor multiplied by the model weight.

```{r}
prevalence |>

  # Join to get model weight
  dplyr::inner_join(
    y = model_weights,
    by = 
      c(
        "Cohort",
        "Factor"
      )
  ) |>
  
  # Make total weight
  dplyr::mutate(
    NetImpact = abs(Count * Value)
  ) |>
  
  # Arrange
  dplyr::arrange(Cohort, desc(NetImpact)) |>
  dplyr::relocate(NetImpact, .after = Factor) |>
    
  # Make a table
  reactable(
    groupBy = "Cohort",
    columns = 
      list(
        Factor = colDef(name = "Risk Factor"),
        NetImpact = colDef(name = "Net Impact", format = colFormat(digits = 2)),
        N = colDef(name = "Discharge Count"),
        Count = colDef(name = "Count"),
        Rate = colDef(name = "Percent", format = colFormat(digits = 1, percent = TRUE))
      ),
    columnGroups = 
      list(
        colGroup(
          name = "Risk Factor Prevalence",
          columns = c("Count", "Rate")
        )
      ),
    searchable = TRUE,
    sortable = TRUE,
    filterable = TRUE,
    resizable = TRUE
  )

```

If you scan through the table you'll notice that the risk factors that have the highest model weight or highest prevalence are not necessarily the ones with the most net impact.

Finally, we could put these in a plot:

```{r}
prevalence |>

  # Join to get model weight
  dplyr::inner_join(
    y = model_weights,
    by = 
      c(
        "Cohort",
        "Factor"
      )
  ) |>
  
  ggplot() +
  geom_point(
    aes(
      x = Count,
      y = exp(Value),
      color = Cohort
    ),
    show.legend = FALSE
  ) +
  geom_hline(yintercept = 1) +
  facet_wrap(~Cohort, scales = "free_x", nrow = 3) +
  theme_minimal() +
  xlab("Risk Factor Count") +
  ylab("Odds Ratio") 
```

Points that are closer to the upper-right quadrant are the most influential. Of course, it would be useful to add interactivity to these plots (e.g., with `plotly`) as to provide the user ability to scan and view what risk factor each point represents, but you get the idea.

## 3. Other Analyses

There are many other possible questions that can be answered analyzing an HSR that can be useful for better understand HRRP program results. We'll list a few more ideas here without actually doing them:

### Diagnosis Comparison

In the discharge reports, CMS supplies the diagnosis code the patient received that led their index discharge to be included in the program. For those who are readmitted, also supplied in this table are the diagnosis codes received at their readmission stay. These can be accessed with the `hsr_discharges()` function and extracting the appropriate columns. Here's an example doing this for the HF cohort:

```{r}
hsr_discharges(
  file = my_report,
  cohort = "HF",
  eligible_only = TRUE
) |>

  # Keep diagnosis columns
  dplyr::select(dplyr::matches("Diagnosis")) |>
  
  # Aggregate
  dplyr::summarize(
    N = dplyr::n(),
    .by = dplyr::everything()
  ) |>
  dplyr::arrange(desc(N))
```

This can be extremely useful to understand (a) which diagnoses are leading to cohort inclusion to begin with, and (b) why the patient came back. Sometimes there may be readmissions that are completely unrelated to the index diagnosis. 

### Outside Hospital Readmissions

Since CMS enforces the HRRP, hospitals get penalized for readmissions that don't even occur at their hospital. If a patient is discharged from your hospital and subsequently readmitted somewhere else, it still counts as _your_ readmission. Thus it is useful to understand the rate of readmissions that are back in your own system versus somewhere else. This is provided the HSR as well.

```{r}
hsr_discharges(
  file = my_report,
  cohort = "HF",
  eligible_only = TRUE
) |>

  # Keep diagnosis columns
  dplyr::select(dplyr::matches("Same Hospital")) |> 
  
  # Make a table
  table(useNA = "ifany")
```

For example, of the 22 HF discharges, three (3) were readmitted: one (1) to the same hospital as the index stay, and two (2) elsewhere (maybe within your system, or not).

### Benchmarking With Other Hospitals

CMS provides access to hospital-level performance data on [QualityNet](https://data.cms.gov/provider-data/). You can use the `pdc_*` functions in this package to explore and import various data files to perform comparative analyses at the hospital level (see `pdc_read()`).

_Don't forget to [subscribe](https://dashboard.mailerlite.com/forms/1517199/154300987644839168/share) to receive email updates when new articles drop!_